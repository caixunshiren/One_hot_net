{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x230166af1f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "random_seed = 420\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size_train = 64\n",
    "batch_size_test = 500\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "fname = \"figures\\cryptography\\\\ground_truth.png\"\n",
    "plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network flowchart:\n",
    "#input vector --- random initialized weights ---> hyperdimensional vecotor ---> one_hot_net l1 ---> ...ln... ---> sigmoid/softmax output\n",
    "#One hot net\n",
    "\n",
    "class One_hot_op(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, W, epsilon):\n",
    "        Z = torch.matmul(W, A)\n",
    "        ctx.Z = Z\n",
    "        ctx.A = A\n",
    "        ctx.W = W\n",
    "        ctx.epsilon = epsilon\n",
    "        ret = Z > epsilon\n",
    "        #print(ret[1:10][1:10])\n",
    "        return ret.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dL_dA):\n",
    "        step = ctx.Z > ctx.epsilon\n",
    "        step = step.float()\n",
    "        dL_dZ = dL_dA * step \n",
    "        \n",
    "        dZ_dW = torch.transpose(ctx.A, 0,1)\n",
    "        dZ_dW = torch.sign(dZ_dW)\n",
    "        dZ_dA = torch.transpose(ctx.W, 0,1)\n",
    "        dZ_dA = torch.sign(dZ_dA)\n",
    "        dA = torch.matmul(dZ_dA,dL_dZ)\n",
    "        dW = torch.matmul(dL_dZ,dZ_dW)\n",
    "        return dA, dW, None\n",
    "\n",
    "\n",
    "class One_hot_layer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, initialization_f, epsilon):\n",
    "        super(One_hot_layer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.initialization_f = initialization_f\n",
    "        initialized_weight = initialization_f(out_dim, in_dim)\n",
    "        self.weight = nn.Parameter(initialized_weight, requires_grad = True)\n",
    "        self.op = One_hot_op\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_dim) + ',' \\\n",
    "               + str(self.out_dim) + ')'\n",
    "        \n",
    "    def forward(self, A):\n",
    "        #print(self.weight[:3][:3])\n",
    "        return self.op.apply(A, self.weight, self.epsilon)\n",
    "    \n",
    "\n",
    "class One_hot_net(nn.Module):\n",
    "    def __init__(self, in_dim, n_class, f_encoder, encoder_multiplier, f_initializer, epsilon, n_layers=2, layer_size_factor=[1, 5], dropout=[-1, 0.5]):\n",
    "        super(One_hot_net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.in_dim = in_dim\n",
    "        feature_len = in_dim * encoder_multiplier\n",
    "        self.feature_len = feature_len\n",
    "        self.n_layers=n_layers\n",
    "        self.layer_size_factor=layer_size_factor\n",
    "        self.dropout=dropout\n",
    "        self.epsilon = epsilon\n",
    "        self.n_class = n_class\n",
    "        self.f_encoder = f_encoder\n",
    "        self.f_initializer = f_initializer\n",
    "        for i in range(n_layers):\n",
    "            if dropout[i] > 0:\n",
    "                self.layers.append(nn.Dropout(dropout[i]))\n",
    "            if i < n_layers - 1:\n",
    "                self.layers.append(\n",
    "                    One_hot_layer(int(feature_len // layer_size_factor[i]), int(feature_len // layer_size_factor[i + 1]), f_initializer, epsilon))\n",
    "        self.tail = nn.Linear(int(feature_len // layer_size_factor[-1]), n_class)\n",
    "    \n",
    "    def flatten(self, X):\n",
    "        return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "    \n",
    "    def unflatten(self, X):\n",
    "        return X.view(X.shape[0], 1,int(self.in_dim**(1/2)), int(self.in_dim**(1/2)))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        #X = self.f_encoder.apply(X)\n",
    "        X = self.f_encoder.apply_wnoise(X, 0.5)\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        X = self.tail(X)\n",
    "        return self.unflatten(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise \n",
    "def apply_gaussian_noise(tensor, sd, device = torch.device(\"cuda:0\")):\n",
    "    tensor = tensor + (sd)*torch.randn(*tuple(tensor.shape)).to(device)\n",
    "    return tensor\n",
    "\n",
    "def apply_binary_noise(tensor, p, device = torch.device(\"cuda:0\")):\n",
    "    #change 1 entries to 0\n",
    "    tensor = tensor * (torch.rand(*tuple(tensor.shape))>(p/2)).float().to(device)\n",
    "    #change 0 entries to 1\n",
    "    tensor = tensor + (torch.rand(*tuple(tensor.shape))>(1-p/2)).float().to(device)\n",
    "    return torch.sign(tensor)\n",
    "\n",
    "#a = [[1,1,0,0,1,1], [1,1,0,0,1,1], [1,1,0,0,1,1]]\n",
    "#a = torch.tensor(a)\n",
    "#print((torch.rand(*tuple(a.shape))<(0.1/2)).float())\n",
    "#print(a)\n",
    "#print(apply_binary_noise(a, 0.1, device = torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializers and encoders\n",
    "def uniform_initializer(out_dim, in_dim, cuda = True):\n",
    "    tensor = torch.empty(out_dim, in_dim)\n",
    "    if cuda:\n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2).cuda()\n",
    "    else: \n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2)\n",
    "\n",
    "class simple_encoder():\n",
    "    def __init__(self, out_dim, in_dim):\n",
    "        self.W = uniform_initializer(out_dim, in_dim)\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return torch.matmul(self.W, X)\n",
    "    \n",
    "class simple_encoder_wthreshold():\n",
    "    def __init__(self, out_dim, in_dim, epsilon, cuda = True):\n",
    "        self.W = uniform_initializer(out_dim, in_dim, cuda)\n",
    "        self.device = torch.device(\"cuda:0\") if cuda else torch.device(\"cpu\")\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def apply(self, X):\n",
    "        return (torch.matmul(self.W, X) > self.epsilon).float()\n",
    "    \n",
    "    def apply_wnoise(self, X, sd):\n",
    "        return (torch.matmul(apply_gaussian_noise(self.W, sd, device = self.device), X) > self.epsilon).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'in_dim': 784,\n",
    "    'n_class': 784,\n",
    "    'f_encoder': simple_encoder_wthreshold(784*20, 784, 1e-3),\n",
    "    'f_initializer': uniform_initializer,\n",
    "    'encoder_multiplier': 20,\n",
    "    'epsilon': 10e-3,\n",
    "    'n_layers': 1,\n",
    "    'layer_size_factor': [1],\n",
    "    'dropout': [-1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model1 = One_hot_net(parameters['in_dim'], parameters['n_class'], parameters['f_encoder'], parameters['encoder_multiplier'], \n",
    "                     parameters['f_initializer'], parameters['epsilon'], parameters['n_layers'], \n",
    "                     parameters['layer_size_factor'], parameters['dropout']).to(device)\n",
    "\n",
    "#model1 = toy_Net().to(device)\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.01, momentum=0.5)\n",
    "#optimizer1 = torch.optim.SGD([{'params': model1.layers.parameters(), 'lr': 0.01}, {'params': model1.tail.parameters(), 'lr': 0.01}], lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 784]      12,293,904\n",
      "================================================================\n",
      "Total params: 12,293,904\n",
      "Trainable params: 12,293,904\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 46.90\n",
      "Estimated Total Size (MB): 46.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model1, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, trainloader, log_interval = 10, device = torch.device(\"cuda:0\")):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = nn.MSELoss()(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "            (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            #torch.save(model.state_dict(), '/results/model.pth')\n",
    "            #torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device = torch.device(\"cuda:0\")):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += nn.MSELoss()(output, data).item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_loss *=500\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.6f}\\n'.format(\n",
    "        test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 1.171135\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.108331\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.716262\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.656667\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.660803\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.597940\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.627041\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.555298\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.568414\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.592509\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.527792\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.483132\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.484107\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.425608\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.452507\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.426987\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.443426\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.426481\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.418990\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.388558\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.375211\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.363324\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.368910\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.380943\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.358267\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.324968\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.366161\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.357673\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.344629\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.330807\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.329230\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.324089\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.312957\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.307319\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.297194\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.297846\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.322940\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.300952\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.298243\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.304151\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.267856\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.302845\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.256777\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.256799\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.282150\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.291081\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.260453\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.257367\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.259771\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.241237\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.249452\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.242534\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.236581\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.242441\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.242701\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.252122\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.249925\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.246164\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.226831\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.224810\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.227377\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.241686\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.210895\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.223539\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.249853\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.216040\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.204619\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.201728\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.223249\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.197464\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.213763\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.210809\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.212446\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.216105\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.210889\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.202781\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.208379\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.198709\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.180416\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.182785\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.211509\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.194062\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.208397\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.187157\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.186271\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.190227\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.181654\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.195708\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.198535\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.183347\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.192949\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.186817\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.180851\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.196276\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.176190\n",
      "\n",
      "Test set: Avg. loss: 0.184097\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.199964\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.180015\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.193453\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.183266\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.181322\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.183627\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.184313\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.187037\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.180436\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.160938\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.178848\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.164130\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.194239\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.173398\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.174681\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.167693\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.153539\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.171699\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.161121\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.169980\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.159906\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.166723\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.166383\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.171284\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.158330\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.154282\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.173547\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.162430\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.171517\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.160137\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.171518\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.166738\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.162037\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.169388\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.153979\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.150069\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.174895\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.158980\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.165556\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.162059\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.155972\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.166123\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.165773\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.163498\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.154135\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.155382\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.160192\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.146191\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.149871\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.154941\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.141397\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.145674\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.151685\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.142255\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.144889\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.144018\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.142856\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.158390\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.136470\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.137401\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.138040\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.147812\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.147777\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.141728\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.153942\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.130296\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.147184\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.149026\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.156958\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.150227\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.147748\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.141254\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.148834\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.134623\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.151053\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.144331\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.138756\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.137603\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.131583\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.139465\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.134671\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.137575\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.132186\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.143383\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.134148\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.130330\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.145389\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.127949\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.141622\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.132385\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.131642\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.139782\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.134649\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.133031\n",
      "\n",
      "Test set: Avg. loss: 0.132242\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.133532\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.145625\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.144354\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.137447\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.126706\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.133124\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.130146\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.127963\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.127729\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.136307\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.128475\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.125778\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.130201\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.127083\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.140314\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.126741\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.132136\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.133445\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.134984\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.133881\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.128276\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.134000\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.139725\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.119176\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.120932\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.123160\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.120143\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.119950\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.118587\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.135720\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.126445\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.119834\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.128691\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.125708\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.125526\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.128220\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.128043\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.122998\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.114626\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.125385\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.138910\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.119091\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.120880\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.119352\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.125148\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.105860\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.118994\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.125137\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.114891\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.115952\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.115280\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.126045\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.120133\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.121379\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.127913\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.126266\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.120183\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.115145\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.120585\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.122894\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.114795\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.117444\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.112934\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.109149\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.114414\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.119000\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.119768\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.111816\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.120482\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.110975\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.108649\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.117727\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.104935\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.112667\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.111081\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.109196\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.110541\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.123192\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.115508\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.120889\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.115572\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.108484\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.114300\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.114096\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.103423\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.109701\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.114775\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.105968\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.102515\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.110412\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.110995\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.104775\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.107950\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.106770\n",
      "\n",
      "Test set: Avg. loss: 0.109802\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.109850\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.111435\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.115368\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.103149\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.116682\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.119695\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.118194\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.095900\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.108286\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.105673\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.109788\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.108139\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.108936\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.108487\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.110342\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.106033\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.106270\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.115816\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.107146\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.105439\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.107644\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.110710\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.114039\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.103054\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.110444\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.105367\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.120969\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.102921\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.109735\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.097653\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.105114\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.105673\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.107005\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.097219\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.101812\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.117260\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.110787\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.103650\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.109232\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.105819\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.107796\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.101977\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.111053\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.107152\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.094869\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.110324\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.098304\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.106071\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.100688\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.102170\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.096684\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.102672\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.109801\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.093939\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.101341\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.097903\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.103586\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.106323\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.107319\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.094674\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.094134\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.102162\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.101347\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.096453\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.096081\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.095869\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.114143\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.101620\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.098985\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.101882\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.108315\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.102810\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.096074\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.097480\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.099337\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.107310\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.100021\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.099218\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.095471\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.097616\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.095157\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.091311\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.099555\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.110076\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.098892\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.105494\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.096840\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.105347\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.094004\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.096959\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.097341\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.095837\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.097647\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.092535\n",
      "\n",
      "Test set: Avg. loss: 0.096636\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.098071\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.098412\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.092704\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.101739\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.099338\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.095925\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.094856\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.096758\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.096562\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.091704\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.096016\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.092069\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.102261\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.093539\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.095906\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.103888\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.097801\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.099386\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.088495\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.089714\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.100497\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.101178\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.101941\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.098480\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.095480\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.089841\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.090403\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.087329\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.095071\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.093684\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.091173\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.094709\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.097072\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.094611\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.094389\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.098670\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.094442\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.095490\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.090902\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.084011\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.081386\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.094377\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.096362\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.089950\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.093564\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.089461\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.096048\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.084340\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.091232\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.089486\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.089252\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.092476\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.087595\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.090754\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.091363\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.093312\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.089696\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.085325\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.096708\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.086954\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.094625\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.088815\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.098101\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.090542\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.096482\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.088063\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.089771\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.090014\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.096331\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.089472\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.094881\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.090512\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.090438\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.085419\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.085759\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.090206\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.087486\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.083182\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.081867\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.093527\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.095614\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.089549\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.095223\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.090800\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.090322\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.091594\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.090915\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.091094\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.093171\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.089150\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.089245\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.088689\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.094037\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.091501\n",
      "\n",
      "Test set: Avg. loss: 0.087680\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.086344\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.083494\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.090856\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.089270\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.091952\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.091952\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.083845\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.083870\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.083382\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.084618\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.093237\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.076720\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.092023\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.090839\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.087373\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.087685\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.085891\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.086753\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.082781\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.089896\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.088067\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.080656\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.085955\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.091663\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.089217\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.084301\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.086892\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.080495\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.083875\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.084009\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.087180\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.082970\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.088854\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.083829\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.081539\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.079816\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.088296\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.083115\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.087451\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.085373\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.090828\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.084927\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.078726\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.078897\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.086468\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.083746\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.080437\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.085929\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.085840\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.081855\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.091964\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.086333\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.086584\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.086291\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.086055\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.088246\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.088192\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.082119\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.088402\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.085970\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.084745\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.080919\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.079965\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.085327\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.083505\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.084764\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.085233\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.085476\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.085808\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.082949\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.086305\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.081302\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.082728\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.081229\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.083763\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.080430\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.079951\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.080961\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.078993\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.073979\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.081770\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.084134\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.084513\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.076726\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.077098\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.079822\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.087719\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.084161\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.086266\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.074560\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.084589\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.085608\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.082647\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.083585\n",
      "\n",
      "Test set: Avg. loss: 0.081079\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.078140\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.086362\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.079873\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.082475\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.086960\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.090882\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.081273\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.085928\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.076497\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.080102\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.082023\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.077987\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.077828\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.077026\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.089263\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.080877\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.075295\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.078573\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.079826\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.076085\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.077053\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.078943\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.075925\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.074731\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.081065\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.079832\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.070638\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.082606\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.076768\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.082595\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.073140\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.078078\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.082394\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.077075\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.083785\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.081939\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.074167\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.072427\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.078155\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.074938\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.078139\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.079884\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.077203\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.083010\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.079464\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.075370\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.078970\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.083342\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.079597\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.080695\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.082725\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.077682\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.076774\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.076525\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.080729\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.078656\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.078257\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.080229\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.077095\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.076077\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.078370\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.077891\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.075227\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.079063\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.077060\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.073776\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.075661\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.073357\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.078531\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.074437\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.078689\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.077307\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.070846\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.076225\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.073466\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.076504\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.077631\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.072121\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.078550\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.072769\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.084796\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.079358\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.075822\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.072943\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.072868\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.070918\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.077495\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.078299\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.076109\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.078416\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.081049\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.075236\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.071662\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.078088\n",
      "\n",
      "Test set: Avg. loss: 0.075953\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.076452\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.079264\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.075002\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.076300\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.071607\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.072768\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.080471\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.074225\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.075670\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.070273\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.070724\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.074401\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.077297\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.068064\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.077952\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.075299\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.074283\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.070951\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.078759\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.078440\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.073428\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.074793\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.081102\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.078486\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.067911\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.073204\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.077127\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.078439\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.075941\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.072370\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.070722\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.078008\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.073763\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.072832\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.073568\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.072817\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.073531\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.074867\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.070657\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.077040\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.070725\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.070102\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.073259\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.070214\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.070998\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.073956\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.073506\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.077240\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.067529\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.075077\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.072103\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.073670\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.068399\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.073825\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.071713\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.070062\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.069148\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.072353\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.068138\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.076132\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.070029\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.073676\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.077414\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.075732\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.074870\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.069581\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.071106\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.078110\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.074989\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.072150\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.071024\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.069893\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.069597\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.066108\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.067037\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.070845\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.076859\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.069605\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.074388\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.065113\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.073031\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.072015\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.072466\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.069280\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.074021\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.069765\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.070132\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.067932\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.069777\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.065395\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.069773\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.074535\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.072795\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.067250\n",
      "\n",
      "Test set: Avg. loss: 0.071776\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.072377\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.069673\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.071488\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.071693\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.073469\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.077122\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.073208\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.070291\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.065123\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.066058\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.069077\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.065807\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.074737\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.069824\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.077795\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.072613\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.065687\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.073334\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.073668\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.064090\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.074473\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.073261\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.065715\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.067590\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.068444\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.071050\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.071891\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.068919\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.074945\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.072890\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.065557\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.075382\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.068231\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.066752\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.067636\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.062486\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.073681\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.075793\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.068242\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.070250\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.069898\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.065440\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.072532\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.070992\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.070947\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.069641\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.073681\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.068303\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.071588\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.068890\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.070594\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.069289\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.068527\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.066126\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.071712\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.067127\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.068418\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.076948\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.068479\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.068696\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.070316\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.068963\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.073803\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.072327\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.072078\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.065796\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.072043\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.070266\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.065796\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.071436\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.064966\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.065514\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.064782\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.068705\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.064522\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.064251\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.063556\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.067197\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.059310\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.064040\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.066203\n"
     ]
    }
   ],
   "source": [
    "model_name = \"crypto_net1\"\n",
    "test(model1, test_loader)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch, model1, optimizer1, train_loader)\n",
    "    test(model1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('MSE loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"figures\\cryptography\\\\reconstructed_160D.png\"\n",
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "reconstructed_example_data = model1(example_data.to(torch.device(\"cuda:0\"))).cpu().detach().numpy()\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(reconstructed_example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "#plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
