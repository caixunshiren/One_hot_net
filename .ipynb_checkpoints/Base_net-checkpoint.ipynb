{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size_train = 64\n",
    "batch_size_test = 500\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network flowchart:\n",
    "#input vector --- random initialized weights ---> hyperdimensional vecotor ---> one_hot_net l1 ---> ...ln... ---> sigmoid/softmax output\n",
    "#One hot net\n",
    "\n",
    "class One_hot_op(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, W, epsilon):\n",
    "        Z = torch.matmul(W, A)\n",
    "        ctx.Z = Z\n",
    "        ctx.A = A\n",
    "        ctx.W = W\n",
    "        ret = Z > epsilon\n",
    "        #print(ret[1:10][1:10])\n",
    "        return ret.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dL_dA):\n",
    "        step = ctx.Z > 0\n",
    "        step = step.float()\n",
    "        dL_dZ = dL_dA * step \n",
    "        \n",
    "        dZ_dW = torch.transpose(ctx.A, 0,1)\n",
    "        dZ_dW = dZ_dW / torch.abs(dZ_dW)\n",
    "        dZ_dA = torch.transpose(ctx.W, 0,1)\n",
    "        dZ_dA = dZ_dA / torch.abs(dZ_dA)\n",
    "        dA = torch.matmul(dZ_dA,dL_dZ)\n",
    "        dW = torch.matmul(dL_dZ,dZ_dW)\n",
    "        return dA, dW, None\n",
    "\n",
    "\n",
    "class One_hot_layer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, initialization_f, epsilon):\n",
    "        super(One_hot_layer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.initialization_f = initialization_f\n",
    "        initialized_weight = initialization_f(out_dim, in_dim)\n",
    "        self.weight = nn.Parameter(initialized_weight, requires_grad = True)\n",
    "        self.op = One_hot_op\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_dim) + ',' \\\n",
    "               + str(self.out_dim) + ')'\n",
    "        \n",
    "    def forward(self, A):\n",
    "        return self.op.apply(A, self.weight, self.epsilon)\n",
    "    \n",
    "\n",
    "class One_hot_net(nn.Module):\n",
    "    def __init__(self, in_dim, n_class, f_encoder, encoder_multiplier, f_initializer, epsilon, n_layers=2, layer_size_factor=[1, 5], dropout=[-1, 0.5]):\n",
    "        super(One_hot_net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.in_dim = in_dim\n",
    "        feature_len = in_dim * encoder_multiplier\n",
    "        self.feature_len = feature_len\n",
    "        self.n_layers=n_layers\n",
    "        self.layer_size_factor=layer_size_factor\n",
    "        self.dropout=dropout\n",
    "        self.n_class = n_class\n",
    "        self.f_encoder = f_encoder\n",
    "        self.f_initializer = f_initializer\n",
    "        for i in range(n_layers):\n",
    "            if dropout[i] > 0:\n",
    "                self.layers.append(nn.Dropout(dropout[i]))\n",
    "            if i < n_layers - 1:\n",
    "                self.layers.append(\n",
    "                    One_hot_layer(int(feature_len // layer_size_factor[i]), int(feature_len // layer_size_factor[i + 1]), f_initializer, epsilon))\n",
    "        self.tail = nn.Linear(int(feature_len // layer_size_factor[-1]), n_class)\n",
    "        self.out = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    def flatten(self, X):\n",
    "        return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        X = self.f_encoder.apply(X)\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        X = self.tail(X)\n",
    "        return self.out(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializers and encoders\n",
    "def uniform_initializer(out_dim, in_dim):\n",
    "    tensor = torch.empty(out_dim, in_dim)\n",
    "    return torch.nn.init.uniform_(tensor, a=-2, b=2).cuda() \n",
    "\n",
    "class simple_encoder():\n",
    "    def __init__(self, out_dim, in_dim):\n",
    "        self.W = uniform_initializer(out_dim, in_dim)\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return torch.matmul(self.W, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'in_dim': 784,\n",
    "    'n_class': 10,\n",
    "    'f_encoder': simple_encoder(784*20, 784),\n",
    "    'f_initializer': uniform_initializer,\n",
    "    'encoder_multiplier': 20,\n",
    "    'epsilon': 10e-3,\n",
    "    'n_layers': 2,\n",
    "    'layer_size_factor': [1, 1],\n",
    "    'dropout': [-1, -1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model1 = One_hot_net(parameters['in_dim'], parameters['n_class'], parameters['f_encoder'], parameters['encoder_multiplier'], \n",
    "                     parameters['f_initializer'], parameters['epsilon'], parameters['n_layers'], \n",
    "                     parameters['layer_size_factor'], parameters['dropout']).to(device)\n",
    "\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.001, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "     One_hot_layer-1                    [20, 2]     245,862,400\n",
      "            Linear-2                   [20, 10]         156,810\n",
      "        LogSoftmax-3                   [20, 10]               0\n",
      "================================================================\n",
      "Total params: 246,019,210\n",
      "Trainable params: 246,019,210\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 938.49\n",
      "Estimated Total Size (MB): 938.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model1, (1, 28, 28), batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, trainloader, log_interval = 10, device = torch.device(\"cuda:0\")):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "            (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            #torch.save(model.state_dict(), '/results/model.pth')\n",
    "            #torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "        return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device = torch.device(\"cuda:0\")):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caixu\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.4287, Accuracy: 638/10000 (6%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.389909\n",
      "\n",
      "Test set: Avg. loss: 2.3607, Accuracy: 1073/10000 (11%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.426191\n",
      "\n",
      "Test set: Avg. loss: 2.2469, Accuracy: 1812/10000 (18%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.203683\n",
      "\n",
      "Test set: Avg. loss: 2.2137, Accuracy: 1402/10000 (14%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.187992\n",
      "\n",
      "Test set: Avg. loss: 2.1183, Accuracy: 2306/10000 (23%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.165634\n",
      "\n",
      "Test set: Avg. loss: 2.0514, Accuracy: 3134/10000 (31%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.058732\n",
      "\n",
      "Test set: Avg. loss: 1.9960, Accuracy: 3296/10000 (33%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.040178\n",
      "\n",
      "Test set: Avg. loss: 1.9104, Accuracy: 4278/10000 (43%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.938468\n",
      "\n",
      "Test set: Avg. loss: 1.8496, Accuracy: 4856/10000 (49%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.854951\n",
      "\n",
      "Test set: Avg. loss: 1.8032, Accuracy: 4987/10000 (50%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.774381\n",
      "\n",
      "Test set: Avg. loss: 1.7623, Accuracy: 4910/10000 (49%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 1.748068\n",
      "\n",
      "Test set: Avg. loss: 1.7137, Accuracy: 4866/10000 (49%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 1.720540\n",
      "\n",
      "Test set: Avg. loss: 1.6697, Accuracy: 5279/10000 (53%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 1.668774\n",
      "\n",
      "Test set: Avg. loss: 1.6268, Accuracy: 5491/10000 (55%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 1.639250\n",
      "\n",
      "Test set: Avg. loss: 1.5486, Accuracy: 6562/10000 (66%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 1.615210\n",
      "\n",
      "Test set: Avg. loss: 1.5084, Accuracy: 6592/10000 (66%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 1.549248\n",
      "\n",
      "Test set: Avg. loss: 1.4740, Accuracy: 6644/10000 (66%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 1.571792\n",
      "\n",
      "Test set: Avg. loss: 1.4448, Accuracy: 6771/10000 (68%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 1.443783\n",
      "\n",
      "Test set: Avg. loss: 1.4006, Accuracy: 6877/10000 (69%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 1.354797\n",
      "\n",
      "Test set: Avg. loss: 1.3643, Accuracy: 7057/10000 (71%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 1.352973\n",
      "\n",
      "Test set: Avg. loss: 1.3304, Accuracy: 7230/10000 (72%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model1, test_loader)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch, model1, optimizer1, train_loader)\n",
    "    test(model1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
