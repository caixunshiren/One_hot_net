{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x249b6f411d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "random_seed = 420\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size_train = 64\n",
    "batch_size_test = 500\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network flowchart:\n",
    "#input vector --- random initialized weights ---> hyperdimensional vecotor ---> one_hot_net l1 ---> ...ln... ---> sigmoid/softmax output\n",
    "#One hot net\n",
    "\n",
    "class One_hot_op(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, W, epsilon):\n",
    "        Z = torch.matmul(W, A)\n",
    "        ctx.Z = Z\n",
    "        ctx.A = A\n",
    "        ctx.W = W\n",
    "        ctx.epsilon = epsilon\n",
    "        ret = Z > epsilon\n",
    "        #print(ret[1:10][1:10])\n",
    "        return ret.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dL_dA):\n",
    "        step = ctx.Z > ctx.epsilon\n",
    "        step = step.float()\n",
    "        dL_dZ = dL_dA * step \n",
    "        \n",
    "        dZ_dW = torch.transpose(ctx.A, 0,1)\n",
    "        dZ_dW = dZ_dW / torch.abs(dZ_dW)\n",
    "        dZ_dA = torch.transpose(ctx.W, 0,1)\n",
    "        dZ_dA = dZ_dA / torch.abs(dZ_dA)\n",
    "        dA = torch.matmul(dZ_dA,dL_dZ)\n",
    "        dW = torch.matmul(dL_dZ,dZ_dW)\n",
    "        return dA, dW, None\n",
    "\n",
    "\n",
    "class One_hot_layer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, initialization_f, epsilon):\n",
    "        super(One_hot_layer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.initialization_f = initialization_f\n",
    "        initialized_weight = initialization_f(out_dim, in_dim)\n",
    "        self.weight = nn.Parameter(initialized_weight, requires_grad = True)\n",
    "        self.op = One_hot_op\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_dim) + ',' \\\n",
    "               + str(self.out_dim) + ')'\n",
    "        \n",
    "    def forward(self, A):\n",
    "        return self.op.apply(A, self.weight, self.epsilon)\n",
    "    \n",
    "\n",
    "class One_hot_net(nn.Module):\n",
    "    def __init__(self, in_dim, n_class, f_encoder, encoder_multiplier, f_initializer, epsilon, n_layers=2, layer_size_factor=[1, 5], dropout=[-1, 0.5]):\n",
    "        super(One_hot_net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.in_dim = in_dim\n",
    "        feature_len = in_dim * encoder_multiplier\n",
    "        self.feature_len = feature_len\n",
    "        self.n_layers=n_layers\n",
    "        self.layer_size_factor=layer_size_factor\n",
    "        self.dropout=dropout\n",
    "        self.epsilon = epsilon\n",
    "        self.n_class = n_class\n",
    "        self.f_encoder = f_encoder\n",
    "        self.f_initializer = f_initializer\n",
    "        for i in range(n_layers):\n",
    "            if dropout[i] > 0:\n",
    "                self.layers.append(nn.Dropout(dropout[i]))\n",
    "            if i < n_layers - 1:\n",
    "                self.layers.append(\n",
    "                    One_hot_layer(int(feature_len // layer_size_factor[i]), int(feature_len // layer_size_factor[i + 1]), f_initializer, epsilon))\n",
    "        self.tail = nn.Linear(int(feature_len // layer_size_factor[-1]), n_class)\n",
    "        self.out = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    def flatten(self, X):\n",
    "        return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        X = self.f_encoder.apply(X)\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        X = self.tail(X)\n",
    "        return self.out(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializers and encoders\n",
    "def uniform_initializer(out_dim, in_dim, cuda = True):\n",
    "    tensor = torch.empty(out_dim, in_dim)\n",
    "    if cuda:\n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2).cuda()\n",
    "    else: \n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2)\n",
    "\n",
    "class simple_encoder():\n",
    "    def __init__(self, out_dim, in_dim):\n",
    "        self.W = uniform_initializer(out_dim, in_dim)\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return torch.matmul(self.W, X)\n",
    "    \n",
    "class simple_encoder_wthreshold():\n",
    "    def __init__(self, out_dim, in_dim, epsilon, cuda = True):\n",
    "        self.W = uniform_initializer(out_dim, in_dim, cuda)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return (torch.matmul(self.W, X) > self.epsilon).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'in_dim': 784,\n",
    "    'n_class': 10,\n",
    "    'f_encoder': simple_encoder(784*20, 784),\n",
    "    'f_initializer': uniform_initializer,\n",
    "    'encoder_multiplier': 20,\n",
    "    'epsilon': 10e-3,\n",
    "    'n_layers': 2,\n",
    "    'layer_size_factor': [1, 1, 1],\n",
    "    'dropout': [-1, -1, -1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model1 = One_hot_net(parameters['in_dim'], parameters['n_class'], parameters['f_encoder'], parameters['encoder_multiplier'], \n",
    "                     parameters['f_initializer'], parameters['epsilon'], parameters['n_layers'], \n",
    "                     parameters['layer_size_factor'], parameters['dropout']).to(device)\n",
    "\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "     One_hot_layer-1                    [-1, 2]     245,862,400\n",
      "            Linear-2                   [-1, 10]         156,810\n",
      "        LogSoftmax-3                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 246,019,210\n",
      "Trainable params: 246,019,210\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 938.49\n",
      "Estimated Total Size (MB): 938.49\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model1, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, trainloader, log_interval = 10, device = torch.device(\"cuda:0\")):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "            (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            #torch.save(model.state_dict(), '/results/model.pth')\n",
    "            #torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device = torch.device(\"cuda:0\")):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caixu\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.4264, Accuracy: 533/10000 (5.3%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.373712\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 17.190836\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 3.455778\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.488327\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.709110\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.536325\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.335757\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.285905\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.345124\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.666539\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.374166\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.546895\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.285413\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.384646\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.184374\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.258399\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.226887\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.243033\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.409232\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.325956\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.239830\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.671615\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.361860\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.605122\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.347242\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.332726\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.392877\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.611809\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.190025\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.630127\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.374252\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.298030\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.300864\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.434808\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.568386\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.152683\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.331007\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.111725\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.242611\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.302603\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.318214\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.271134\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.358214\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.271493\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.084341\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.209219\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.366143\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.473319\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.340202\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.284563\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.271779\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.243306\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.254817\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 2.301972\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.304076\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.303157\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 2.301399\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 2.303028\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.304229\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 2.302754\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.302152\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 2.301423\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.301462\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 2.301678\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b53d48a6a571>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0macc_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0macc_max\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-7cedabeb16e9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, model, optimizer, trainloader, log_interval, device)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-004b5fda35be>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1688\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1690\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = \"base_net1\"\n",
    "acc_max = test(model1, test_loader)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch, model1, optimizer1, train_loader)\n",
    "    acc = test(model1, test_loader)\n",
    "    if acc > acc_max:\n",
    "        acc = acc_max\n",
    "        print()\n",
    "        print(\"--- saving model for best accuracy ---\")\n",
    "        torch.save(model1.state_dict(), 'results/'+model_name+'.pth')\n",
    "        torch.save(optimizer1.state_dict(), 'results/'+model_name+'_optimizer.pth')\n",
    "        print(\"--- saving finished ---\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model1.state_dict(), 'results/base_model.pth')\n",
    "#torch.save(optimizer1.state_dict(), 'results/base_optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
