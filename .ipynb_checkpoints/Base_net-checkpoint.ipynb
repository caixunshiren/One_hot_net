{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11e5f25f1f0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "random_seed = 420\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "batch_size_train = 64\n",
    "batch_size_test = 500\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network flowchart:\n",
    "#input vector --- random initialized weights ---> hyperdimensional vecotor ---> one_hot_net l1 ---> ...ln... ---> sigmoid/softmax output\n",
    "#One hot net\n",
    "\n",
    "class One_hot_op(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, W, epsilon):\n",
    "        Z = torch.matmul(W, A)\n",
    "        ctx.Z = Z\n",
    "        ctx.A = A\n",
    "        ctx.W = W\n",
    "        ctx.epsilon = epsilon\n",
    "        ret = Z > epsilon\n",
    "        #print(ret[1:10][1:10])\n",
    "        return ret.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dL_dA):\n",
    "        step = ctx.Z > ctx.epsilon\n",
    "        step = step.float()\n",
    "        dL_dZ = dL_dA * step \n",
    "        \n",
    "        dZ_dW = torch.transpose(ctx.A, 0,1)\n",
    "        dZ_dW = torch.sign(dZ_dW)\n",
    "        dZ_dA = torch.transpose(ctx.W, 0,1)\n",
    "        dZ_dA = torch.sign(dZ_dA)\n",
    "        dA = torch.matmul(dZ_dA,dL_dZ)\n",
    "        dW = torch.matmul(dL_dZ,dZ_dW)\n",
    "        return dA, dW, None\n",
    "\n",
    "\n",
    "class One_hot_layer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, initialization_f, epsilon):\n",
    "        super(One_hot_layer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.initialization_f = initialization_f\n",
    "        initialized_weight = initialization_f(out_dim, in_dim)\n",
    "        self.weight = nn.Parameter(initialized_weight, requires_grad = True)\n",
    "        self.op = One_hot_op\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_dim) + ',' \\\n",
    "               + str(self.out_dim) + ')'\n",
    "        \n",
    "    def forward(self, A):\n",
    "        #print(self.weight[:3][:3])\n",
    "        return self.op.apply(A, self.weight, self.epsilon)\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,initialization_f = None,device = torch.device(\"cuda:0\")):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.device = device\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_dim, in_dim), requires_grad = True)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim,1), requires_grad = True)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_dim) + ',' \\\n",
    "               + str(self.out_dim) + ')'\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / self.weight.size(1) ** 1 / 2\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, X, noise = None):\n",
    "        if noise is None or noise == 0:\n",
    "            return torch.matmul(self.weight, X) + self.bias\n",
    "        else:\n",
    "            return torch.matmul(apply_gaussian_noise(self.weight, noise, device = self.device), X) + apply_gaussian_noise(self.bias, noise, device = self.device)\n",
    "\n",
    "class One_hot_net(nn.Module):\n",
    "    def __init__(self, in_dim, n_class, f_encoder, encoder_multiplier, f_initializer, epsilon, n_layers=2, layer_size_factor=[1, 5], dropout=[-1, 0.5]):\n",
    "        super(One_hot_net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.in_dim = in_dim\n",
    "        feature_len = in_dim * encoder_multiplier\n",
    "        self.feature_len = feature_len\n",
    "        self.n_layers=n_layers\n",
    "        self.layer_size_factor=layer_size_factor\n",
    "        self.dropout=dropout\n",
    "        self.epsilon = epsilon\n",
    "        self.n_class = n_class\n",
    "        self.f_encoder = f_encoder\n",
    "        self.f_initializer = f_initializer\n",
    "        for i in range(n_layers):\n",
    "            if dropout[i] > 0:\n",
    "                self.layers.append(nn.Dropout(dropout[i]))\n",
    "            if i < n_layers - 1:\n",
    "                self.layers.append(\n",
    "                    One_hot_layer(int(feature_len // layer_size_factor[i]), int(feature_len // layer_size_factor[i + 1]), f_initializer, epsilon))\n",
    "        #self.tail = nn.Linear(int(feature_len // layer_size_factor[-1]), n_class)\n",
    "        self.tail = Linear(int(feature_len // layer_size_factor[-1]), n_class, f_initializer)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    \n",
    "    def flatten(self, X):\n",
    "        return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        X = self.f_encoder.apply_wnoise(X, sd = 0.2)\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        #X = torch.transpose(X, 0, 1)\n",
    "        #X = apply_binary_noise(X, 0.05)\n",
    "        X = self.tail(X, noise = 0.2)\n",
    "        return self.out(X).T\n",
    "\n",
    "class toy_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_Net, self).__init__()\n",
    "        self.fc2 = Linear(784, 10)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    def flatten(self, X):\n",
    "            return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = x.T\n",
    "        x = (x > -0.4).float()\n",
    "        x = self.fc2(x, noise = 0)\n",
    "        return self.out(x).T\n",
    "    \n",
    "class toy_Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_Net2, self).__init__()\n",
    "        self.fc1 = Linear(784, 20*784)\n",
    "        self.fc2 = Linear(20*784, 10)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    def flatten(self, X):\n",
    "            return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = x.T\n",
    "        x = self.fc1(x, noise = 0.5)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x, noise = 0.5)\n",
    "        return self.out(x).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise \n",
    "def apply_gaussian_noise(tensor, sd, device = torch.device(\"cuda:0\")):\n",
    "    tensor = tensor + (sd)*torch.randn(*tuple(tensor.shape)).to(device)\n",
    "    return tensor\n",
    "\n",
    "def apply_binary_noise(tensor, p, device = torch.device(\"cuda:0\")):\n",
    "    #change 1 entries to 0\n",
    "    tensor = tensor * (torch.rand(*tuple(tensor.shape))>(p/2)).float().to(device)\n",
    "    #change 0 entries to 1\n",
    "    tensor = tensor + (torch.rand(*tuple(tensor.shape))>(1-p/2)).float().to(device)\n",
    "    return torch.sign(tensor)\n",
    "\n",
    "#a = [[1,1,0,0,1,1], [1,1,0,0,1,1], [1,1,0,0,1,1]]\n",
    "#a = torch.tensor(a)\n",
    "#print((torch.rand(*tuple(a.shape))<(0.1/2)).float())\n",
    "#print(a)\n",
    "#print(apply_binary_noise(a, 0.1, device = torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializers and encoders\n",
    "def uniform_initializer(out_dim, in_dim, cuda = True):\n",
    "    tensor = torch.empty(out_dim, in_dim)\n",
    "    if cuda:\n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2).cuda()\n",
    "    else: \n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2)\n",
    "\n",
    "class simple_encoder():\n",
    "    def __init__(self, out_dim, in_dim):\n",
    "        self.W = uniform_initializer(out_dim, in_dim)\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return torch.matmul(self.W, X)\n",
    "    \n",
    "class simple_encoder_wthreshold():\n",
    "    def __init__(self, out_dim, in_dim, epsilon, cuda = True):\n",
    "        self.W = uniform_initializer(out_dim, in_dim, cuda)\n",
    "        self.device = torch.device(\"cuda:0\") if cuda else torch.device(\"cpu\")\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def apply(self, X):\n",
    "        return (torch.matmul(self.W, X) > self.epsilon).float()\n",
    "    \n",
    "    def apply_wnoise(self, X, sd):\n",
    "        return (torch.matmul(apply_gaussian_noise(self.W, sd, device = self.device), X) > self.epsilon).float()\n",
    "    \n",
    "class non_linear_encoder():\n",
    "    def __init__(self, out_dim, in_dim, activation, cuda = True):\n",
    "        self.W = uniform_initializer(out_dim, in_dim, cuda)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return self.activation((torch.matmul(self.W, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'in_dim': 784,\n",
    "    'n_class': 10,\n",
    "    'f_encoder': simple_encoder_wthreshold(784*80, 784, 10e-3),\n",
    "    'f_initializer': uniform_initializer,\n",
    "    'encoder_multiplier': 80,\n",
    "    'epsilon': 10e-3,\n",
    "    'n_layers': 1,\n",
    "    'layer_size_factor': [1, 1, 1],\n",
    "    'dropout': [0.3, -1, -1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "#model1 = One_hot_net(parameters['in_dim'], parameters['n_class'], parameters['f_encoder'], parameters['encoder_multiplier'], \n",
    "#                     parameters['f_initializer'], parameters['epsilon'], parameters['n_layers'], \n",
    "#                     parameters['layer_size_factor'], parameters['dropout']).to(device)\n",
    "\n",
    "model1 = toy_Net2().to(device)\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.01, momentum=0.5)\n",
    "#optimizer1 = torch.optim.SGD([{'params': model1.layers.parameters(), 'lr': 0.01}, {'params': model1.tail.parameters(), 'lr': 0.01}], lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                    [-1, 2]          56,520\n",
      "            Linear-2                    [-1, 2]             730\n",
      "        LogSoftmax-3                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 57,250\n",
      "Trainable params: 57,250\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.22\n",
      "Estimated Total Size (MB): 0.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model1, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, trainloader, log_interval = 10, device = torch.device(\"cuda:0\")):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "            (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            #torch.save(model.state_dict(), '/results/model.pth')\n",
    "            #torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device = torch.device(\"cuda:0\")):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 62.7936, Accuracy: 1086/10000 (10.9%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 45.820541\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 48.085632\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 53.666580\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 43.834476\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 37.548420\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 32.133640\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 29.875734\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 32.027988\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 27.911472\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 23.107382\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 21.889639\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 19.337582\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 15.622774\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 14.972243\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 11.298779\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 10.413835\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 11.609274\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 8.459884\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 10.808119\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 10.948913\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 6.958870\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 10.055821\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 7.298030\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 8.131430\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 7.682780\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 7.077633\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 8.606327\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 7.283227\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 5.224351\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 7.095794\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 6.671908\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 4.722512\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 6.371491\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 4.674207\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 5.709067\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 6.858888\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 5.191422\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 4.817240\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 5.006537\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 5.439433\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 5.422617\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 6.788370\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 6.409854\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 4.138201\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 3.941838\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 4.631424\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 3.799614\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 5.514397\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 5.472655\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 3.950732\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 4.602509\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 3.554852\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 5.048273\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 3.423590\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 5.858028\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 5.849730\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 3.292275\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 4.332659\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 3.375153\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 3.055953\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 3.863908\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 3.727139\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 3.192494\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 3.677365\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 3.420738\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 3.961748\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 4.920538\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 2.685087\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 2.887520\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 4.518157\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 3.625937\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 3.082061\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 3.407963\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 4.928150\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 3.576350\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 3.433133\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 2.847335\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 4.781867\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 3.268386\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 3.422116\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.988788\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 3.384251\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 3.807051\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 3.662724\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 2.958357\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 3.829714\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 3.478868\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 3.082939\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 4.278083\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 4.626495\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.764651\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 4.211495\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 3.154533\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 2.697258\n",
      "\n",
      "Test set: Avg. loss: 3.0590, Accuracy: 986/10000 (9.9%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 4.613541\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 2.625809\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 2.835247\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 3.217539\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 2.800884\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 2.938618\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 3.789666\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 3.512350\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 2.409589\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 3.237125\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 3.478390\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 3.177892\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 2.985518\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 3.061274\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 2.978717\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 2.697922\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 2.612949\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 2.657674\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 3.185176\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 2.903421\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 4.128161\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 2.566556\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 3.515087\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 2.496068\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 2.942445\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 2.824153\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 4.680542\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 3.113462\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 2.685587\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 2.633326\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.873288\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 3.077636\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 3.941104\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 3.040840\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 2.368563\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 2.645738\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 2.721925\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 2.718327\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 2.477326\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 2.411235\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.596597\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 2.783455\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 2.823164\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 3.114660\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 3.086382\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 2.751450\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 2.512838\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 3.532272\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 3.427873\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 2.393664\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 3.081555\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 2.959896\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 3.197744\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 2.548226\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 3.099343\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 3.018651\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 2.867172\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 2.582571\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 2.532666\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 2.527403\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 3.104158\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 2.661745\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 2.943092\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 2.705760\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 2.624815\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 2.968393\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 2.793271\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 2.902452\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 2.955460\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 2.726895\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.912848\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 2.523098\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 2.705937\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 2.796086\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 2.629689\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 3.142482\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 2.592669\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 2.692241\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 2.573770\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 2.554319\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.844694\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 2.870070\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 2.865297\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 2.695663\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 3.065986\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 2.965037\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 2.480684\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 3.064145\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 3.202006\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 2.527773\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.696651\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 3.138147\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 3.052299\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 2.452313\n",
      "\n",
      "Test set: Avg. loss: 2.7287, Accuracy: 1041/10000 (10.4%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 3.038607\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 2.746391\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 3.055004\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 2.837453\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 2.480811\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 2.827549\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 3.045888\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 3.144878\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 2.512195\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 2.822536\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.945066\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 2.534660\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 2.344068\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 3.211074\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 3.123477\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 3.285862\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 2.367848\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 2.741512\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 2.923704\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 2.592072\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.825831\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 2.733073\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 2.457601\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 4.361818\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 3.070184\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 2.802731\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 2.671978\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 2.511549\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 2.656073\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 2.929718\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.862328\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 2.916789\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 2.579818\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 2.541752\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 2.714842\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 2.400697\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 2.419902\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 2.939641\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 2.955780\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 3.082213\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.751885\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 2.734040\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 2.548970\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 2.680946\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 2.897764\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 2.629826\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 2.815083\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 2.630406\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 2.863011\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 2.627737\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.592057\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 2.653372\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 2.892121\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 2.686852\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 2.642593\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 2.719061\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 2.486657\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 2.947008\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 2.932433\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 2.432665\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.882582\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 2.733761\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 2.619508\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 2.435259\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 2.897314\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 2.601510\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 2.425347\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 2.574046\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 2.668251\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 2.843875\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 3.767184\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 3.137154\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 2.716383\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 2.578186\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 2.847256\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 2.467463\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 2.434198\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 2.773122\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 2.871812\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 2.462265\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.675948\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 2.573535\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 3.040959\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 2.476367\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 2.654170\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 2.924447\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 2.788970\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 2.370487\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 2.452691\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 2.837251\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.303910\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 2.522561\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 2.854224\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 2.377801\n",
      "\n",
      "Test set: Avg. loss: 2.6245, Accuracy: 1033/10000 (10.3%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.702612\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 2.510345\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 2.457036\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 2.607810\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 2.975141\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 2.632656\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 2.439111\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 2.479754\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 2.529013\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 2.511172\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.547383\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 2.821296\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 2.495167\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 2.444427\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 2.581792\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 2.737774\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 2.261420\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 3.087031\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 2.473325\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 3.112579\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.435895\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 2.377656\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 2.401836\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 2.351167\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 2.593285\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 2.720428\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 2.756149\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 2.824643\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 2.704784\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 2.734733\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.702250\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 2.852515\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 2.991774\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 2.474599\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 2.553367\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 2.381993\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 2.460191\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 2.972363\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 2.512173\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 2.566071\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.487077\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 2.686264\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 2.520204\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 2.776541\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 2.339682\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 2.468776\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 2.979550\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 2.567616\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 2.342400\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 2.528542\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.792655\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 2.556098\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 2.998455\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 2.672626\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 3.014523\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 2.603955\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 2.306593\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 2.492311\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 2.377185\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 2.578798\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.427986\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 2.478535\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 2.921809\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 2.388638\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 3.035388\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 2.604265\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 2.539487\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 3.381922\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 2.698690\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 2.360245\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.268186\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 2.597269\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 2.382788\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 2.533916\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 2.476089\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 2.433204\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 2.948020\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 2.949098\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 2.284433\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 2.526882\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.504837\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 2.530946\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 2.556672\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 2.917482\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 2.280632\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 2.483269\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 2.521861\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 2.479680\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 2.572834\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 2.467351\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.369040\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 2.706482\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 2.682817\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 2.363940\n",
      "\n",
      "Test set: Avg. loss: 2.6109, Accuracy: 957/10000 (9.6%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.475419\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 3.736515\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 2.641925\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 2.617792\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 2.465923\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 2.621877\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 2.675206\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 2.665573\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 2.504140\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 2.703049\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.691151\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 2.560874\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 2.441859\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 2.730237\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 2.617585\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 2.542436\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 2.599601\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 2.392235\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 2.553144\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 2.407505\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.649319\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 2.570146\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 2.566243\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 2.347546\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 2.462441\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 2.545209\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 2.282942\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 2.469170\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 2.326202\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 2.986356\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.599745\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 2.510570\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 2.451264\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 2.366941\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 2.337653\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 2.593859\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 2.722226\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 3.070410\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 2.614222\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 2.402391\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 3.179876\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 2.684166\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 2.437228\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 2.537287\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 2.577624\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 2.529319\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 2.531307\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 2.412735\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 2.294916\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 2.372530\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.372593\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 2.350879\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 2.894262\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 2.368578\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 2.542276\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 2.726412\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 2.536857\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 2.551029\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 2.695084\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 2.495229\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.632838\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 2.455868\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 2.428786\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 2.411681\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 2.408353\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 2.790860\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 2.457053\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 2.375060\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 2.412162\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 2.419568\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 2.477440\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 2.492130\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 2.398979\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 2.349362\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 2.399763\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 2.647429\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 2.518362\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 2.456737\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 2.392792\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 2.585451\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.429134\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 2.517695\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 2.689595\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 2.385418\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 2.635630\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 2.452744\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 2.554369\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 2.583906\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 2.581929\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 2.610175\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 2.455998\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 2.394638\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 2.585093\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 2.674909\n",
      "\n",
      "Test set: Avg. loss: 2.5030, Accuracy: 1050/10000 (10.5%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.376200\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 2.379297\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 2.432397\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 2.500481\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 2.592275\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 2.551417\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 2.533123\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 2.446367\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 2.623902\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 2.509022\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 3.155078\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 2.731965\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 3.415183\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 2.438504\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 2.434213\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 2.380476\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 2.528999\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 2.574826\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 2.586936\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 2.480876\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 2.417890\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 2.475728\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 2.388766\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 2.511978\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 2.356600\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 2.630997\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 2.469746\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 2.620918\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 3.117417\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 2.341970\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 2.382775\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 2.577116\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 2.477320\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 3.129517\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 3.153328\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 2.412791\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 2.434130\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 2.631907\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 2.725200\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 2.460259\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 2.563513\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 2.428110\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 2.570983\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 2.491526\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 3.007853\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 2.556241\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 2.671729\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 2.647148\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 2.368253\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 2.528991\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 2.531372\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 2.640696\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 2.450174\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 2.706208\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 2.645993\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 2.376035\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 2.718815\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 2.514533\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 2.628828\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 2.389098\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 2.662854\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 2.448128\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 2.568163\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 2.446121\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 2.440555\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 2.459783\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 2.613240\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 2.467594\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 2.387644\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 2.278099\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 2.321805\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 2.407222\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 2.835922\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 2.492518\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 2.541392\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 2.549992\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 2.530475\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 2.483681\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 2.578446\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 2.736761\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.764297\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 2.249736\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 2.435062\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 2.543944\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 2.305419\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 2.968184\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 2.535098\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 2.495111\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 2.597599\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 2.427050\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 2.593590\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 2.388029\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 2.416983\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 2.442205\n",
      "\n",
      "Test set: Avg. loss: 2.5042, Accuracy: 1006/10000 (10.1%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.439492\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 2.784931\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 2.524046\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 2.405366\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 2.599253\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 2.632341\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 2.652550\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 2.360295\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 2.533678\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 2.709881\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 2.619713\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 2.543594\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 2.450624\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 2.498085\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 2.474535\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 2.698367\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 2.399560\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 2.703118\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 2.682776\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 2.520131\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 2.501467\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 2.658923\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 2.749924\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 2.731478\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 2.394607\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 2.401878\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 2.409872\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 2.429516\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 2.337730\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 2.482173\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 2.485710\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 2.432874\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 2.594894\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 2.380777\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 2.814636\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 2.595625\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 2.368289\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 2.393685\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 2.833428\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 2.342091\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 2.391932\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 2.521162\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 2.468400\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 2.444781\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 2.487069\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 2.572504\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 2.904046\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 2.303451\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 2.761448\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 2.487051\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 2.564824\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 2.332682\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 2.448182\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 2.443486\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 2.557779\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 2.357209\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 2.377822\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 2.506074\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 2.346718\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 2.533282\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 2.446430\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 2.540578\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 2.624691\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 2.354104\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 2.678586\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 2.593198\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 2.501756\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 2.458622\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 2.679909\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 2.399050\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 2.520934\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 2.496959\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 2.637630\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 2.369376\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 2.411964\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 2.484383\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 2.290673\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 2.422148\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 2.578686\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 2.256316\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 2.511888\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 2.315471\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 2.454937\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 2.552463\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 2.405251\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 2.524317\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 2.335623\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 2.517849\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 2.380237\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 2.371332\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 2.425652\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 2.410504\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 2.406110\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 2.561291\n",
      "\n",
      "Test set: Avg. loss: 2.4956, Accuracy: 976/10000 (9.8%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.352509\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 2.910397\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 2.395577\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 2.562816\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 2.326875\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 2.350654\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 2.340527\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 2.680076\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 2.543809\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 2.374413\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 2.257794\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 2.895805\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 2.590925\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 2.284868\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 2.912777\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 2.427490\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 2.334982\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 2.290781\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 2.417443\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 2.402616\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 2.458982\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 2.374911\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 2.381357\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 2.480123\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 2.272800\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 2.420892\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 2.280216\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 2.454694\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 2.682057\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 2.462739\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 2.532677\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 2.600854\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 2.452026\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 2.574461\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 2.563506\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 2.389318\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 2.389193\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 2.492014\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 2.512678\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 2.429267\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 2.380981\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 2.483977\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 2.595351\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 2.430544\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 2.779988\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 2.387695\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 2.570174\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 2.463192\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 2.482456\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 2.687891\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 2.312395\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 2.891966\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 2.374777\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 2.400112\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 2.994428\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 2.465636\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 2.377550\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 2.618759\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 2.593096\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 2.347624\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 2.482633\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 2.560469\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 2.820414\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 2.417046\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 2.377650\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 2.692712\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 2.699221\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 2.618254\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 2.423286\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 2.697056\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 2.493639\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 2.414929\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 2.483349\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 2.529255\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 2.556543\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 2.438003\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 2.418943\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 2.496655\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 2.855463\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 2.371192\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 2.652574\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 2.386986\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 2.923584\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 2.608638\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 2.506303\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 2.399254\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 2.479588\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 2.411242\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 2.403595\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 2.442809\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 2.584522\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 2.475166\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 2.334899\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 2.427201\n",
      "\n",
      "Test set: Avg. loss: 2.5203, Accuracy: 1000/10000 (10.0%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.739918\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 2.362763\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 2.422360\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 2.453427\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 2.310586\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 2.367890\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 2.809865\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 2.410113\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 2.447297\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 2.457403\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 2.489017\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 2.478140\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 2.387045\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 2.359905\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 2.379960\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 2.322409\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 2.419356\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 2.429085\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 2.409246\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 2.509763\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 2.438912\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 2.574603\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 2.597502\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 2.478523\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 2.539289\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 2.301875\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 2.498010\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 2.361064\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 2.468817\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 2.388588\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 2.399436\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 2.426928\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 2.550151\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 2.297772\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 2.489868\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 2.337866\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 2.448671\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 2.413904\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 2.414605\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 2.281243\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 2.427757\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 2.301146\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 2.401392\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 2.423820\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 2.603089\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 2.339115\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 2.531955\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 2.637536\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 2.396610\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 2.372887\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 2.318215\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 2.374982\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 2.374876\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 2.616558\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 2.406249\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 2.536615\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 2.560309\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 3.045069\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 2.441945\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 2.816145\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 2.367195\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 2.407204\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 2.340155\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 2.375154\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 2.453610\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 2.307658\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 2.311286\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 2.531110\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 2.355568\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 2.321173\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 2.736708\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 2.415615\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 2.404549\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 2.344083\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 2.528211\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 2.461420\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 2.721090\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 2.428874\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 2.410478\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 2.267267\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 2.572704\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 2.514040\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 2.337073\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 2.502244\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 2.272118\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 2.827124\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 2.337835\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 2.649569\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 2.567695\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 2.541385\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 2.822011\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 2.388878\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 2.347057\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 2.487905\n",
      "\n",
      "Test set: Avg. loss: 2.4721, Accuracy: 1067/10000 (10.7%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 2.421137\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 2.523226\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 2.707345\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 2.570844\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 2.486168\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 2.246244\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 2.325456\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 2.468650\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 2.363690\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 2.460084\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 2.705605\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 2.309877\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 2.431884\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 2.452554\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 2.369980\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 2.621455\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 2.394469\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 2.447877\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 2.384259\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 2.534087\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 2.431442\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 2.395508\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 2.435557\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 2.327174\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 2.365718\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 2.606768\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 2.308419\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 2.310390\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 2.390034\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 2.445434\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 2.642419\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 2.451116\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 2.390070\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 2.579035\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 2.476641\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 2.506002\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 2.459788\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 2.450702\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 2.326146\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 2.332553\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 2.476621\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 2.380349\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 2.415919\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 2.421368\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 2.401968\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 2.797820\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 2.347262\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 2.527657\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 2.410965\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 2.314742\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 2.310387\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 2.417737\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 2.432998\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 2.524173\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 2.396981\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 3.108028\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 2.502169\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 2.524935\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 2.573530\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 2.491117\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 2.333760\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 2.618725\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 2.511343\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 2.473522\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 2.696344\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 2.408139\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 2.410683\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 2.495872\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 2.349876\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 2.364061\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 2.376158\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 2.494747\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 2.295021\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 2.439115\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 2.603185\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 2.376893\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 2.436088\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 2.421304\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 2.418780\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 2.529038\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 2.395417\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 2.337485\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 2.590675\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 2.371019\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 2.422968\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 2.370904\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 2.478141\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 2.389435\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 2.433803\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 2.527764\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 2.643399\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 2.414452\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 2.431561\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 2.880020\n",
      "\n",
      "Test set: Avg. loss: 2.4786, Accuracy: 1038/10000 (10.4%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 2.322611\n",
      "Train Epoch: 11 [640/60000 (1%)]\tLoss: 2.283530\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 2.440852\n",
      "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 2.451292\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 2.381946\n",
      "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 2.392678\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 2.400758\n",
      "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 2.386067\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 2.587729\n",
      "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 2.410706\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 2.491043\n",
      "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 2.367856\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 2.616190\n",
      "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 2.417224\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 2.525375\n",
      "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 2.372975\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 2.405897\n",
      "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 2.474190\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 2.520891\n",
      "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 2.754741\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 2.825823\n",
      "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 2.411677\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 2.738196\n",
      "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 2.515857\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 2.807693\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 2.550603\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 2.440039\n",
      "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 2.428898\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 2.467041\n",
      "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 2.409464\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 2.285594\n",
      "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 2.660711\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 2.535103\n",
      "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 2.339257\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 2.407143\n",
      "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 2.582307\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 2.565721\n",
      "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 2.418422\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 2.480351\n",
      "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 2.829248\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 2.344050\n",
      "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 2.398063\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 2.371240\n",
      "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 2.517595\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 2.445877\n",
      "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 2.445708\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 2.399924\n",
      "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 2.505307\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 2.797215\n",
      "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 2.633710\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 2.507571\n",
      "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 2.558249\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 2.492895\n",
      "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 2.431146\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 2.340379\n",
      "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 2.542581\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 2.365973\n",
      "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 2.355199\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 2.357537\n",
      "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 2.383715\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 2.686827\n",
      "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 2.339051\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 3.008785\n",
      "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 2.418508\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 3.060835\n",
      "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 2.467768\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 2.410637\n",
      "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 2.471129\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 2.460567\n",
      "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 2.557851\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 2.343055\n",
      "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 2.594051\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 2.419848\n",
      "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 2.345493\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 2.480000\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 2.484060\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 2.464566\n",
      "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 2.419361\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 2.804439\n",
      "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 2.388944\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 2.298276\n",
      "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 2.498217\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 2.382324\n",
      "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 2.388505\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 2.341198\n",
      "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 2.298078\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 2.303020\n",
      "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 2.445551\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 2.367015\n",
      "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 2.349111\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 2.494325\n",
      "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 2.571964\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 2.358352\n",
      "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 2.487641\n",
      "\n",
      "Test set: Avg. loss: 2.4699, Accuracy: 1069/10000 (10.7%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 2.493528\n",
      "Train Epoch: 12 [640/60000 (1%)]\tLoss: 2.445608\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 2.402028\n",
      "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 2.409512\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 2.664546\n",
      "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 2.658557\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 2.381187\n",
      "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 2.438777\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 2.494547\n",
      "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 2.709749\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 2.466453\n",
      "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 2.467930\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 2.423877\n",
      "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 2.574997\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 2.468309\n",
      "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 2.661310\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 2.430020\n",
      "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 2.450190\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 2.436055\n",
      "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 2.553241\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 2.422404\n",
      "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 2.313983\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 2.440846\n",
      "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 2.411108\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 2.494051\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 2.520267\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 2.373582\n",
      "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 2.380703\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 2.672730\n",
      "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 2.401209\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 2.597754\n",
      "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 2.542779\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 2.351847\n",
      "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 2.208739\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 2.300992\n",
      "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 2.552079\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 2.768681\n",
      "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 2.393612\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 2.564929\n",
      "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 2.416049\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 2.405571\n",
      "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 2.959039\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 2.350660\n",
      "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 2.299634\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 2.350630\n",
      "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 2.312438\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 2.359342\n",
      "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 2.472479\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 2.383340\n",
      "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 2.439332\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 2.396421\n",
      "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 2.383630\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 2.403377\n",
      "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 2.392745\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 2.405301\n",
      "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 2.465098\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 2.313069\n",
      "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 2.332558\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 2.354572\n",
      "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 2.360419\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 2.276045\n",
      "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 2.361755\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 2.307806\n",
      "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 2.492813\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 2.528394\n",
      "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 2.454641\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 2.485088\n",
      "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 2.519349\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 2.544293\n",
      "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 2.403044\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 2.458461\n",
      "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 2.321113\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 2.472998\n",
      "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 2.383028\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 2.598080\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 2.495556\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 2.489560\n",
      "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 2.320445\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 2.981654\n",
      "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 2.329048\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 2.473516\n",
      "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 2.414821\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 2.408944\n",
      "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 2.375439\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 2.494168\n",
      "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 2.489382\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 2.579124\n",
      "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 2.478945\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 2.633359\n",
      "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 2.630963\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 2.673391\n",
      "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 2.418229\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 2.370824\n",
      "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 2.524824\n",
      "\n",
      "Test set: Avg. loss: 2.4484, Accuracy: 1018/10000 (10.2%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 2.431385\n",
      "Train Epoch: 13 [640/60000 (1%)]\tLoss: 2.354671\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 2.520440\n",
      "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 2.605054\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 2.389022\n",
      "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 2.289859\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 2.576568\n",
      "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 2.360869\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 2.760725\n",
      "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 2.654681\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 2.381690\n",
      "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 2.554010\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 2.372320\n",
      "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 2.662692\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 2.352914\n",
      "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 2.432429\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 2.427781\n",
      "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 2.402246\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 2.272930\n",
      "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 2.411706\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 2.583675\n",
      "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 2.454101\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 2.351099\n",
      "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 2.419383\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 2.400938\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 2.524359\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 2.508733\n",
      "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 2.394527\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 2.342973\n",
      "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 2.459101\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 2.378522\n",
      "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 2.493406\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 2.500489\n",
      "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 2.541918\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 2.538426\n",
      "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 2.338436\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 2.449057\n",
      "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 2.353377\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 2.484648\n",
      "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 2.523031\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 2.461765\n",
      "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 2.465624\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 2.485541\n",
      "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 2.411055\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 2.317655\n",
      "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 2.492966\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 2.415247\n",
      "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 2.484089\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 2.449668\n",
      "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 2.476327\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 2.460097\n",
      "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 2.462838\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 2.681941\n",
      "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 2.448694\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 2.406943\n",
      "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 2.381541\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 2.598346\n",
      "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 2.489487\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 2.514094\n",
      "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 2.362497\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 2.378466\n",
      "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 2.469951\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 2.630051\n",
      "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 2.345926\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 2.451109\n",
      "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 2.770656\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 2.578098\n",
      "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 2.545069\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 2.843522\n",
      "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 2.395539\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 2.670989\n",
      "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 2.512027\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 2.509922\n",
      "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 2.480624\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 2.313053\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 2.728538\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 2.437380\n",
      "Train Epoch: 13 [49280/60000 (82%)]\tLoss: 2.722296\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 2.371934\n",
      "Train Epoch: 13 [50560/60000 (84%)]\tLoss: 2.391009\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 2.455112\n",
      "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 2.479959\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 2.508345\n",
      "Train Epoch: 13 [53120/60000 (88%)]\tLoss: 2.462512\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 2.531843\n",
      "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 2.333806\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 2.433251\n",
      "Train Epoch: 13 [55680/60000 (93%)]\tLoss: 2.454118\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 2.687268\n",
      "Train Epoch: 13 [56960/60000 (95%)]\tLoss: 2.316788\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 2.970609\n",
      "Train Epoch: 13 [58240/60000 (97%)]\tLoss: 2.506076\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 2.351177\n",
      "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 2.505294\n",
      "\n",
      "Test set: Avg. loss: 2.4568, Accuracy: 1021/10000 (10.2%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 2.488821\n",
      "Train Epoch: 14 [640/60000 (1%)]\tLoss: 2.424338\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 2.447166\n",
      "Train Epoch: 14 [1920/60000 (3%)]\tLoss: 2.358390\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 2.604036\n",
      "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 2.361363\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 2.449440\n",
      "Train Epoch: 14 [4480/60000 (7%)]\tLoss: 2.495438\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 2.382523\n",
      "Train Epoch: 14 [5760/60000 (10%)]\tLoss: 2.515105\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 2.508331\n",
      "Train Epoch: 14 [7040/60000 (12%)]\tLoss: 2.540054\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 2.524743\n",
      "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 2.475401\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 2.405037\n",
      "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 2.537184\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 2.429873\n",
      "Train Epoch: 14 [10880/60000 (18%)]\tLoss: 2.450086\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 2.523380\n",
      "Train Epoch: 14 [12160/60000 (20%)]\tLoss: 2.353703\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 2.398377\n",
      "Train Epoch: 14 [13440/60000 (22%)]\tLoss: 2.445275\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 2.490194\n",
      "Train Epoch: 14 [14720/60000 (25%)]\tLoss: 2.320374\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 2.439235\n",
      "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 2.496567\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 2.557791\n",
      "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 2.562512\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 2.325897\n",
      "Train Epoch: 14 [18560/60000 (31%)]\tLoss: 2.466998\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 2.419731\n",
      "Train Epoch: 14 [19840/60000 (33%)]\tLoss: 2.341960\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 2.429791\n",
      "Train Epoch: 14 [21120/60000 (35%)]\tLoss: 2.339407\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 2.461372\n",
      "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 2.339863\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 2.424831\n",
      "Train Epoch: 14 [23680/60000 (39%)]\tLoss: 2.380477\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 2.398171\n",
      "Train Epoch: 14 [24960/60000 (42%)]\tLoss: 2.360354\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 2.405031\n",
      "Train Epoch: 14 [26240/60000 (44%)]\tLoss: 2.427096\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 2.422412\n",
      "Train Epoch: 14 [27520/60000 (46%)]\tLoss: 2.447100\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 2.539015\n",
      "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 2.449170\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 2.267877\n",
      "Train Epoch: 14 [30080/60000 (50%)]\tLoss: 2.541819\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 2.448280\n",
      "Train Epoch: 14 [31360/60000 (52%)]\tLoss: 2.338429\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 2.413964\n",
      "Train Epoch: 14 [32640/60000 (54%)]\tLoss: 2.425136\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 2.428658\n",
      "Train Epoch: 14 [33920/60000 (57%)]\tLoss: 2.318016\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 2.393801\n",
      "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 2.503178\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 2.451584\n",
      "Train Epoch: 14 [36480/60000 (61%)]\tLoss: 2.350056\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 2.397324\n",
      "Train Epoch: 14 [37760/60000 (63%)]\tLoss: 2.498641\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 2.600782\n",
      "Train Epoch: 14 [39040/60000 (65%)]\tLoss: 2.429950\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 2.547878\n",
      "Train Epoch: 14 [40320/60000 (67%)]\tLoss: 2.431463\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 2.496423\n",
      "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 2.393650\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 2.438212\n",
      "Train Epoch: 14 [42880/60000 (71%)]\tLoss: 2.440683\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 2.361055\n",
      "Train Epoch: 14 [44160/60000 (74%)]\tLoss: 2.492184\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 2.638725\n",
      "Train Epoch: 14 [45440/60000 (76%)]\tLoss: 2.436121\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 2.333914\n",
      "Train Epoch: 14 [46720/60000 (78%)]\tLoss: 2.384882\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 2.324983\n",
      "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 2.348475\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 2.475890\n",
      "Train Epoch: 14 [49280/60000 (82%)]\tLoss: 2.360009\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 2.531074\n",
      "Train Epoch: 14 [50560/60000 (84%)]\tLoss: 2.328154\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 2.507016\n",
      "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 2.441650\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 2.498765\n",
      "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 2.464118\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 2.554762\n",
      "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 2.418112\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 2.380016\n",
      "Train Epoch: 14 [55680/60000 (93%)]\tLoss: 2.300455\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 2.452587\n",
      "Train Epoch: 14 [56960/60000 (95%)]\tLoss: 2.335901\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 2.358550\n",
      "Train Epoch: 14 [58240/60000 (97%)]\tLoss: 2.684852\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 2.494307\n",
      "Train Epoch: 14 [59520/60000 (99%)]\tLoss: 2.410386\n",
      "\n",
      "Test set: Avg. loss: 2.4717, Accuracy: 959/10000 (9.6%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 2.434669\n",
      "Train Epoch: 15 [640/60000 (1%)]\tLoss: 2.873274\n",
      "Train Epoch: 15 [1280/60000 (2%)]\tLoss: 2.376368\n",
      "Train Epoch: 15 [1920/60000 (3%)]\tLoss: 2.450353\n",
      "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 2.406600\n",
      "Train Epoch: 15 [3200/60000 (5%)]\tLoss: 2.614206\n",
      "Train Epoch: 15 [3840/60000 (6%)]\tLoss: 2.354712\n",
      "Train Epoch: 15 [4480/60000 (7%)]\tLoss: 2.420844\n",
      "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 2.452221\n",
      "Train Epoch: 15 [5760/60000 (10%)]\tLoss: 2.655265\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 2.729955\n",
      "Train Epoch: 15 [7040/60000 (12%)]\tLoss: 2.587186\n",
      "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 2.473779\n",
      "Train Epoch: 15 [8320/60000 (14%)]\tLoss: 2.440395\n",
      "Train Epoch: 15 [8960/60000 (15%)]\tLoss: 2.500143\n",
      "Train Epoch: 15 [9600/60000 (16%)]\tLoss: 2.361604\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 2.319956\n",
      "Train Epoch: 15 [10880/60000 (18%)]\tLoss: 2.366359\n",
      "Train Epoch: 15 [11520/60000 (19%)]\tLoss: 2.406290\n",
      "Train Epoch: 15 [12160/60000 (20%)]\tLoss: 2.371935\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 2.430590\n",
      "Train Epoch: 15 [13440/60000 (22%)]\tLoss: 2.347495\n",
      "Train Epoch: 15 [14080/60000 (23%)]\tLoss: 2.398269\n",
      "Train Epoch: 15 [14720/60000 (25%)]\tLoss: 2.508582\n",
      "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 2.330499\n",
      "Train Epoch: 15 [16000/60000 (27%)]\tLoss: 2.504896\n",
      "Train Epoch: 15 [16640/60000 (28%)]\tLoss: 2.466934\n",
      "Train Epoch: 15 [17280/60000 (29%)]\tLoss: 2.331588\n",
      "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 2.539046\n",
      "Train Epoch: 15 [18560/60000 (31%)]\tLoss: 2.465771\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 2.421997\n",
      "Train Epoch: 15 [19840/60000 (33%)]\tLoss: 2.374529\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 2.304290\n",
      "Train Epoch: 15 [21120/60000 (35%)]\tLoss: 2.649715\n",
      "Train Epoch: 15 [21760/60000 (36%)]\tLoss: 2.448528\n",
      "Train Epoch: 15 [22400/60000 (37%)]\tLoss: 2.329822\n",
      "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 2.594542\n",
      "Train Epoch: 15 [23680/60000 (39%)]\tLoss: 2.569259\n",
      "Train Epoch: 15 [24320/60000 (41%)]\tLoss: 2.389084\n",
      "Train Epoch: 15 [24960/60000 (42%)]\tLoss: 2.274035\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 2.446687\n",
      "Train Epoch: 15 [26240/60000 (44%)]\tLoss: 2.457753\n",
      "Train Epoch: 15 [26880/60000 (45%)]\tLoss: 2.297002\n",
      "Train Epoch: 15 [27520/60000 (46%)]\tLoss: 2.336775\n",
      "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 2.384260\n",
      "Train Epoch: 15 [28800/60000 (48%)]\tLoss: 2.298759\n",
      "Train Epoch: 15 [29440/60000 (49%)]\tLoss: 2.416025\n",
      "Train Epoch: 15 [30080/60000 (50%)]\tLoss: 2.395099\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 2.395394\n",
      "Train Epoch: 15 [31360/60000 (52%)]\tLoss: 2.491011\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 2.431167\n",
      "Train Epoch: 15 [32640/60000 (54%)]\tLoss: 2.354695\n",
      "Train Epoch: 15 [33280/60000 (55%)]\tLoss: 2.727239\n",
      "Train Epoch: 15 [33920/60000 (57%)]\tLoss: 2.738623\n",
      "Train Epoch: 15 [34560/60000 (58%)]\tLoss: 2.338322\n",
      "Train Epoch: 15 [35200/60000 (59%)]\tLoss: 2.342157\n",
      "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 2.592005\n",
      "Train Epoch: 15 [36480/60000 (61%)]\tLoss: 2.399403\n",
      "Train Epoch: 15 [37120/60000 (62%)]\tLoss: 2.349761\n",
      "Train Epoch: 15 [37760/60000 (63%)]\tLoss: 2.416634\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 2.634286\n",
      "Train Epoch: 15 [39040/60000 (65%)]\tLoss: 2.403274\n",
      "Train Epoch: 15 [39680/60000 (66%)]\tLoss: 2.338826\n",
      "Train Epoch: 15 [40320/60000 (67%)]\tLoss: 2.396368\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 2.559409\n",
      "Train Epoch: 15 [41600/60000 (69%)]\tLoss: 2.455463\n",
      "Train Epoch: 15 [42240/60000 (70%)]\tLoss: 2.383296\n",
      "Train Epoch: 15 [42880/60000 (71%)]\tLoss: 2.484701\n",
      "Train Epoch: 15 [43520/60000 (72%)]\tLoss: 2.410331\n",
      "Train Epoch: 15 [44160/60000 (74%)]\tLoss: 2.446772\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 2.476050\n",
      "Train Epoch: 15 [45440/60000 (76%)]\tLoss: 2.547541\n",
      "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 2.651561\n",
      "Train Epoch: 15 [46720/60000 (78%)]\tLoss: 2.424634\n",
      "Train Epoch: 15 [47360/60000 (79%)]\tLoss: 2.426921\n",
      "Train Epoch: 15 [48000/60000 (80%)]\tLoss: 2.522356\n",
      "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 2.334349\n",
      "Train Epoch: 15 [49280/60000 (82%)]\tLoss: 2.317200\n",
      "Train Epoch: 15 [49920/60000 (83%)]\tLoss: 2.459478\n",
      "Train Epoch: 15 [50560/60000 (84%)]\tLoss: 2.341033\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 2.396019\n",
      "Train Epoch: 15 [51840/60000 (86%)]\tLoss: 2.330796\n",
      "Train Epoch: 15 [52480/60000 (87%)]\tLoss: 2.313337\n",
      "Train Epoch: 15 [53120/60000 (88%)]\tLoss: 2.387078\n",
      "Train Epoch: 15 [53760/60000 (90%)]\tLoss: 2.416472\n",
      "Train Epoch: 15 [54400/60000 (91%)]\tLoss: 2.458553\n",
      "Train Epoch: 15 [55040/60000 (92%)]\tLoss: 2.413476\n",
      "Train Epoch: 15 [55680/60000 (93%)]\tLoss: 2.447475\n",
      "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 2.445777\n",
      "Train Epoch: 15 [56960/60000 (95%)]\tLoss: 2.489333\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 2.488573\n",
      "Train Epoch: 15 [58240/60000 (97%)]\tLoss: 2.364036\n",
      "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 2.448039\n",
      "Train Epoch: 15 [59520/60000 (99%)]\tLoss: 2.685849\n",
      "\n",
      "Test set: Avg. loss: 2.4403, Accuracy: 1052/10000 (10.5%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"base_net1\"\n",
    "acc_max = test(model1, test_loader)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch, model1, optimizer1, train_loader)\n",
    "    acc = test(model1, test_loader)\n",
    "    if acc > acc_max:\n",
    "        acc = acc_max\n",
    "        print()\n",
    "        print(\"--- saving model for best accuracy ---\")\n",
    "        #torch.save(model1.state_dict(), 'results/'+model_name+'.pth')\n",
    "        #torch.save(optimizer1.state_dict(), 'results/'+model_name+'_optimizer.pth')\n",
    "        print(\"--- saving finished ---\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'negative log likelihood loss')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model1.state_dict(), 'results/base_model.pth')\n",
    "#torch.save(optimizer1.state_dict(), 'results/base_optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
