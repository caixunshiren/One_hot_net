{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d019e5e1d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "batch_size_train = 64\n",
    "batch_size_test = 500\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network flowchart:\n",
    "#input vector --- random initialized weights ---> hyperdimensional vecotor ---> one_hot_net l1 ---> ...ln... ---> sigmoid/softmax output\n",
    "#One hot net\n",
    "\n",
    "class One_hot_op(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, W, epsilon):\n",
    "        Z = torch.matmul(W, A)\n",
    "        ctx.Z = Z\n",
    "        ctx.A = A\n",
    "        ctx.W = W\n",
    "        ctx.epsilon = epsilon\n",
    "        ret = Z > epsilon\n",
    "        #print(ret[1:10][1:10])\n",
    "        return ret.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dL_dA):\n",
    "        step = ctx.Z > ctx.epsilon\n",
    "        step = step.float()\n",
    "        dL_dZ = dL_dA * step \n",
    "        \n",
    "        dZ_dW = torch.transpose(ctx.A, 0,1)\n",
    "        dZ_dW = torch.sign(dZ_dW)\n",
    "        dZ_dA = torch.transpose(ctx.W, 0,1)\n",
    "        dZ_dA = torch.sign(dZ_dA)\n",
    "        dA = torch.matmul(dZ_dA,dL_dZ)\n",
    "        dW = torch.matmul(dL_dZ,dZ_dW)\n",
    "        return dA, dW, None\n",
    "\n",
    "\n",
    "class One_hot_layer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, initialization_f, epsilon):\n",
    "        super(One_hot_layer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.initialization_f = initialization_f\n",
    "        initialized_weight = initialization_f(out_dim, in_dim)\n",
    "        self.weight = nn.Parameter(initialized_weight, requires_grad = True)\n",
    "        self.op = One_hot_op\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_dim) + ',' \\\n",
    "               + str(self.out_dim) + ')'\n",
    "        \n",
    "    def forward(self, A):\n",
    "        #print(self.weight[:3][:3])\n",
    "        return self.op.apply(A, self.weight, self.epsilon)\n",
    "\n",
    "class Linear_noise_op(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, W, noise):\n",
    "        ctx.noise = noise\n",
    "        return W\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dW):\n",
    "        #print(\"go\")\n",
    "        #print(dW.shape)\n",
    "        return apply_gaussian_noise(dW, ctx.noise, device = dW.device), None\n",
    "    \n",
    "class Linear(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,initialization_f = None,device = torch.device(\"cuda:0\")):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.device = device\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_dim, in_dim), requires_grad = True)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim,1), requires_grad = True)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_dim) + ',' \\\n",
    "               + str(self.out_dim) + ')'\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / self.weight.size(1) ** 1 / 2\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, X, noise = None, grad_noise = None):\n",
    "        #print(self.__class__.__name__, self.weight[:2,:2])\n",
    "        \n",
    "        if grad_noise is not None and grad_noise != 0:\n",
    "            W = Linear_noise_op.apply(self.weight, grad_noise)\n",
    "            B = Linear_noise_op.apply(self.bias, grad_noise)\n",
    "        else:\n",
    "            W = self.weight\n",
    "            B = self.bias\n",
    "        \n",
    "        \n",
    "        if noise is None or noise == 0:\n",
    "            return torch.matmul(W, X) + B\n",
    "        else:\n",
    "            return torch.matmul(apply_gaussian_noise(W, noise, device = self.device), X) + apply_gaussian_noise(B, noise, device = self.device)\n",
    "        '''\n",
    "        #return torch.matmul(self.weight, X) + self.bias\n",
    "        if noise is None or noise == 0:\n",
    "            return torch.matmul(self.weight, X) + self.bias\n",
    "        else:\n",
    "            return torch.matmul(apply_gaussian_noise(self.weight, noise, device = self.device), X) + apply_gaussian_noise(self.bias, noise, device = self.device)\n",
    "        '''\n",
    "class One_hot_net(nn.Module):\n",
    "    def __init__(self, in_dim, n_class, f_encoder, encoder_multiplier, f_initializer, epsilon, n_layers=2, layer_size_factor=[1, 5], dropout=[-1, 0.5]):\n",
    "        super(One_hot_net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.in_dim = in_dim\n",
    "        feature_len = int(in_dim * encoder_multiplier)\n",
    "        self.feature_len = feature_len\n",
    "        self.n_layers=n_layers\n",
    "        self.layer_size_factor=layer_size_factor\n",
    "        self.dropout=dropout\n",
    "        self.epsilon = epsilon\n",
    "        self.n_class = n_class\n",
    "        self.f_encoder = f_encoder\n",
    "        self.f_initializer = f_initializer\n",
    "        for i in range(n_layers):\n",
    "            if dropout[i] > 0:\n",
    "                self.layers.append(nn.Dropout(dropout[i]))\n",
    "            if i < n_layers - 1:\n",
    "                self.layers.append(\n",
    "                    One_hot_layer(int(feature_len // layer_size_factor[i]), int(feature_len // layer_size_factor[i + 1]), f_initializer, epsilon))\n",
    "        #self.tail = nn.Linear(int(feature_len // layer_size_factor[-1]), n_class)\n",
    "        self.tail = Linear(int(feature_len // layer_size_factor[-1]), n_class, f_initializer)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    \n",
    "    def flatten(self, X):\n",
    "        return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        #X = self.f_encoder.apply_wnoise(X, sd = 0.5)\n",
    "        X = self.f_encoder.apply(X)\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        #X = torch.transpose(X, 0, 1)\n",
    "        #X = apply_binary_noise(X, 0.05)\n",
    "        X = self.tail(X, noise = 0, grad_noise = 0)\n",
    "        return self.out(X).T\n",
    "\n",
    "class toy_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_Net, self).__init__()\n",
    "        self.fc2 = Linear(784, 10)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    def flatten(self, X):\n",
    "            return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = x.T\n",
    "        x = (x > -0.4).float()\n",
    "        x = self.fc2(x, noise = 0)\n",
    "        return self.out(x).T\n",
    "    \n",
    "class toy_Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_Net2, self).__init__()\n",
    "        self.fc1 = Linear(784, 300)\n",
    "        self.fc2 = Linear(300, 10)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    def flatten(self, X):\n",
    "            return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = x.T\n",
    "        x = self.fc1(x, noise = 0, grad_noise = 0)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x, noise = 0, grad_noise = 0)\n",
    "        return self.out(x).T\n",
    "    \n",
    "class toy_Net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_Net3, self).__init__()\n",
    "        self.fc1 = Linear(784, 400)\n",
    "        self.fc2 = Linear(400, 100)\n",
    "        self.fc3 = Linear(100, 10)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    def flatten(self, X):\n",
    "            return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = x.T\n",
    "        x = self.fc1(x, noise = 0.5, grad_noise = 1)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x, noise = 0.5, grad_noise = 1)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc3(x, noise = 0.5, grad_noise = 1)\n",
    "        return self.out(x).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise \n",
    "def apply_gaussian_noise(tensor, sd, device = torch.device(\"cuda:0\")):\n",
    "    tensor = tensor + (sd)*torch.randn(*tuple(tensor.shape)).to(device)\n",
    "    return tensor\n",
    "\n",
    "def apply_binary_noise(tensor, p, device = torch.device(\"cuda:0\")):\n",
    "    #change 1 entries to 0\n",
    "    tensor = tensor * (torch.rand(*tuple(tensor.shape))>(p/2)).float().to(device)\n",
    "    #change 0 entries to 1\n",
    "    tensor = tensor + (torch.rand(*tuple(tensor.shape))>(1-p/2)).float().to(device)\n",
    "    return torch.sign(tensor)\n",
    "\n",
    "#a = [[1,1,0,0,1,1], [1,1,0,0,1,1], [1,1,0,0,1,1]]\n",
    "#a = torch.tensor(a)\n",
    "#print((torch.rand(*tuple(a.shape))<(0.1/2)).float())\n",
    "#print(a)\n",
    "#print(apply_binary_noise(a, 0.1, device = torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializers and encoders\n",
    "def uniform_initializer(out_dim, in_dim, cuda = True):\n",
    "    tensor = torch.empty(out_dim, in_dim)\n",
    "    if cuda:\n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2).cuda()\n",
    "    else: \n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2)\n",
    "\n",
    "class simple_encoder():\n",
    "    def __init__(self, out_dim, in_dim):\n",
    "        self.W = uniform_initializer(out_dim, in_dim)\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return torch.matmul(self.W, X)\n",
    "    \n",
    "class simple_encoder_wthreshold():\n",
    "    def __init__(self, out_dim, in_dim, epsilon, cuda = True):\n",
    "        self.W = uniform_initializer(out_dim, in_dim, cuda)\n",
    "        self.device = torch.device(\"cuda:0\") if cuda else torch.device(\"cpu\")\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def apply(self, X):\n",
    "        return (torch.matmul(self.W, X) > self.epsilon).float()\n",
    "    \n",
    "    def apply_wnoise(self, X, sd):\n",
    "        return (torch.matmul(apply_gaussian_noise(self.W, sd, device = self.device), X) > self.epsilon).float()\n",
    "    \n",
    "class non_linear_encoder():\n",
    "    def __init__(self, out_dim, in_dim, activation, cuda = True):\n",
    "        self.W = uniform_initializer(out_dim, in_dim, cuda)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return self.activation((torch.matmul(self.W, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'in_dim': 784,\n",
    "    'n_class': 10,\n",
    "    'f_encoder': simple_encoder_wthreshold(784*1, 784, 10e-3),\n",
    "    'f_initializer': uniform_initializer,\n",
    "    'encoder_multiplier': 1,\n",
    "    'epsilon': 10e-3,\n",
    "    'n_layers': 1,\n",
    "    'layer_size_factor': [1, 1, 1],\n",
    "    'dropout': [0.3, -1, -1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model1 = One_hot_net(parameters['in_dim'], parameters['n_class'], parameters['f_encoder'], parameters['encoder_multiplier'], parameters['f_initializer'], parameters['epsilon'], parameters['n_layers'], parameters['layer_size_factor'], parameters['dropout']).to(device)\n",
    "#model1 = toy_Net2().to(device)\n",
    "#model1 = toy_Net3().to(device)\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.01, momentum=0.5)\n",
    "#optimizer1 = torch.optim.SGD([{'params': model1.layers.parameters(), 'lr': 0.01}, {'params': model1.tail.parameters(), 'lr': 0.01}], lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Dropout-1                    [-1, 2]               0\n",
      "            Linear-2                    [-1, 2]           7,850\n",
      "        LogSoftmax-3                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model1, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, trainloader, log_interval = 10, device = torch.device(\"cuda:0\")):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "            (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            #torch.save(model.state_dict(), '/results/model.pth')\n",
    "            #torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device = torch.device(\"cuda:0\")):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3033, Accuracy: 937/10000 (9.4%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304966\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.944821\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.788110\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.533559\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.374284\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.102466\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.123170\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.074926\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.196562\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.012167\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.819501\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.889710\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.794213\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.777416\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.864224\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.911075\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.824466\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.658250\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.598782\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.586383\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.708013\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.505804\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.555255\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.650280\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.727917\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.712891\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.647472\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.549852\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.613549\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.560482\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.486445\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.568743\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.454632\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.675438\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.463406\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.551997\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.549259\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.440270\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.489263\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.545238\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.474906\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.523058\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.680070\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.373885\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.509946\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.527897\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.464222\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.515701\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.662612\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.576941\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.625194\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.526379\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.442870\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.497276\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.430863\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.500181\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.451683\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.554524\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.562467\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.548842\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.671852\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.497095\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.345764\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.292187\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.457343\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.457554\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.580769\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.522424\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.441804\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.428799\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.614401\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.384082\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.456868\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.536314\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.517596\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.657389\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.421432\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.650341\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.443322\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.724611\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.534331\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.581708\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.468476\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.516764\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.511356\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.392899\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.335705\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.299065\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.406580\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.608619\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.451041\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.382402\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.461390\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.262502\n",
      "\n",
      "Test set: Avg. loss: 0.3873, Accuracy: 8965/10000 (89.7%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.480444\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.434889\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.537820\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.440433\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.362041\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.462308\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.312705\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.571858\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.336301\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.395438\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.400262\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.507211\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.514743\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.499966\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.517305\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.472543\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.379464\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.396820\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.599663\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.460079\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.552754\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.383968\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.446254\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.431726\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.447020\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.310948\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.373313\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.344843\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.453227\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.364821\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.461587\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.433274\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.480011\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.414463\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.445659\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.514743\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.425262\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.444642\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.357273\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.433613\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.367068\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.342204\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.422051\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.424935\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.603606\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.365885\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.429087\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.381051\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.325040\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.498999\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.563791\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.473260\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.338905\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.460775\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.370596\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.392309\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.415002\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.243339\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.402816\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.406796\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.324600\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.317672\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.347462\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.453192\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.551516\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.496166\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.467719\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.438855\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.354428\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.454123\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.445561\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.184518\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.404840\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.362012\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.401415\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.379779\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.482924\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.448950\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.327711\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.367347\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.257206\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.254128\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.486475\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.271787\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.540461\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.489463\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.447621\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.353420\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.497011\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.402603\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.566634\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.399101\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.380678\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.243782\n",
      "\n",
      "Test set: Avg. loss: 0.3338, Accuracy: 9067/10000 (90.7%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.553228\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.372323\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.337408\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.310656\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.338032\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.302917\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.433000\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.293896\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.236389\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.256739\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.462773\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.489484\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.525392\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.512261\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.369905\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.361282\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.453590\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.386732\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.264314\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.593549\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.294225\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.454547\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.420635\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.522432\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.354571\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.272652\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.467845\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.425091\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.350757\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.603013\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.491832\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.428480\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.577236\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.554554\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.276448\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.458338\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.344351\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.257381\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.315250\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.467374\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.503065\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.432493\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.398114\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.380041\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.357999\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.449226\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.484919\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.373837\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.434293\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.380017\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.316566\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.370471\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.388486\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.364577\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.272718\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.499134\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.669653\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.360878\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.364741\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.348053\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.582767\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.201483\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.364838\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.477578\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.364964\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.408375\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.290003\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.267467\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.368730\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.392052\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.364346\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.536960\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.506684\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.491330\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.353383\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.291009\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.418722\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.316630\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.403857\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.388419\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.397175\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.306912\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.415317\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.331013\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.269331\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.335173\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.222167\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.605457\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.454510\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.504129\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.352177\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.495598\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.264810\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.285079\n",
      "\n",
      "Test set: Avg. loss: 0.3090, Accuracy: 9143/10000 (91.4%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.494724\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.336537\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.266210\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.505111\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.375104\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.380612\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.474033\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.332437\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.338774\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.326345\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.650453\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.368961\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.438076\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.265195\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.490548\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.343720\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.415170\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.327680\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.293358\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.341492\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.334457\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.363406\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.190723\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.580211\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.355661\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.353119\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.495199\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.254737\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.547368\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.161368\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.426828\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.234075\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.325505\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.218597\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.284142\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.331693\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.348708\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.268921\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.463098\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.448961\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.323637\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.266763\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.222924\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.330515\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.255956\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.365548\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.418887\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.395521\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.372584\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.332131\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.408145\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.435919\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.275996\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.466639\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.226042\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.258444\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.322828\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.369377\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.572714\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.447677\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.381687\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.426096\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.334381\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.306479\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.361396\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.465148\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.393389\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.306561\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.408459\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.490340\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.425572\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.402114\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.313586\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.341210\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.259234\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.304864\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.482517\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.453269\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.486024\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.200109\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.353542\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.295763\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.179006\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.374987\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.316416\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.253444\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.291973\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.292325\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.309029\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.304082\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.450239\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.313814\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.287060\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.589312\n",
      "\n",
      "Test set: Avg. loss: 0.2951, Accuracy: 9163/10000 (91.6%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.162968\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.337217\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.285258\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.416371\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.318762\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.291927\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.339036\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.307344\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.429721\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.314941\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.533382\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.329320\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.286024\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.347652\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.405636\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.407485\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.295719\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.276389\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.321730\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.281588\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.402665\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.496945\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.710174\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.395377\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.221073\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.307327\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.601819\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.493968\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.378748\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.430676\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.280652\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.336847\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.330738\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.332349\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.340435\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.164473\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.369459\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.513579\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.391706\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.409485\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.709114\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.377692\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.303479\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.274802\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.347542\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.416862\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.318507\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.332514\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.401643\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.411543\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.281111\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.373691\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.305027\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.580144\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.336837\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.451249\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.353492\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.380117\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.293290\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.243503\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.190802\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.285154\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.382477\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.295625\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.324889\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.351793\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.539760\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.286397\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.335367\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.408173\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.438221\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.379475\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.508211\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.307393\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.232941\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.417174\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.273587\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.288715\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.341827\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.602032\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.383224\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.432520\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.342318\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.321706\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.427130\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.371543\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.323116\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.313765\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.421279\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.373272\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.415963\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.272037\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.290577\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.184410\n",
      "\n",
      "Test set: Avg. loss: 0.2866, Accuracy: 9160/10000 (91.6%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.211010\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.487277\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.497950\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.220147\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.342870\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.270453\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.358638\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.361695\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.334718\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.453648\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.303232\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.267029\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.386691\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.371912\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.419466\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.425225\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.649087\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.361844\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.462578\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.571625\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.282006\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.472107\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.463553\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.277383\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.412292\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.444558\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.283302\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.407449\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.317119\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.276405\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.307817\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.317494\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.303782\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.282820\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.412946\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.417081\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.356878\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.276031\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.443464\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.280589\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.455306\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.281349\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.293842\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.463557\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.331240\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.327664\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.186860\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.514033\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.207993\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.509404\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.467556\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.389018\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.219616\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.336550\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.450042\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.333580\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.235710\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.351339\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.333494\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.280771\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.392566\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.522129\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.336373\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.312561\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.484230\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.404690\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.253703\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.438567\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.291643\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.421613\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.356345\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.471670\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.313978\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.494477\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.579339\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.401501\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.283197\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.318412\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.264041\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.311182\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.230013\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.330982\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.229969\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.319250\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.434585\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.445827\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.231428\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.264475\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.376268\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.281030\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.291508\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.332339\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.214146\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.365091\n",
      "\n",
      "Test set: Avg. loss: 0.2822, Accuracy: 9190/10000 (91.9%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.526920\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.511203\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.249525\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.450286\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.258437\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.382747\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.445073\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.302600\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.481404\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.295905\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.321237\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.382864\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.320449\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.326403\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.629999\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.451123\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.418270\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.407361\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.462965\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.206702\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.410228\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.399716\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.384874\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.378655\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.322772\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.361372\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.427112\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.227307\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.241772\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.204749\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.431150\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.421856\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.401036\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.308380\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.452899\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.271683\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.389045\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.196615\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.275057\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.444785\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.368006\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.439355\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.421711\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.346785\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.254655\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.322137\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.604654\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.442790\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.315490\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.304893\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.241976\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.249417\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.490602\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.418344\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.472974\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.181234\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.224881\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.510208\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.292576\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.253334\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.404015\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.277350\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.297816\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.275264\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.305331\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.298798\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.315530\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.321088\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.469696\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.286679\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.316486\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.299296\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.340871\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.301431\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.397559\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.285935\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.081173\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.363477\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.247249\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.386839\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.320454\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.311141\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.363032\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.313628\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.291099\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.264249\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.437459\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.403549\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.312549\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.275951\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.527824\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.323506\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.361449\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.363067\n",
      "\n",
      "Test set: Avg. loss: 0.2794, Accuracy: 9194/10000 (91.9%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.295025\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.413835\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.211188\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.404073\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.357504\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.159039\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.305910\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.150658\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.331560\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.361107\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.407921\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.219638\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.484357\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.320465\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.440467\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.344911\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.449290\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.291203\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.456186\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.241131\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.345060\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.481834\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.432000\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.312215\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.289593\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.422815\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.309779\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.306530\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.432982\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.387565\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.510241\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.370155\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.336486\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.187427\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.355237\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.279183\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.334339\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.322973\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.278340\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.315591\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.282369\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.338308\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.418090\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.443494\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.382201\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.312148\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.403153\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.465750\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.310744\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.376149\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.254208\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.322450\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.465198\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.274165\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.398694\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.447768\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.267955\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.351129\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.337984\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.361436\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.269187\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.485496\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.455846\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.403040\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.358105\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.373801\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.301332\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.293090\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.353485\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.424759\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.267947\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.292995\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.393300\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.258606\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.235867\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.334002\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.174097\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.291826\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.217218\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.499212\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.244501\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.293207\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.540967\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.281799\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.529841\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.216827\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.538822\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.235987\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.307102\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.433829\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.434737\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.295143\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.192327\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.308825\n",
      "\n",
      "Test set: Avg. loss: 0.2749, Accuracy: 9199/10000 (92.0%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.272033\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.289839\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.168096\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.414395\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.315702\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.311687\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.284252\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.236441\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.414054\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.412037\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.595790\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.313059\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.215272\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.254353\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.259327\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.279840\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.516214\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.354267\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.503012\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.343549\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.365527\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.407200\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.221426\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.497508\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.351361\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.421902\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.391681\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.269508\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.401238\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.347198\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.299726\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.291802\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.301760\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.269109\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.381711\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.277800\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.350783\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.313420\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.406278\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.521351\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.382002\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.317596\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.262953\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.385654\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.352504\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.254489\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.489408\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.611098\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.262906\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.271874\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.236428\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.357037\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.260586\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.412428\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.229894\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.522377\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.349266\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.395396\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.497458\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.247571\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.613727\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.226319\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.321627\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.524667\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.381849\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.320634\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.192744\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.416957\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.245407\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.239406\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.370211\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.282601\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.382081\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.220573\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.445222\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.479943\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.391677\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.448702\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.262170\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.377230\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.291461\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.288632\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.432536\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.389790\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.341423\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.258198\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.205802\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.261450\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.315799\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.364299\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.484760\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.406782\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.251011\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.198505\n",
      "\n",
      "Test set: Avg. loss: 0.2724, Accuracy: 9190/10000 (91.9%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.223760\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.321146\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.301305\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.256156\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.261192\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.320221\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.345446\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.274348\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.346745\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.210794\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.191809\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.283010\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.270305\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.463511\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.427935\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.352477\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.576034\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.315120\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.187300\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.305500\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.271970\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.423865\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.337327\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.336517\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.332566\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.305850\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.633952\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.358102\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.210104\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.330074\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.488399\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.434457\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.185031\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.280352\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.300768\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.286589\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.463942\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.470291\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.277118\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.468541\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.475522\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.432016\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.349696\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.254151\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.353414\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.476329\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.370808\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.374873\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.184152\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.285482\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.362884\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.204668\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.268909\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.250819\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.296216\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.296617\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.255206\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.237741\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.236638\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.239117\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.344928\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.481226\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.353965\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.517037\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.245602\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.529050\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.460618\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.281566\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.331000\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.338749\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.322196\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.303859\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.339020\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.295008\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.422030\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.406102\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.327937\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.364193\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.504966\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.177297\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.310930\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.199390\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.282008\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.271988\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.367168\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.499365\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.242775\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.426920\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.379833\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.445623\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.400804\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.373606\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.358652\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.299511\n",
      "\n",
      "Test set: Avg. loss: 0.2673, Accuracy: 9216/10000 (92.2%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.419124\n",
      "Train Epoch: 11 [640/60000 (1%)]\tLoss: 0.398154\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.248760\n",
      "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.228162\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.484700\n",
      "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.416639\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.282847\n",
      "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 0.254398\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.356939\n",
      "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 0.224518\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.287581\n",
      "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 0.367085\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.396182\n",
      "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.488125\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.270643\n",
      "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.192184\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.352897\n",
      "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 0.274673\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.587357\n",
      "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.275185\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.187911\n",
      "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.414518\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.313625\n",
      "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.307204\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.324810\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.300306\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.251740\n",
      "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.324921\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.501196\n",
      "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 0.166426\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.350855\n",
      "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 0.200473\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.463434\n",
      "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 0.253413\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.370090\n",
      "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.370767\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.491277\n",
      "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.303867\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.336924\n",
      "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 0.367798\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.383303\n",
      "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 0.363774\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.361290\n",
      "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 0.375358\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.352619\n",
      "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.301182\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.189471\n",
      "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 0.432387\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.368591\n",
      "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.394054\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.416632\n",
      "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 0.266519\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.430149\n",
      "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.313376\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.360645\n",
      "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.305978\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.421549\n",
      "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.193718\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.231994\n",
      "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.435607\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.363735\n",
      "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.349061\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.285765\n",
      "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 0.271057\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.222224\n",
      "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.386489\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.512198\n",
      "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.432012\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.182449\n",
      "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.239021\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.323590\n",
      "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.329003\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.405931\n",
      "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 0.207147\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.493610\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.413834\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.439333\n",
      "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 0.245866\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.473798\n",
      "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.266172\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.253441\n",
      "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.304924\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.218879\n",
      "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.183652\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.213531\n",
      "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.336340\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.252698\n",
      "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 0.193801\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.362554\n",
      "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 0.313959\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.265022\n",
      "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.467528\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.215126\n",
      "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.227552\n",
      "\n",
      "Test set: Avg. loss: 0.2668, Accuracy: 9213/10000 (92.1%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.239497\n",
      "Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.177260\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.420264\n",
      "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.187901\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.362223\n",
      "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.327043\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.417768\n",
      "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.328175\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.367317\n",
      "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 0.693075\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.255655\n",
      "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.271241\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.251106\n",
      "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 0.285249\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.279076\n",
      "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.399273\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.275350\n",
      "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 0.229284\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.338751\n",
      "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 0.322169\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.297805\n",
      "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.320776\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.499682\n",
      "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.349958\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.373012\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.157609\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.462773\n",
      "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.281111\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.315503\n",
      "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 0.274706\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.446676\n",
      "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 0.273184\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.331444\n",
      "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 0.466870\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.228365\n",
      "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.283496\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.222756\n",
      "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 0.348706\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.426500\n",
      "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.451221\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.509888\n",
      "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.333943\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.294974\n",
      "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.497142\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.276468\n",
      "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.362536\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.354039\n",
      "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 0.333086\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.518285\n",
      "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 0.285314\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.445179\n",
      "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 0.290481\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.448691\n",
      "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.209915\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.427322\n",
      "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.332309\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.209570\n",
      "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 0.247629\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.295673\n",
      "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.418296\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.309404\n",
      "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.404569\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.416178\n",
      "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.264604\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.390720\n",
      "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.432997\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.209451\n",
      "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.305593\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.346490\n",
      "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 0.609776\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.318195\n",
      "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 0.293552\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.323253\n",
      "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 0.475940\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.429736\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.288636\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.296641\n",
      "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 0.269792\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.328329\n",
      "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 0.234074\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.306390\n",
      "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.527746\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.258515\n",
      "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 0.167837\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.339791\n",
      "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.431192\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.356456\n",
      "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 0.350742\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.329825\n",
      "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.406712\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.340243\n",
      "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.269332\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.238987\n",
      "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.560197\n",
      "\n",
      "Test set: Avg. loss: 0.2638, Accuracy: 9227/10000 (92.3%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.372092\n",
      "Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.525763\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.355286\n",
      "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 0.306906\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.439979\n",
      "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.230627\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.312407\n",
      "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.258010\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.535559\n",
      "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.197487\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.201972\n",
      "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.389410\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.228648\n",
      "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.206945\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.362439\n",
      "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.479529\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.302542\n",
      "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 0.353207\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.322318\n",
      "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.237404\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.273113\n",
      "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 0.434295\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.338907\n",
      "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.264544\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.382418\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.189877\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.557077\n",
      "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.234668\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.549923\n",
      "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 0.370496\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.294107\n",
      "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 0.237773\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.324890\n",
      "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.236109\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.340777\n",
      "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.180746\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.300360\n",
      "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 0.414016\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.247986\n",
      "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 0.461615\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.331524\n",
      "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.427801\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.345702\n",
      "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 0.396917\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.277968\n",
      "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.191644\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.305558\n",
      "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.293052\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.276247\n",
      "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.142124\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.473305\n",
      "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 0.300615\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.253644\n",
      "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.384095\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.218502\n",
      "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.467480\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.357261\n",
      "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 0.118980\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.413020\n",
      "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 0.641996\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.419253\n",
      "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.122425\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.325238\n",
      "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 0.358945\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.381120\n",
      "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.437238\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.240803\n",
      "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.287721\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.192764\n",
      "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.448725\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.327716\n",
      "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 0.297085\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.293521\n",
      "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.187590\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.268140\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.507884\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.253745\n",
      "Train Epoch: 13 [49280/60000 (82%)]\tLoss: 0.332604\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.190496\n",
      "Train Epoch: 13 [50560/60000 (84%)]\tLoss: 0.232233\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.416564\n",
      "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 0.391144\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.175723\n",
      "Train Epoch: 13 [53120/60000 (88%)]\tLoss: 0.229444\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.252483\n",
      "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.341530\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.305220\n",
      "Train Epoch: 13 [55680/60000 (93%)]\tLoss: 0.341772\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.743171\n",
      "Train Epoch: 13 [56960/60000 (95%)]\tLoss: 0.342701\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.302624\n",
      "Train Epoch: 13 [58240/60000 (97%)]\tLoss: 0.499053\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.184694\n",
      "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.332417\n",
      "\n",
      "Test set: Avg. loss: 0.2613, Accuracy: 9223/10000 (92.2%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.327912\n",
      "Train Epoch: 14 [640/60000 (1%)]\tLoss: 0.241440\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.223895\n",
      "Train Epoch: 14 [1920/60000 (3%)]\tLoss: 0.462410\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.310396\n",
      "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.325304\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.278921\n",
      "Train Epoch: 14 [4480/60000 (7%)]\tLoss: 0.455359\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.485737\n",
      "Train Epoch: 14 [5760/60000 (10%)]\tLoss: 0.331278\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.248861\n",
      "Train Epoch: 14 [7040/60000 (12%)]\tLoss: 0.487901\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.593122\n",
      "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 0.328267\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.295999\n",
      "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.262220\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.310382\n",
      "Train Epoch: 14 [10880/60000 (18%)]\tLoss: 0.430917\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.263807\n",
      "Train Epoch: 14 [12160/60000 (20%)]\tLoss: 0.237190\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.352259\n",
      "Train Epoch: 14 [13440/60000 (22%)]\tLoss: 0.331542\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.317391\n",
      "Train Epoch: 14 [14720/60000 (25%)]\tLoss: 0.231962\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.324964\n",
      "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.307528\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.278526\n",
      "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.180085\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.362825\n",
      "Train Epoch: 14 [18560/60000 (31%)]\tLoss: 0.393081\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.326910\n",
      "Train Epoch: 14 [19840/60000 (33%)]\tLoss: 0.509216\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.294860\n",
      "Train Epoch: 14 [21120/60000 (35%)]\tLoss: 0.335189\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.342282\n",
      "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.288340\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.209265\n",
      "Train Epoch: 14 [23680/60000 (39%)]\tLoss: 0.276407\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.460491\n",
      "Train Epoch: 14 [24960/60000 (42%)]\tLoss: 0.422662\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.438986\n",
      "Train Epoch: 14 [26240/60000 (44%)]\tLoss: 0.415474\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.315061\n",
      "Train Epoch: 14 [27520/60000 (46%)]\tLoss: 0.526028\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.258510\n",
      "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.383290\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.218527\n",
      "Train Epoch: 14 [30080/60000 (50%)]\tLoss: 0.452185\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.359287\n",
      "Train Epoch: 14 [31360/60000 (52%)]\tLoss: 0.350290\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.410377\n",
      "Train Epoch: 14 [32640/60000 (54%)]\tLoss: 0.305644\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.400433\n",
      "Train Epoch: 14 [33920/60000 (57%)]\tLoss: 0.301243\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.497084\n",
      "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.158739\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.251540\n",
      "Train Epoch: 14 [36480/60000 (61%)]\tLoss: 0.311016\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.168079\n",
      "Train Epoch: 14 [37760/60000 (63%)]\tLoss: 0.481461\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.170608\n",
      "Train Epoch: 14 [39040/60000 (65%)]\tLoss: 0.401229\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.376843\n",
      "Train Epoch: 14 [40320/60000 (67%)]\tLoss: 0.348760\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.296994\n",
      "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.188415\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.203267\n",
      "Train Epoch: 14 [42880/60000 (71%)]\tLoss: 0.295067\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.382816\n",
      "Train Epoch: 14 [44160/60000 (74%)]\tLoss: 0.292918\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.220426\n",
      "Train Epoch: 14 [45440/60000 (76%)]\tLoss: 0.287159\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.248452\n",
      "Train Epoch: 14 [46720/60000 (78%)]\tLoss: 0.289754\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.393329\n",
      "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.351966\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.278850\n",
      "Train Epoch: 14 [49280/60000 (82%)]\tLoss: 0.347209\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.219680\n",
      "Train Epoch: 14 [50560/60000 (84%)]\tLoss: 0.331061\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.572111\n",
      "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 0.558180\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.276076\n",
      "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.443107\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.200315\n",
      "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.278415\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.115596\n",
      "Train Epoch: 14 [55680/60000 (93%)]\tLoss: 0.325945\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.414181\n",
      "Train Epoch: 14 [56960/60000 (95%)]\tLoss: 0.428774\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.241686\n",
      "Train Epoch: 14 [58240/60000 (97%)]\tLoss: 0.363563\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.412003\n"
     ]
    }
   ],
   "source": [
    "model_name = \"base_net1\"\n",
    "acc_max = test(model1, test_loader)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch, model1, optimizer1, train_loader)\n",
    "    acc = test(model1, test_loader)\n",
    "    if acc > acc_max:\n",
    "        acc = acc_max\n",
    "        print()\n",
    "        print(\"--- saving model for best accuracy ---\")\n",
    "        #torch.save(model1.state_dict(), 'results/'+model_name+'.pth')\n",
    "        #torch.save(optimizer1.state_dict(), 'results/'+model_name+'_optimizer.pth')\n",
    "        print(\"--- saving finished ---\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model1.state_dict(), 'results/base_model.pth')\n",
    "#torch.save(optimizer1.state_dict(), 'results/base_optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "fig = plt.figure()\n",
    "model1.eval()\n",
    "output = model1(example_data.to(torch.device(\"cuda:0\"))).cpu().detach()\n",
    "prediction = torch.argmax(output, dim = 1)\n",
    "print(prediction)\n",
    "for i in range(20):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"G T: {} P: {}\".format(example_targets[i], prediction[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
