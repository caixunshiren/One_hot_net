{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "batch_size_train = 64\n",
    "batch_size_test = 500\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network flowchart:\n",
    "#input vector --- random initialized weights ---> hyperdimensional vecotor ---> one_hot_net l1 ---> ...ln... ---> sigmoid/softmax output\n",
    "#One hot net\n",
    "\n",
    "class One_hot_op(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, W, epsilon):\n",
    "        Z = torch.matmul(W, A)\n",
    "        ctx.Z = Z\n",
    "        ctx.A = A\n",
    "        ctx.W = W\n",
    "        ret = Z > epsilon\n",
    "        #print(ret[1:10][1:10])\n",
    "        return ret.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dL_dA):\n",
    "        step = ctx.Z > 0\n",
    "        step = step.float()\n",
    "        dL_dZ = dL_dA * step \n",
    "        \n",
    "        dZ_dW = torch.transpose(ctx.A, 0,1)\n",
    "        dZ_dW = dZ_dW / torch.abs(dZ_dW)\n",
    "        dZ_dA = torch.transpose(ctx.W, 0,1)\n",
    "        dZ_dA = dZ_dA / torch.abs(dZ_dA)\n",
    "        dA = torch.matmul(dZ_dA,dL_dZ)\n",
    "        dW = torch.matmul(dL_dZ,dZ_dW)\n",
    "        return dA, dW, None\n",
    "\n",
    "\n",
    "class One_hot_layer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, initialization_f, epsilon):\n",
    "        super(One_hot_layer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.initialization_f = initialization_f\n",
    "        initialized_weight = initialization_f(out_dim, in_dim)\n",
    "        self.weight = nn.Parameter(initialized_weight, requires_grad = True)\n",
    "        self.op = One_hot_op\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_dim) + ',' \\\n",
    "               + str(self.out_dim) + ')'\n",
    "        \n",
    "    def forward(self, A):\n",
    "        return self.op.apply(A, self.weight, self.epsilon)\n",
    "    \n",
    "\n",
    "class One_hot_net(nn.Module):\n",
    "    def __init__(self, in_dim, n_class, f_encoder, encoder_multiplier, f_initializer, epsilon, n_layers=2, layer_size_factor=[1, 5], dropout=[-1, 0.5]):\n",
    "        super(One_hot_net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.in_dim = in_dim\n",
    "        feature_len = in_dim * encoder_multiplier\n",
    "        self.feature_len = feature_len\n",
    "        self.n_layers=n_layers\n",
    "        self.layer_size_factor=layer_size_factor\n",
    "        self.dropout=dropout\n",
    "        self.n_class = n_class\n",
    "        self.f_encoder = f_encoder\n",
    "        self.f_initializer = f_initializer\n",
    "        for i in range(n_layers):\n",
    "            if dropout[i] > 0:\n",
    "                self.layers.append(nn.Dropout(dropout[i]))\n",
    "            if i < n_layers - 1:\n",
    "                self.layers.append(\n",
    "                    One_hot_layer(int(feature_len // layer_size_factor[i]), int(feature_len // layer_size_factor[i + 1]), f_initializer, epsilon))\n",
    "        self.tail = nn.Linear(int(feature_len // layer_size_factor[-1]), n_class)\n",
    "        self.out = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    def flatten(self, X):\n",
    "        return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        X = self.f_encoder.apply(X)\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        X = self.tail(X)\n",
    "        return self.out(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializers and encoders\n",
    "def uniform_initializer(out_dim, in_dim):\n",
    "    tensor = torch.empty(out_dim, in_dim)\n",
    "    return torch.nn.init.uniform_(tensor, a=-2, b=2).cuda() \n",
    "\n",
    "class simple_encoder():\n",
    "    def __init__(self, out_dim, in_dim):\n",
    "        self.W = uniform_initializer(out_dim, in_dim)\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return torch.matmul(self.W, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'in_dim': 784,\n",
    "    'n_class': 10,\n",
    "    'f_encoder': simple_encoder(784*20, 784),\n",
    "    'f_initializer': uniform_initializer,\n",
    "    'encoder_multiplier': 20,\n",
    "    'epsilon': 10e-3,\n",
    "    'n_layers': 2,\n",
    "    'layer_size_factor': [1, 1, 1],\n",
    "    'dropout': [-1, -1, -1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model1 = One_hot_net(parameters['in_dim'], parameters['n_class'], parameters['f_encoder'], parameters['encoder_multiplier'], \n",
    "                     parameters['f_initializer'], parameters['epsilon'], parameters['n_layers'], \n",
    "                     parameters['layer_size_factor'], parameters['dropout']).to(device)\n",
    "\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "     One_hot_layer-1                    [-1, 2]     553,190,400\n",
      "            Linear-2                   [-1, 10]         235,210\n",
      "        LogSoftmax-3                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 553,425,610\n",
      "Trainable params: 553,425,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 2111.15\n",
      "Estimated Total Size (MB): 2111.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model1, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, trainloader, log_interval = 10, device = torch.device(\"cuda:0\")):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "            (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            #torch.save(model.state_dict(), '/results/model.pth')\n",
    "            #torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "        return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device = torch.device(\"cuda:0\")):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caixu\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3326, Accuracy: 1091/10000 (11%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.334950\n",
      "\n",
      "Test set: Avg. loss: 6.4233, Accuracy: 1202/10000 (12%)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.06 GiB (GPU 0; 8.00 GiB total capacity; 6.26 GiB already allocated; 0 bytes free; 6.28 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-e2a27b974be7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-c778780750e0>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, model, optimizer, trainloader, log_interval, device)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\function.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# _forward_cls is defined by derived class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-fb1c4cd4744b>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(ctx, dL_dA)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mdZ_dW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdZ_dW\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ_dW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mdZ_dA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mdZ_dA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdZ_dA\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ_dA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mdA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ_dA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdL_dZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdL_dZ\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdZ_dW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.06 GiB (GPU 0; 8.00 GiB total capacity; 6.26 GiB already allocated; 0 bytes free; 6.28 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "test(model1, test_loader)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch, model1, optimizer1, train_loader)\n",
    "    test(model1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
