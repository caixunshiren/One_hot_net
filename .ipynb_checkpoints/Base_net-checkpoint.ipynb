{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23b533a11d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "random_seed = 69\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size_train = 64\n",
    "batch_size_test = 500\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network flowchart:\n",
    "#input vector --- random initialized weights ---> hyperdimensional vecotor ---> one_hot_net l1 ---> ...ln... ---> sigmoid/softmax output\n",
    "#One hot net\n",
    "\n",
    "class One_hot_op(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, W, epsilon):\n",
    "        Z = torch.matmul(W, A)\n",
    "        ctx.Z = Z\n",
    "        ctx.A = A\n",
    "        ctx.W = W\n",
    "        ctx.epsilon = epsilon\n",
    "        ret = Z > epsilon\n",
    "        #print(ret[1:10][1:10])\n",
    "        return ret.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dL_dA):\n",
    "        step = ctx.Z > ctx.epsilon\n",
    "        step = step.float()\n",
    "        dL_dZ = dL_dA * step \n",
    "        \n",
    "        dZ_dW = torch.transpose(ctx.A, 0,1)\n",
    "        dZ_dW = torch.sign(dZ_dW)\n",
    "        dZ_dA = torch.transpose(ctx.W, 0,1)\n",
    "        dZ_dA = torch.sign(dZ_dA)\n",
    "        dA = torch.matmul(dZ_dA,dL_dZ)\n",
    "        dW = torch.matmul(dL_dZ,dZ_dW)\n",
    "        return dA, dW, None\n",
    "\n",
    "\n",
    "class One_hot_layer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, initialization_f, epsilon):\n",
    "        super(One_hot_layer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.initialization_f = initialization_f\n",
    "        initialized_weight = initialization_f(out_dim, in_dim)\n",
    "        self.weight = nn.Parameter(initialized_weight, requires_grad = True)\n",
    "        self.op = One_hot_op\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_dim) + ',' \\\n",
    "               + str(self.out_dim) + ')'\n",
    "        \n",
    "    def forward(self, A):\n",
    "        #print(self.weight[:3][:3])\n",
    "        return self.op.apply(A, self.weight, self.epsilon)\n",
    "\n",
    "class Linear_noise_op(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, W, noise):\n",
    "        ctx.noise = noise\n",
    "        return W\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dW):\n",
    "        #print(\"go\")\n",
    "        #print(dW.shape)\n",
    "        return apply_gaussian_noise(dW, ctx.noise, device = dW.device), None\n",
    "    \n",
    "class Linear(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,initialization_f = None,device = torch.device(\"cuda:0\")):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.device = device\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_dim, in_dim), requires_grad = True)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim,1), requires_grad = True)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_dim) + ',' \\\n",
    "               + str(self.out_dim) + ')'\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / self.weight.size(1) ** 1 / 2\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, X, noise = None, grad_noise = None):\n",
    "        #print(self.__class__.__name__, self.weight[:2,:2])\n",
    "        \n",
    "        if grad_noise is not None and grad_noise != 0:\n",
    "            W = Linear_noise_op.apply(self.weight, grad_noise)\n",
    "            B = Linear_noise_op.apply(self.bias, grad_noise)\n",
    "        else:\n",
    "            W = self.weight\n",
    "            B = self.bias\n",
    "        \n",
    "        \n",
    "        if noise is None or noise == 0:\n",
    "            return torch.matmul(W, X) + B\n",
    "        else:\n",
    "            return torch.matmul(apply_gaussian_noise(W, noise, device = self.device), X) + apply_gaussian_noise(B, noise, device = self.device)\n",
    "        '''\n",
    "        #return torch.matmul(self.weight, X) + self.bias\n",
    "        if noise is None or noise == 0:\n",
    "            return torch.matmul(self.weight, X) + self.bias\n",
    "        else:\n",
    "            return torch.matmul(apply_gaussian_noise(self.weight, noise, device = self.device), X) + apply_gaussian_noise(self.bias, noise, device = self.device)\n",
    "        '''\n",
    "class One_hot_net(nn.Module):\n",
    "    def __init__(self, in_dim, n_class, f_encoder, encoder_multiplier, f_initializer, epsilon, n_layers=2, layer_size_factor=[1, 5], dropout=[-1, 0.5]):\n",
    "        super(One_hot_net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.in_dim = in_dim\n",
    "        feature_len = int(in_dim * encoder_multiplier)\n",
    "        self.feature_len = feature_len\n",
    "        self.n_layers=n_layers\n",
    "        self.layer_size_factor=layer_size_factor\n",
    "        self.dropout=dropout\n",
    "        self.epsilon = epsilon\n",
    "        self.n_class = n_class\n",
    "        self.f_encoder = f_encoder\n",
    "        self.f_initializer = f_initializer\n",
    "        for i in range(n_layers):\n",
    "            if dropout[i] > 0:\n",
    "                self.layers.append(nn.Dropout(dropout[i]))\n",
    "            if i < n_layers - 1:\n",
    "                self.layers.append(\n",
    "                    One_hot_layer(int(feature_len // layer_size_factor[i]), int(feature_len // layer_size_factor[i + 1]), f_initializer, epsilon))\n",
    "        #self.tail = nn.Linear(int(feature_len // layer_size_factor[-1]), n_class)\n",
    "        self.tail = Linear(int(feature_len // layer_size_factor[-1]), n_class, f_initializer)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    \n",
    "    def flatten(self, X):\n",
    "        return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        #X = self.f_encoder.apply_wnoise(X, sd = 0.5)\n",
    "        X = self.f_encoder.apply(X)\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        #X = torch.transpose(X, 0, 1)\n",
    "        #X = apply_binary_noise(X, 0.05)\n",
    "        X = self.tail(X, noise = 0, grad_noise = 0)\n",
    "        return self.out(X).T\n",
    "\n",
    "class toy_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_Net, self).__init__()\n",
    "        self.fc2 = Linear(784, 10)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    def flatten(self, X):\n",
    "            return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = x.T\n",
    "        x = (x > -0.4).float()\n",
    "        x = self.fc2(x, noise = 0)\n",
    "        return self.out(x).T\n",
    "    \n",
    "class toy_Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_Net2, self).__init__()\n",
    "        self.fc1 = Linear(784, 300)\n",
    "        self.fc2 = Linear(300, 10)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    def flatten(self, X):\n",
    "            return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = x.T\n",
    "        x = self.fc1(x, noise = 0, grad_noise = 1)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x, noise = 0, grad_noise = 1)\n",
    "        return self.out(x).T\n",
    "    \n",
    "class toy_Net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_Net3, self).__init__()\n",
    "        self.fc1 = Linear(784, 400)\n",
    "        self.fc2 = Linear(400, 100)\n",
    "        self.fc3 = Linear(100, 10)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    def flatten(self, X):\n",
    "            return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = x.T\n",
    "        x = self.fc1(x, noise = 0.5, grad_noise = 1)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x, noise = 0.5, grad_noise = 1)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc3(x, noise = 0.5, grad_noise = 1)\n",
    "        return self.out(x).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise \n",
    "def apply_gaussian_noise(tensor, sd, device = torch.device(\"cuda:0\")):\n",
    "    tensor = tensor + (sd)*torch.randn(*tuple(tensor.shape)).to(device)\n",
    "    return tensor\n",
    "\n",
    "def apply_binary_noise(tensor, p, device = torch.device(\"cuda:0\")):\n",
    "    #change 1 entries to 0\n",
    "    tensor = tensor * (torch.rand(*tuple(tensor.shape))>(p/2)).float().to(device)\n",
    "    #change 0 entries to 1\n",
    "    tensor = tensor + (torch.rand(*tuple(tensor.shape))>(1-p/2)).float().to(device)\n",
    "    return torch.sign(tensor)\n",
    "\n",
    "#a = [[1,1,0,0,1,1], [1,1,0,0,1,1], [1,1,0,0,1,1]]\n",
    "#a = torch.tensor(a)\n",
    "#print((torch.rand(*tuple(a.shape))<(0.1/2)).float())\n",
    "#print(a)\n",
    "#print(apply_binary_noise(a, 0.1, device = torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializers and encoders\n",
    "def uniform_initializer(out_dim, in_dim, cuda = True):\n",
    "    tensor = torch.empty(out_dim, in_dim)\n",
    "    if cuda:\n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2).cuda()\n",
    "    else: \n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2)\n",
    "\n",
    "class simple_encoder():\n",
    "    def __init__(self, out_dim, in_dim):\n",
    "        self.W = uniform_initializer(out_dim, in_dim)\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return torch.matmul(self.W, X)\n",
    "    \n",
    "class simple_encoder_wthreshold():\n",
    "    def __init__(self, out_dim, in_dim, epsilon, cuda = True):\n",
    "        self.W = uniform_initializer(out_dim, in_dim, cuda)\n",
    "        self.device = torch.device(\"cuda:0\") if cuda else torch.device(\"cpu\")\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def apply(self, X):\n",
    "        return (torch.matmul(self.W, X) > self.epsilon).float()\n",
    "    \n",
    "    def apply_wnoise(self, X, sd):\n",
    "        return (torch.matmul(apply_gaussian_noise(self.W, sd, device = self.device), X) > self.epsilon).float()\n",
    "    \n",
    "class non_linear_encoder():\n",
    "    def __init__(self, out_dim, in_dim, activation, cuda = True):\n",
    "        self.W = uniform_initializer(out_dim, in_dim, cuda)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return self.activation((torch.matmul(self.W, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'in_dim': 784,\n",
    "    'n_class': 10,\n",
    "    'f_encoder': simple_encoder_wthreshold(784//2, 784, 10e-3),\n",
    "    'f_initializer': uniform_initializer,\n",
    "    'encoder_multiplier': 0.5,\n",
    "    'epsilon': 10e-3,\n",
    "    'n_layers': 1,\n",
    "    'layer_size_factor': [1, 1, 1],\n",
    "    'dropout': [0.3, -1, -1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "#model1 = One_hot_net(parameters['in_dim'], parameters['n_class'], parameters['f_encoder'], parameters['encoder_multiplier'], parameters['f_initializer'], parameters['epsilon'], parameters['n_layers'], parameters['layer_size_factor'], parameters['dropout']).to(device)\n",
    "model1 = toy_Net2().to(device)\n",
    "#model1 = toy_Net3().to(device)\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.01, momentum=0.5)\n",
    "#optimizer1 = torch.optim.SGD([{'params': model1.layers.parameters(), 'lr': 0.01}, {'params': model1.tail.parameters(), 'lr': 0.01}], lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                    [-1, 2]         235,500\n",
      "            Linear-2                    [-1, 2]           3,010\n",
      "        LogSoftmax-3                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 238,510\n",
      "Trainable params: 238,510\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.91\n",
      "Estimated Total Size (MB): 0.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model1, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, trainloader, log_interval = 10, device = torch.device(\"cuda:0\")):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #print(output[:5,:])\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        #model.fc1.grad[:3,:3]\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "            (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            #torch.save(model.state_dict(), '/results/model.pth')\n",
    "            #torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device = torch.device(\"cuda:0\")):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3026, Accuracy: 914/10000 (9.1%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302616\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.172009\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.857322\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.637913\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.546280\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.461864\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.418909\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.344484\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.273917\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.197947\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.054526\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.960834\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.823658\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.672199\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.636651\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.144582\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.672448\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.242076\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.969339\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.909777\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.259945\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.603174\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.369630\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.723016\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.979724\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.186778\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.141414\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.130428\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.787797\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.693114\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.896182\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.765756\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.608702\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.899451\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.263215\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.488228\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.978978\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.058398\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.425829\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.007652\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.699845\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.792309\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.545866\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.041138\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 3.349319\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.098138\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.143245\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.991018\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.163520\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 1.968509\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.997551\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 1.454381\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 2.470092\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 3.147321\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.602059\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.952004\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 3.179362\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 3.614090\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.195049\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 2.453724\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.452560\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 2.983454\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.895729\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 2.547636\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.626223\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 3.191377\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 2.318255\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 3.076992\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 2.474653\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 2.273021\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.712733\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 3.114776\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 2.201466\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 3.028510\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 4.097667\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 2.204821\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.681256\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 2.678758\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 2.176928\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 2.589049\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 5.293499\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 2.243139\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 2.465375\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 3.262348\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.736095\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 3.552010\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 2.331087\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 2.433219\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 3.823524\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 2.602159\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.846474\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 3.190951\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 2.919435\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 2.221413\n",
      "\n",
      "Test set: Avg. loss: 2.7755, Accuracy: 8105/10000 (81.0%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.219800\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 3.389445\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 1.476411\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 2.089806\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 3.158678\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 1.880274\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 3.951522\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 4.234399\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 2.326251\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 4.260943\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.732983\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 3.928026\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 1.986440\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 2.805424\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 3.584192\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 2.804231\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 1.856227\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 4.006042\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 3.632789\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 5.670051\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.001301\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 3.763269\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 2.117181\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 2.973974\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 3.765386\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 1.546042\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 3.852231\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 5.841108\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 3.871177\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 6.791634\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 4.932041\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 2.134587\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 2.716007\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 1.716579\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 1.488220\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 6.048357\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 2.174219\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 4.123589\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 1.488653\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 6.894299\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 3.043205\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 4.197793\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 3.283073\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 3.232401\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 3.579158\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 3.720962\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 1.022362\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 3.108924\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 1.284171\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 4.823406\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 6.499786\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 5.199968\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 3.472829\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 4.038918\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 3.234913\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 9.315000\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 5.044693\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 7.262581\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 2.718909\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 1.841760\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.344058\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 3.726570\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 5.789423\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 3.610980\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 4.027150\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 2.327865\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 3.805632\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 1.899682\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 5.464073\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 2.899692\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 3.696413\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 4.389780\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 4.935306\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 2.473323\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 5.067954\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 4.361183\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 2.178784\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 2.020261\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 3.802381\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 3.452635\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.300794\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 4.125408\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 2.715230\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 6.152027\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 4.440464\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 3.454683\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 4.959591\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 2.802392\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 6.972610\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 1.342290\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 3.410961\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 2.308940\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 4.116940\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 3.297250\n",
      "\n",
      "Test set: Avg. loss: 3.7207, Accuracy: 8485/10000 (84.8%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.958452\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 5.985793\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 1.658983\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 4.309705\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 2.088608\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 1.893183\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 5.300030\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 3.573373\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 4.646870\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 6.325061\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 4.328935\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 4.573455\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 3.820357\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 6.307855\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 7.900847\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 5.776583\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 1.698517\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 3.981376\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 2.262384\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 7.128610\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 6.626914\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 4.898396\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 4.900817\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 3.548200\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 5.271889\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 5.123937\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 3.020194\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 6.497533\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 2.913244\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 9.749350\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 3.485570\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 4.370251\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 1.998759\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 2.756953\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 3.927698\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 6.301454\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 1.845581\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 2.944990\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 3.463220\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 3.893963\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.881229\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 5.424731\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 1.504071\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 5.371394\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 3.792587\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 3.980906\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 3.088164\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 5.167762\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 6.557258\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 5.517848\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 4.990348\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 3.811512\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 3.501622\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 6.449687\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 6.934013\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 2.141563\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 2.340823\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 4.661222\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 2.812840\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 5.373349\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 6.026978\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 8.536987\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 4.060497\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.717345\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 3.762763\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 3.538307\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 3.120188\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 2.954220\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 5.502744\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 6.231950\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.545026\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 6.690539\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 7.402580\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 4.035349\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 4.063026\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 3.708390\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 6.467716\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 1.651902\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 4.634947\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 3.441619\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 3.974215\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 3.350912\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 2.640555\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 7.192620\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 4.526466\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 7.525579\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 5.346438\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 5.353086\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 6.196264\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 5.228259\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 3.552092\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 3.578226\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 2.291301\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 5.908820\n",
      "\n",
      "Test set: Avg. loss: 4.7125, Accuracy: 8584/10000 (85.8%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 6.543786\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 6.670352\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 5.821324\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 3.145212\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 4.571497\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 3.777399\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 2.641085\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 2.748274\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 1.675835\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 3.462256\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 3.830784\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 3.289143\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 2.813217\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 3.662560\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.929277\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 5.464700\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 6.586747\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 4.428426\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 4.458032\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 4.726697\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 6.981831\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 8.550031\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 2.170091\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 3.379924\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 2.710723\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 4.290650\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 3.463885\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 7.403465\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 6.269962\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 5.069605\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 6.076577\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 3.539589\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 4.579626\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 3.881104\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 4.887315\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 5.539546\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.650835\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 7.341420\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 5.218211\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 4.685795\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 3.170377\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 4.792769\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 10.603909\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 8.257191\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 4.013520\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 8.115786\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 4.810014\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 9.764296\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 6.623407\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 5.046779\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 4.589093\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 5.481007\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 7.428319\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 7.074409\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 6.765246\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 5.448193\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 5.332075\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 4.351705\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 8.776539\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 3.495138\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 9.306092\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 2.395098\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 1.380894\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 5.831905\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 6.299139\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 6.150936\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 6.548730\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 5.596129\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 2.041785\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 6.290151\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 3.082612\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 6.072616\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 4.657181\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 6.211264\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 3.087816\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 3.644594\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 2.877115\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 4.326517\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 1.199198\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 6.579605\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 6.218461\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 6.301479\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 3.254144\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 8.831658\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 7.685239\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 6.777329\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 3.068521\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 4.048756\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 10.262336\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 5.706225\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 5.194719\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 7.202268\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 2.920521\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 3.469022\n",
      "\n",
      "Test set: Avg. loss: 5.2196, Accuracy: 8748/10000 (87.5%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 3.378055\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 4.463706\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 3.964365\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 5.331587\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 2.727757\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 5.846467\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 4.223302\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 8.434767\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 5.154425\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 6.473596\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 8.378458\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 6.261863\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 4.978783\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 8.496801\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 6.241723\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 3.061990\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 4.294868\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 4.205174\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 3.271047\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 3.423138\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.774610\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 6.616983\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 5.857691\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 2.634800\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 2.639028\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 4.183698\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 6.445527\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 7.473107\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 8.187754\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 3.374943\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 3.386364\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 2.399861\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 6.970520\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 7.488342\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 5.259440\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 7.131974\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 7.579641\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 7.640691\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.048950\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 4.721313\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 9.168083\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 5.639380\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 6.064315\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 7.700413\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 3.714036\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 7.499377\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 4.413498\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 2.049379\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 6.241915\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 9.880293\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 4.392717\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 4.920374\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 4.521245\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 3.659785\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 5.486990\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 8.946671\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 5.678855\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 5.764200\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 3.782718\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 9.447366\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 8.468725\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 8.181384\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 3.245925\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 8.331975\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 2.247530\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 10.203128\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 1.131397\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 8.301270\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 5.094578\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 4.501933\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 9.808516\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 6.303495\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 2.537429\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 6.411780\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 5.114516\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 6.050619\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 5.738223\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 5.373840\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 4.583329\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 5.266449\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 3.020116\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 7.419115\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 5.411012\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 7.519690\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 8.533485\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 7.241595\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 9.612948\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 5.593426\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 3.417721\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 4.353258\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 4.554621\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 4.360746\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 7.820195\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 5.299480\n",
      "\n",
      "Test set: Avg. loss: 5.2450, Accuracy: 8871/10000 (88.7%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"base_net1\"\n",
    "acc_max = test(model1, test_loader)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch, model1, optimizer1, train_loader)\n",
    "    acc = test(model1, test_loader)\n",
    "    if acc > acc_max:\n",
    "        acc = acc_max\n",
    "        print()\n",
    "        print(\"--- saving model for best accuracy ---\")\n",
    "        #torch.save(model1.state_dict(), 'results/'+model_name+'.pth')\n",
    "        #torch.save(optimizer1.state_dict(), 'results/'+model_name+'_optimizer.pth')\n",
    "        print(\"--- saving finished ---\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'negative log likelihood loss')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model1.state_dict(), 'results/base_model.pth')\n",
    "#torch.save(optimizer1.state_dict(), 'results/base_optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "fig = plt.figure()\n",
    "model1.eval()\n",
    "output = model1(example_data.to(torch.device(\"cuda:0\"))).cpu().detach()\n",
    "prediction = torch.argmax(output, dim = 1)\n",
    "print(prediction)\n",
    "for i in range(20):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"G T: {} P: {}\".format(example_targets[i], prediction[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
