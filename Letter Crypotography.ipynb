{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x229719931d0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 94\n",
    "skdim = 10\n",
    "noise = 2\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_initializer(out_dim, in_dim, cuda = True):\n",
    "    tensor = torch.empty(out_dim, in_dim)\n",
    "    if cuda:\n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2).cuda()\n",
    "    else: \n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2)\n",
    "\n",
    "def uniform_initializer2(out_dim, in_dim, cuda = True):\n",
    "    tensor = torch.empty(out_dim, in_dim)\n",
    "    if cuda:\n",
    "        return torch.nn.init.uniform_(tensor, a=-0.5, b=0.5).cuda()\n",
    "    else: \n",
    "        return torch.nn.init.uniform_(tensor, a=-0.5, b=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 94])\n"
     ]
    }
   ],
   "source": [
    "#secret keys\n",
    "#ascii 0-94\n",
    "secret_key = uniform_initializer(skdim, classes)\n",
    "print(secret_key.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise \n",
    "def apply_gaussian_noise(tensor, sd, device = torch.device(\"cuda:0\")):\n",
    "    tensor = tensor + (sd)*torch.randn(*tuple(tensor.shape)).to(device)\n",
    "    return tensor\n",
    "\n",
    "class simple_encoder_wthreshold():\n",
    "    def __init__(self, out_dim, in_dim, epsilon, cuda = True):\n",
    "        self.W = uniform_initializer(out_dim, in_dim, cuda)\n",
    "        self.device = torch.device(\"cuda:0\") if cuda else torch.device(\"cpu\")\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def apply(self, X):\n",
    "        return (torch.matmul(self.W, X) > self.epsilon).float()\n",
    "    \n",
    "    def apply_wnoise(self, X, sd):\n",
    "        #print(X.shape, self.W.shape)\n",
    "        if sd == 0:\n",
    "            return self.apply(X)\n",
    "        return (torch.matmul(apply_gaussian_noise(self.W, sd, device = self.device), X) > self.epsilon).float()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot net    \n",
    "\n",
    "class One_hot_net(nn.Module):\n",
    "    def __init__(self, in_dim, n_class, f_encoder, encoder_multiplier, f_initializer, epsilon):\n",
    "        super(One_hot_net, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        feature_len = in_dim * encoder_multiplier\n",
    "        self.feature_len = feature_len\n",
    "        self.epsilon = epsilon\n",
    "        self.n_class = n_class\n",
    "        self.f_encoder = f_encoder\n",
    "        self.f_initializer = f_initializer\n",
    "        self.tail = nn.Linear(feature_len, n_class)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.f_encoder.apply_wnoise(X, noise)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        X = self.tail(X)\n",
    "        return X.transpose(1,0)\n",
    "    \n",
    "    def decrypt(self, X):\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        X = self.tail(X)\n",
    "        return X.transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'in_dim': 10,\n",
    "    'n_class': 10,\n",
    "    'f_encoder': simple_encoder_wthreshold(1000, 10, 0),\n",
    "    'f_initializer': uniform_initializer,\n",
    "    'encoder_multiplier': 100,\n",
    "    'epsilon': 10e-3,\n",
    "    'n_layers': 1,\n",
    "    'layer_size_factor': [1],\n",
    "    'dropout': [-1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model1 = One_hot_net(parameters['in_dim'], parameters['n_class'], parameters['f_encoder'], parameters['encoder_multiplier'], \n",
    "                     parameters['f_initializer'], parameters['epsilon']).to(device)\n",
    "\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.005, momentum=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 500])\n"
     ]
    }
   ],
   "source": [
    "#create training and testing set\n",
    "def make_data(size):\n",
    "    data = torch.rand(size) * classes\n",
    "    return data.long()\n",
    "\n",
    "n_epochs = 100\n",
    "n_batch = 200\n",
    "batch_size_train = 500\n",
    "batch_size_test = 500\n",
    "\n",
    "train_loader = [secret_key[:, make_data(500)] for i in range(n_batch)]\n",
    "test_loader = [secret_key[:, make_data(500)] for i in range(n_batch)]\n",
    "\n",
    "print(train_loader[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, trainloader, log_interval = 10, device = torch.device(\"cuda:0\")):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = nn.MSELoss()(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(float(train_loss))\n",
    "    print('Epoch: {}, Train set: Avg. loss: {:.6f}'.format(epoch,\n",
    "        train_loss))\n",
    "    return model, optimizer\n",
    "def test(model, test_loader, device = torch.device(\"cuda:0\")):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += nn.MSELoss()(output, data).item()\n",
    "    test_loss /= len(test_loader)\n",
    "    test_losses.append(float(test_loss))\n",
    "    #print(output[:,0]-data[:,0])\n",
    "    print('Test set: Avg. loss: {:.6f}'.format(\n",
    "        test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train set: Avg. loss: 0.518317\n",
      "Test set: Avg. loss: 0.198323\n",
      "Epoch: 2, Train set: Avg. loss: 0.160629\n",
      "Test set: Avg. loss: 0.140666\n",
      "Epoch: 3, Train set: Avg. loss: 0.135653\n",
      "Test set: Avg. loss: 0.130570\n",
      "Epoch: 4, Train set: Avg. loss: 0.126191\n",
      "Test set: Avg. loss: 0.123714\n",
      "Epoch: 5, Train set: Avg. loss: 0.120896\n",
      "Test set: Avg. loss: 0.119684\n",
      "Epoch: 6, Train set: Avg. loss: 0.116661\n",
      "Test set: Avg. loss: 0.114497\n",
      "Epoch: 7, Train set: Avg. loss: 0.112103\n",
      "Test set: Avg. loss: 0.109015\n",
      "Epoch: 8, Train set: Avg. loss: 0.107045\n",
      "Test set: Avg. loss: 0.105983\n",
      "Epoch: 9, Train set: Avg. loss: 0.105503\n",
      "Test set: Avg. loss: 0.104672\n",
      "Epoch: 10, Train set: Avg. loss: 0.102252\n",
      "Test set: Avg. loss: 0.100265\n",
      "Epoch: 11, Train set: Avg. loss: 0.099806\n",
      "Test set: Avg. loss: 0.098326\n",
      "Epoch: 12, Train set: Avg. loss: 0.098636\n",
      "Test set: Avg. loss: 0.096718\n",
      "Epoch: 13, Train set: Avg. loss: 0.096929\n",
      "Test set: Avg. loss: 0.095227\n",
      "Epoch: 14, Train set: Avg. loss: 0.095274\n",
      "Test set: Avg. loss: 0.093749\n",
      "Epoch: 15, Train set: Avg. loss: 0.094348\n",
      "Test set: Avg. loss: 0.093725\n",
      "Epoch: 16, Train set: Avg. loss: 0.093138\n",
      "Test set: Avg. loss: 0.092624\n",
      "Epoch: 17, Train set: Avg. loss: 0.091947\n",
      "Test set: Avg. loss: 0.091991\n",
      "Epoch: 18, Train set: Avg. loss: 0.091223\n",
      "Test set: Avg. loss: 0.090797\n",
      "Epoch: 19, Train set: Avg. loss: 0.090990\n",
      "Test set: Avg. loss: 0.090894\n",
      "Epoch: 20, Train set: Avg. loss: 0.090098\n",
      "Test set: Avg. loss: 0.089838\n",
      "Epoch: 21, Train set: Avg. loss: 0.089946\n",
      "Test set: Avg. loss: 0.090008\n",
      "Epoch: 22, Train set: Avg. loss: 0.089844\n",
      "Test set: Avg. loss: 0.089363\n",
      "Epoch: 23, Train set: Avg. loss: 0.089442\n",
      "Test set: Avg. loss: 0.088835\n",
      "Epoch: 24, Train set: Avg. loss: 0.090016\n",
      "Test set: Avg. loss: 0.088128\n",
      "Epoch: 25, Train set: Avg. loss: 0.088703\n",
      "Test set: Avg. loss: 0.088561\n",
      "Epoch: 26, Train set: Avg. loss: 0.088082\n",
      "Test set: Avg. loss: 0.089321\n",
      "Epoch: 27, Train set: Avg. loss: 0.088641\n",
      "Test set: Avg. loss: 0.087428\n",
      "Epoch: 28, Train set: Avg. loss: 0.088839\n",
      "Test set: Avg. loss: 0.087644\n",
      "Epoch: 29, Train set: Avg. loss: 0.088354\n",
      "Test set: Avg. loss: 0.087822\n",
      "Epoch: 30, Train set: Avg. loss: 0.088082\n",
      "Test set: Avg. loss: 0.088352\n",
      "Epoch: 31, Train set: Avg. loss: 0.088165\n",
      "Test set: Avg. loss: 0.088125\n",
      "Epoch: 32, Train set: Avg. loss: 0.087911\n",
      "Test set: Avg. loss: 0.088122\n",
      "Epoch: 33, Train set: Avg. loss: 0.087991\n",
      "Test set: Avg. loss: 0.088674\n",
      "Epoch: 34, Train set: Avg. loss: 0.087257\n",
      "Test set: Avg. loss: 0.088163\n",
      "Epoch: 35, Train set: Avg. loss: 0.087826\n",
      "Test set: Avg. loss: 0.087305\n",
      "Epoch: 36, Train set: Avg. loss: 0.088171\n",
      "Test set: Avg. loss: 0.088121\n",
      "Epoch: 37, Train set: Avg. loss: 0.087699\n",
      "Test set: Avg. loss: 0.088486\n",
      "Epoch: 38, Train set: Avg. loss: 0.087375\n",
      "Test set: Avg. loss: 0.088074\n",
      "Epoch: 39, Train set: Avg. loss: 0.086103\n",
      "Test set: Avg. loss: 0.086986\n",
      "Epoch: 40, Train set: Avg. loss: 0.087636\n",
      "Test set: Avg. loss: 0.088295\n",
      "Epoch: 41, Train set: Avg. loss: 0.087683\n",
      "Test set: Avg. loss: 0.087796\n",
      "Epoch: 42, Train set: Avg. loss: 0.087643\n",
      "Test set: Avg. loss: 0.087725\n",
      "Epoch: 43, Train set: Avg. loss: 0.088185\n",
      "Test set: Avg. loss: 0.088630\n",
      "Epoch: 44, Train set: Avg. loss: 0.087934\n",
      "Test set: Avg. loss: 0.088541\n",
      "Epoch: 45, Train set: Avg. loss: 0.087559\n",
      "Test set: Avg. loss: 0.087320\n",
      "Epoch: 46, Train set: Avg. loss: 0.087270\n",
      "Test set: Avg. loss: 0.088064\n",
      "Epoch: 47, Train set: Avg. loss: 0.086863\n",
      "Test set: Avg. loss: 0.088729\n",
      "Epoch: 48, Train set: Avg. loss: 0.087375\n",
      "Test set: Avg. loss: 0.089073\n",
      "Epoch: 49, Train set: Avg. loss: 0.087819\n",
      "Test set: Avg. loss: 0.086909\n",
      "Epoch: 50, Train set: Avg. loss: 0.087149\n",
      "Test set: Avg. loss: 0.087301\n",
      "Epoch: 51, Train set: Avg. loss: 0.088422\n",
      "Test set: Avg. loss: 0.088296\n",
      "Epoch: 52, Train set: Avg. loss: 0.087421\n",
      "Test set: Avg. loss: 0.087496\n",
      "Epoch: 53, Train set: Avg. loss: 0.087913\n",
      "Test set: Avg. loss: 0.087229\n",
      "Epoch: 54, Train set: Avg. loss: 0.087346\n",
      "Test set: Avg. loss: 0.087867\n",
      "Epoch: 55, Train set: Avg. loss: 0.087697\n",
      "Test set: Avg. loss: 0.087683\n",
      "Epoch: 56, Train set: Avg. loss: 0.087881\n",
      "Test set: Avg. loss: 0.087144\n",
      "Epoch: 57, Train set: Avg. loss: 0.087996\n",
      "Test set: Avg. loss: 0.087624\n",
      "Epoch: 58, Train set: Avg. loss: 0.087166\n",
      "Test set: Avg. loss: 0.087554\n",
      "Epoch: 59, Train set: Avg. loss: 0.087593\n",
      "Test set: Avg. loss: 0.088346\n",
      "Epoch: 60, Train set: Avg. loss: 0.087273\n",
      "Test set: Avg. loss: 0.087766\n",
      "Epoch: 61, Train set: Avg. loss: 0.087567\n",
      "Test set: Avg. loss: 0.088088\n",
      "Epoch: 62, Train set: Avg. loss: 0.087559\n",
      "Test set: Avg. loss: 0.087154\n",
      "Epoch: 63, Train set: Avg. loss: 0.088301\n",
      "Test set: Avg. loss: 0.087509\n",
      "Epoch: 64, Train set: Avg. loss: 0.087314\n",
      "Test set: Avg. loss: 0.087958\n",
      "Epoch: 65, Train set: Avg. loss: 0.087416\n",
      "Test set: Avg. loss: 0.087185\n",
      "Epoch: 66, Train set: Avg. loss: 0.087948\n",
      "Test set: Avg. loss: 0.088270\n",
      "Epoch: 67, Train set: Avg. loss: 0.087516\n",
      "Test set: Avg. loss: 0.087654\n",
      "Epoch: 68, Train set: Avg. loss: 0.087460\n",
      "Test set: Avg. loss: 0.087849\n",
      "Epoch: 69, Train set: Avg. loss: 0.087615\n",
      "Test set: Avg. loss: 0.088203\n",
      "Epoch: 70, Train set: Avg. loss: 0.087301\n",
      "Test set: Avg. loss: 0.087743\n",
      "Epoch: 71, Train set: Avg. loss: 0.087499\n",
      "Test set: Avg. loss: 0.088350\n",
      "Epoch: 72, Train set: Avg. loss: 0.087204\n",
      "Test set: Avg. loss: 0.087479\n",
      "Epoch: 73, Train set: Avg. loss: 0.088048\n",
      "Test set: Avg. loss: 0.087158\n",
      "Epoch: 74, Train set: Avg. loss: 0.087707\n",
      "Test set: Avg. loss: 0.087210\n",
      "Epoch: 75, Train set: Avg. loss: 0.087164\n",
      "Test set: Avg. loss: 0.087246\n",
      "Epoch: 76, Train set: Avg. loss: 0.087880\n",
      "Test set: Avg. loss: 0.088717\n",
      "Epoch: 77, Train set: Avg. loss: 0.087313\n",
      "Test set: Avg. loss: 0.087498\n",
      "Epoch: 78, Train set: Avg. loss: 0.087281\n",
      "Test set: Avg. loss: 0.087818\n",
      "Epoch: 79, Train set: Avg. loss: 0.087548\n",
      "Test set: Avg. loss: 0.087314\n",
      "Epoch: 80, Train set: Avg. loss: 0.087388\n",
      "Test set: Avg. loss: 0.087509\n",
      "Epoch: 81, Train set: Avg. loss: 0.087836\n",
      "Test set: Avg. loss: 0.086964\n",
      "Epoch: 82, Train set: Avg. loss: 0.087516\n",
      "Test set: Avg. loss: 0.087224\n",
      "Epoch: 83, Train set: Avg. loss: 0.088063\n",
      "Test set: Avg. loss: 0.087975\n",
      "Epoch: 84, Train set: Avg. loss: 0.087728\n",
      "Test set: Avg. loss: 0.087863\n",
      "Epoch: 85, Train set: Avg. loss: 0.087886\n",
      "Test set: Avg. loss: 0.087559\n",
      "Epoch: 86, Train set: Avg. loss: 0.087542\n",
      "Test set: Avg. loss: 0.087344\n",
      "Epoch: 87, Train set: Avg. loss: 0.087476\n",
      "Test set: Avg. loss: 0.087559\n",
      "Epoch: 88, Train set: Avg. loss: 0.086634\n",
      "Test set: Avg. loss: 0.088103\n",
      "Epoch: 89, Train set: Avg. loss: 0.088137\n",
      "Test set: Avg. loss: 0.088032\n",
      "Epoch: 90, Train set: Avg. loss: 0.086822\n",
      "Test set: Avg. loss: 0.087242\n",
      "Epoch: 91, Train set: Avg. loss: 0.087935\n",
      "Test set: Avg. loss: 0.087631\n",
      "Epoch: 92, Train set: Avg. loss: 0.088124\n",
      "Test set: Avg. loss: 0.087561\n",
      "Epoch: 93, Train set: Avg. loss: 0.087510\n",
      "Test set: Avg. loss: 0.087267\n",
      "Epoch: 94, Train set: Avg. loss: 0.087654\n",
      "Test set: Avg. loss: 0.088089\n",
      "Epoch: 95, Train set: Avg. loss: 0.086831\n",
      "Test set: Avg. loss: 0.087985\n",
      "Epoch: 96, Train set: Avg. loss: 0.087759\n",
      "Test set: Avg. loss: 0.088227\n",
      "Epoch: 97, Train set: Avg. loss: 0.086783\n",
      "Test set: Avg. loss: 0.087643\n",
      "Epoch: 98, Train set: Avg. loss: 0.087485\n",
      "Test set: Avg. loss: 0.086977\n",
      "Epoch: 99, Train set: Avg. loss: 0.087806\n",
      "Test set: Avg. loss: 0.088050\n",
      "Epoch: 100, Train set: Avg. loss: 0.087742\n",
      "Test set: Avg. loss: 0.088016\n"
     ]
    }
   ],
   "source": [
    "model_name = \"crypto_net1\"\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch, model1, optimizer1, train_loader)\n",
    "    test(model1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(train_losses), len(test_losses))\n",
    "#print(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAn7UlEQVR4nO3de3wV9Z3/8dcndxLuIVRJIEGLF4oQNaLilbrWW1ta7cUWrNa6rm4VrT+v5edvrdVf3e5uvVS31LpqV9lqvaGttl66stJqVUBUECyIBCIqIYGQEC65fPaPmYST5CQkIUMu834+HofMzJkz5/udczjv8/1+58yYuyMiIvGV0tsFEBGR3qUgEBGJOQWBiEjMKQhERGJOQSAiEnNpvV2Arho1apQXFRX1djFERPqVxYsXb3L3vGT39bsgKCoqYtGiRb1dDBGRfsXMStu7T11DIiIxpyAQEYk5BYGISMz1uzECERlY6urqKCsrY8eOHb1dlAEhKyuLgoIC0tPTO/0YBYGI9KqysjKGDBlCUVERZtbbxenX3J2KigrKysoYP358px+nriER6VU7duwgNzdXIdADzIzc3Nwut64UBCLS6xQCPac7+zI2QbBsGdx4I5SX93ZJRET6ltgEwYoVcMst8OmnvV0SEelLKioqKC4upri4mP3224/8/Pzm+V27dnX42EWLFjF79uwuPV9RURGbNm3amyL3uNgMFjcNoNfV9W45RKRvyc3NZenSpQDcdNNNDB48mKuvvrr5/vr6etLSkn9UlpSUUFJSsi+KGanYtAiaXsf6+t4th4j0fRdccAFXXXUV06dP57rrruONN95g2rRpHH744UybNo33338fgAULFvDFL34RCELkwgsv5OSTT+aAAw7grrvu6vTzlZaWcsoppzB58mROOeUU1q1bB8Bjjz3GpEmTmDJlCieeeCIAy5cvZ+rUqRQXFzN58mRWrVq11/VVi0BE+owrr4Twy3mPKS6GO+7o+uP+9re/8dJLL5GamsrWrVt55ZVXSEtL46WXXuKHP/whTzzxRJvHrFy5kpdffpnq6moOPvhgLr300k4dz3/ZZZfxne98h/PPP5/777+f2bNnM3/+fG6++Waef/558vPz2bJlCwBz587liiuuYObMmezatYuGhoauV64VBYGISBJf//rXSU1NBaCqqorzzz+fVatWYWbUtfNBctZZZ5GZmUlmZiajR4/m008/paCgYI/P9dprr/Hkk08CcN5553HttdcCcNxxx3HBBRfwjW98g7PPPhuAY489lltvvZWysjLOPvtsJkyYsNd1jTQIzOx04E4gFbjP3W9rdf/JwNPAh+GiJ9395ijKoiAQ6fu68809Kjk5Oc3TN954I9OnT+epp55i7dq1nHzyyUkfk5mZ2TydmppKfTf7opsOAZ07dy6vv/46zz77LMXFxSxdupRvf/vbHH300Tz77LOcdtpp3HfffXz+85/v1vM0iWyMwMxSgXuAM4CJwLfMbGKSVRe6e3F4iyQEQEEgIt1XVVVFfn4+AA8++GCPb3/atGk88sgjAMybN4/jjz8egA8++ICjjz6am2++mVGjRrF+/XrWrFnDAQccwOzZs/nyl7/MO++8s9fPH+Vg8VRgtbuvcfddwCPAjAifr0MKAhHprmuvvZYbbriB4447rkf65CdPnkxBQQEFBQVcddVV3HXXXTzwwANMnjyZhx56iDvvvBOAa665hsMOO4xJkyZx4oknMmXKFB599FEmTZpEcXExK1eu5Dvf+c5el8fcfa83knTDZl8DTnf3i8L584Cj3f2yhHVOBp4AyoANwNXuvryj7ZaUlHh3Lkzz9tvBoNETT0DY1SYifcCKFSs49NBDe7sYA0qyfWpmi9096bGuUY4RJPudc+vUWQIUunuNmZ0JzAfajHyY2cXAxQDjxo3rVmHUIhARSS7KrqEyYGzCfAHBt/5m7r7V3WvC6eeAdDMb1XpD7n6vu5e4e0leXtJLbu6RgkBEJLkog+BNYIKZjTezDOBc4JnEFcxsPwuHx81salieiigKoyAQEUkusq4hd683s8uA5wkOH73f3Zeb2SXh/XOBrwGXmlk9sB041yMatFAQiIgkF+nvCMLunudaLZubMH03cHeUZWiiU0yIiCQXm3MNqUUgIpKcTjEhIrFWUVHBKaecAsAnn3xCamoqTQelvPHGG2RkZHT4+AULFpCRkcG0adPa3Pfggw+yaNEi7r57n3R8dJuCQERibU+nod6TBQsWMHjw4KRB0F+oa0hE+pd586CoCFJSgr/z5vX4UyxevJiTTjqJI488ktNOO42PP/4YgLvuuouJEycyefJkzj33XNauXcvcuXO5/fbbKS4uZuHChZ3a/s9+9jMmTZrEpEmTuCM8wdK2bds466yzmDJlCpMmTeLRRx8F4Prrr29+zq4EVFfEpkWQmgpmCgKRfm3ePLj4YqitDeZLS4N5gJkze+Qp3J3LL7+cp59+mry8PB599FHmzJnD/fffz2233caHH35IZmYmW7ZsYfjw4VxyySVdakUsXryYBx54gNdffx135+ijj+akk05izZo1jBkzhmeffRYIzm9UWVnJU089xcqVKzGz5lNR97TYtAggOHJIRw2J9GNz5uwOgSa1tcHyHrJz506WLVvGqaeeSnFxMbfccgtlZWVAcI6gmTNn8vDDD7d71bI9+fOf/8xXv/pVcnJyGDx4MGeffTYLFy7ksMMO46WXXuK6665j4cKFDBs2jKFDh5KVlcVFF13Ek08+SXZ2do/VM1GsgiA9XS0CkX4tvHJXp5d3g7vzuc99jqVLl7J06VLeffddXnjhBQCeffZZvv/977N48WKOPPLIbp1mur2fSh100EEsXryYww47jBtuuIGbb76ZtLQ03njjDc455xzmz5/P6aefvld1a4+CQET6j/bONdbNc5Alk5mZSXl5Oa+99hoAdXV1LF++nMbGRtavX8/06dP56U9/ypYtW6ipqWHIkCFUV1d3evsnnngi8+fPp7a2lm3btvHUU09xwgknsGHDBrKzs5k1axZXX301S5YsoaamhqqqKs4880zuuOOO5kHtnhabMQJQEIj0e7fe2nKMACA7O1jeQ1JSUnj88ceZPXs2VVVV1NfXc+WVV3LQQQcxa9YsqqqqcHd+8IMfMHz4cL70pS/xta99jaeffpqf//znnHDCCS229+CDDzJ//vzm+b/+9a9ccMEFTJ06FYCLLrqIww8/nOeff55rrrmGlJQU0tPT+cUvfkF1dTUzZsxgx44duDu33357j9UzUWSnoY5Kd09DDTBmDJx1FvzqVz1cKBHpti6fhnrevGBMYN26oCVw6609NlA8UPSl01D3ORosFhkAZs7UB38P0xiBiEjMKQhEpNf1ty7qvqw7+1JBICK9Kisri4qKCoVBD3B3KioqyMrK6tLjYjVGoCAQ6XsKCgooKyujvLy8t4syIGRlZVFQUNClx8QqCDRYLNL3pKenM378+N4uRqypa0hEJOYUBCIiMacgEBGJOQWBiEjMKQhERGIuVkGgo4ZERNqKVRCoRSAi0paCQEQk5hQEIiIxpyAQEYm5WAWBBotFRNqKVRCoRSAi0paCQEQk5hQEIiIxF7sgqK8HXf9CRGS3WAVBWnj1BQ0Yi4jsFqsgSE8P/ioIRER2i2UQaJxARGQ3BYGISMwpCEREYk5BICISc5EGgZmdbmbvm9lqM7u+g/WOMrMGM/talOXRUUMiIm1FFgRmlgrcA5wBTAS+ZWYT21nvn4HnoypLE7UIRETairJFMBVY7e5r3H0X8AgwI8l6lwNPABsjLAugIBARSSbKIMgH1ifMl4XLmplZPvBVYG5HGzKzi81skZktKi8v73aBFAQiIm1FGQSWZFnrkzvcAVzn7g0dbcjd73X3EncvycvL63aBFAQiIm2lRbjtMmBswnwBsKHVOiXAI2YGMAo408zq3X1+FAVqGixWEIiI7BZlELwJTDCz8cBHwLnAtxNXcPfxTdNm9iDw+6hCAHSKCRGRZCILAnevN7PLCI4GSgXud/flZnZJeH+H4wJRUNeQiEhbUbYIcPfngOdaLUsaAO5+QZRlAQWBiEgy+mWxiEjMxSoINFgsItJWrIJAg8UiIm3FMgjUIhAR2U1BICIScwoCEZGYUxCIiMRcrIJA1yMQEWkrVkGgFoGISFsKAhGRmFMQiIjEXKyCIDU1+KsgEBHZLVZBYBYMGCsIRER2i1UQQNA9pKOGRER2i2UQqEUgIrKbgkBEJOYUBCIiMRe7INBgsYhIS7ELAg0Wi4i01KUgMLMUMxsaVWH2BXUNiYi0tMcgMLP/MrOhZpYDvAe8b2bXRF+0aCgIRERa6kyLYKK7bwW+AjwHjAPOi7JQUVIQiIi01JkgSDezdIIgeNrd6wCPtFQRUhCIiLTUmSD4JbAWyAFeMbNCYGuUhYqSjhoSEWkpbU8ruPtdwF0Ji0rNbHp0RYqWjhoSEWmpM4PFV4SDxWZm/2FmS4DP74OyRUJdQyIiLXWma+jCcLD4C0Ae8F3gtkhLFSEFgYhIS50JAgv/ngk84O5vJyzrdxQEIiItdSYIFpvZCwRB8LyZDQEaoy1WdDRYLCLS0h4Hi4HvAcXAGnevNbNcgu6hfkmDxSIiLXXmqKFGMysAvm1mAP/j7r+LvGQRUdeQiEhLnTlq6DbgCoLTS7wHzDazn0RdsKgoCEREWupM19CZQLG7NwKY2a+Bt4AboixYVBQEIiItdfbso8MTpodFUI59RoPFIiItdaZF8BPgLTN7meCw0RPpp60BUItARKS1zgwW/8bMFgBHEQTBde7+SdQFi4qOGhIRaandIDCzI1otKgv/jjGzMe6+JLpiRUctAhGRljpqEfxbB/c5nTjfkJmdDtwJpAL3ufttre6fAfyY4Adq9cCV7v7nPW13bzQFgTtYv/19tIhIz2k3CNx9r84wamapwD3AqQStiTfN7Bl3fy9htT8Bz7i7m9lk4LfAIXvzvHuSnh78bWgIBo5FROIuyovXTwVWu/sad98FPALMSFzB3WvcvekiNznsgwveNH34q3tIRCQQZRDkA+sT5svCZS2Y2VfNbCXwLHBhsg2Z2cVmtsjMFpWXl+9VoZpaBBowFhEJRBkEyXrg23zjd/en3P0Qgkth/jjZhtz9XncvcfeSvLy8vSpUUxCoRSAiEmg3CMxsVsL0ca3uu6wT2y4DxibMFwAb2lvZ3V8BDjSzUZ3YdrcpCEREWuqoRXBVwvTPW92XtAunlTeBCWY23swygHOBZxJXMLPPWngmu/Bw1QygohPb7jYFgYhISx0dN2PtTCebb8Pd68OWw/MEh4/e7+7LzeyS8P65wDnAd8ysDtgOfDNh8DgSGiwWEWmpoyDwdqaTzSffgPtzwHOtls1NmP5n4J87s62eosFiEZGWOgqCQ8zsHYJv/weG04TzB0Resoioa0hEpKWOguDQfVaKfUhBICLSUke/LC5NnA8vUXkisM7dF0ddsKgoCEREWuro8NHfm9mkcHp/YBnB0UIPmdmV+6Z4PU9BICLSUkeHj45392Xh9HeBF939S8DRdO7w0T5JRw2JiLTUURAkflSeQnj0j7tXE5wttF/SUUMiIi11NFi83swuJ/iF8BHAHwHMbBCQvg/KFgl1DYmItNRRi+B7wOeACwh+6LUlXH4M8EC0xYqOgkBEpKWOjhraCFySZPnLwMtRFipKCgIRkZY6ulTlM+3dB+DuX+754kRPg8UiIi11NEZwLMH1BH4DvE4nzi/UH2iwWESkpY6CYD+Cy0x+C/g2wYVjfuPuy/dFwaKiriERkZbaHSx29wZ3/6O7n08wQLwaWBAeSdRvKQhERFrq8PLtZpYJnEXQKigC7gKejL5Y0VEQiIi01NFg8a+BScAfgB8l/Mq4X9NgsYhISx21CM4DtgEHAbPDC4lBMGjs7j404rJFQi0CEZGWOvodQZQXtu81OmpIRKSlAflh3xG1CEREWopdEGiMQESkpdgFgVkQBgoCEZFA7IIAFAQiIoliGQTp6RosFhFpEtsgUItARCSgIBARiTkFgYhIzMUyCDRYLCKyWyyDQC0CEZHdYhsEOmpIRCQQ2yBQi0BEJKAgEBGJuVgGgQaLRUR2i2UQqEUgIrJbbINAg8UiIoHYBoFaBCIiAQWBiEjMKQhERGIulkGgo4ZERHaLNAjM7HQze9/MVpvZ9Unun2lm74S3V81sSpTlaaLBYhGR3SILAjNLBe4BzgAmAt8ys4mtVvsQOMndJwM/Bu6NpDDz5kFREaSkQFER6es/UItARCSUFuG2pwKr3X0NgJk9AswA3mtawd1fTVj/r0BBj5di3jy4+GKorQ3mS0tJL3uFuiH7A9k9/nQiIv1NlF1D+cD6hPmycFl7vgf8IdkdZnaxmS0ys0Xl5eVdK8WcObtDIJTesJ26rdu7th0RkQEqyiCwJMs86Ypm0wmC4Lpk97v7ve5e4u4leXl5XSvFunVtFqVRT11jate2IyIyQEUZBGXA2IT5AmBD65XMbDJwHzDD3St6vBTjxrVZlE4ddZbR408lItIfRRkEbwITzGy8mWUA5wLPJK5gZuOAJ4Hz3P1vkZTi1lshu+VYQHoa1KdmRvJ0IiL9TWRB4O71wGXA88AK4LfuvtzMLjGzS8LV/h+QC/y7mS01s0U9XpCZM+Hee6GwEMygsJD0s75AXYO6hkREAMw9abd9n1VSUuKLFu1dXtx8M/zTPwW/JUhVHohIDJjZYncvSXZfbH9ZDPp1sYgIxDQI0tODvwoCEZGYB4FOMyEiEvMgUItARERBICISewoCEZGYi2UQ6KghEZHdYhkEahGIiOwW6yDQUUMiIjEPArUIREQUBCIisRfrINixo3fLISLSF8QyCA49NPi7eHHvlkNEpC+IZRDk5wdh8OKLvV0SEZHeF8sgAPi7v4NXXlH3kIhIbIPg1FNh+3Z49dXeLomISO+KbRCcfHLwC2N1D4lI3MU2CIYMgWOOURCIiMQvCObNg6IiSEnh1Hd/xpIlTkVFbxdKRKT3xCsI5s2Diy+G0lJw59Sqx3A3/nTTwt4umYhIr4lXEMyZA7W1zbNH8SbD2MKLD5b1YqFERHpXvIJg3boWs2k0MJ2XebHmWNx7qUwiIr0sXkEwblybRafyIqUUsXp1L5RHRKQPiFcQ3HorZGe3WHRm2oukUcecg36LFxYF4wgiIjESryCYORPuvRcKC8EMcnMpSlnHj7mRx/gG/7nupGAwWWEgIjFi3s86x0tKSnzRokU9s7GiIigtpYEUTuFPLOZIllLMgYUNsHZtzzyHiEgfYGaL3b0k2X3xahG0Fg4ep9LIQ5xHGvXMZB51pR8FIaGWgYjEQLyDIGHweCxl/JJ/4HWO4RyeoKp0s7qJRCQW4h0ErQaPv8Fj3MXl/IEzOIo3WVY7HmbNUutARAa0eAdB4uBx6HLu5mWmU80QjuGv3Mf3aCxdp9aBiAxY8Q4CCMJg7doWYXA8f2ExR3IUb/L33MfJLGBF7Ti1DkRkQFIQNGnVTTSGj/lvPs9/cCHLmMQU3uYW5lBfWqbWgYgMKAqCJkm6iQy4kAdYySGcwxPcyC0cy2u8V1uo1oGIDBgKgkRN3UQPP9yidTCacn7Dt/ktX+dDxnMES7iT2XhpqVoHItLvKQiSSdI6APg6j7Ocz/EFXuBK7uQcnmBLbXrQOhg1KrilpKilICL9ioKgPe20Dj7DRp5mBv/GVfyOL3EES3iSr7KpAqioAPfgegdqKYhIPxFpEJjZ6Wb2vpmtNrPrk9x/iJm9ZmY7zezqKMvSbe2MHVzF7SzkBBpI5RyeJI9NHMxKruZf+IgxwXUPNI4gIv1AZEFgZqnAPcAZwETgW2Y2sdVqlcBs4F+jKkePaKd1cAyv8z4Hs5DjuY3rmMAq7uBKDmAN/8g9rGF80Do477zgJHcKBRHpg6JsEUwFVrv7GnffBTwCzEhcwd03uvubQF2E5eg5SVoHWezkeP7CdfyU3/MlVjGB7/IA93ERB7KGk1jAfX4hlYxQKIhInxRlEOQD6xPmy8Jl/Vs7rYMm41nLXC7lQ8ZzKz/kUz7D33MfuVQyjlLO8t8xh1t4ofQgts36hyAUEgeaNegsIvtYlEFgSZZ165zXZnaxmS0ys0Xl5eV7WawekuTaBuTmNt+dzwZ+yE9YwaG8wVH8hOs5kVcoo4Cfci2n8QIjqOR4FjKn4ge8UHEENZ4dDDgnDjp/97sKCRGJVGTXIzCzY4Gb3P20cP4GAHf/SZJ1bwJq3H2PYwU9ej2CKMybFxwxVFvb7io15PAXjuNlpvMy01nMkTSQhtFIAWUcyAd8ltUcxrtM5h0O411yqWy7IbMgMJoCqLISRo7c8/S4ccEvqWfO7Lgec+YEp+ruzPoi0qd1dD0C3D2SG5AGrAHGAxnA28Dn2ln3JuDqzmz3yCOP9D7v4YfdCwvdwd0s+NvBbSuD/XlO9R9xo5/Hr30af/ZRbGyx2hjK/DT+4FfzU7+bf/SnmOFvUOIrOchLGevl5HoDe36u5ltTuXJzg5vZ7ulk5U62fmGh+6WXBn8TH78vpgsLg/2cuL/3tH5PlbWrz93ZbfbWvuyvZW3vdegr5evOvuzMeytxnS4AFnk7n6uRXqHMzM4E7gBSgfvd/VYzuyQMoLlmth+wCBgKNAI1wER339reNvt8i6C1pm/WpaW7v8F3ggOf8hneYTLvMJl3OYx3mMwKDmUnWUkfk8FOiljLAaxhBJvJYBcZ7CKdOtKoJ5UGsqllGFUMo4pGUignj42MJpUGDuctjmAJh7CSdOqTlmknmewgiwZSGUYVaTTsxc7ZS037swv7dUA8t+w2EF+HztQpOzvomu5CK72jFkG8L1W5r3UzFBI1YpSTRxkFfEQ+1QxhB1lsI4ePyGcNB7CGA9jKUOpIZyeZ1JNGA6nUkc52BtFAWottjqCSnWRSSw4AKTQwik3sxycMpoZNjKKcPDYzsk15hlLFSCrJYRs5bGMQ23GMBlJpJIVMdjKI7QxiO4ZTTxqNpDCSSgooI5+PaCSFjYxmI6PZzAiqGUI1Q0ihkZFUMpJKhrOFwdQwhGpGs5EpvM2hrCCNej7gQP7CcazgUOpJwzEcC/91GkmhglzKyWMLw0mnjkFsJ5tahlDNMKoYylYGU9Ncj6FsZThbGEYVO8iiglw2MYpKRrKZEWxhOGnUsz8fM4YN1JLNUop5i8PZylAmsIqD+Buj2chmRlBBLjvIYhSbyKOcYVSxjRy2MpQaBrODLHaS2fx6Ne2n/fiEItYylvXUkc4WhjfXYTA1DKaGetLCUufQQGrza7OF4axnLGUUkEJjizI1kEoDqdSS3fz6bmcQw6hiOFsYylZSaSCFRgxvfi81kEo2tQymhmxqm5fvIoNMdja/BzYymg8ZzzqCiz8NCV/VkVQ274MdZPER+XxEPo2kMJqN5FHOULY2v3ZNZawlm11kkMEuMtkJwGZGsJkRbCOHHLYxmBoy2UklI8P36wiy2MEQqslhG42ksIsM6khnENuby5ROXbg3GsijnCLWUkgpH7M/r3Esr3EshnMESziCJWSxg/eYyHtM5CPym1+3VBoYzUb24xOGs4V60thFBoYzlvUUsZZcKljHOD7gQDYwhgLKmMAqiljLFobzEfl8zP5UMSx8dQfjWHP5vsALfIWngxe4sLBLl9RVEPRFiX3wHfXrV1fDrl099rQObCOHKoYBkEc5GdTRQAqrmMBijuR9DuZTPsMn7EcNg5v/446kkmxqyWQnqTSwheFUkEslI6klm23ksJ1BGN78IbKLjOb/yEDz8kpGsoExzaFkNJJLBSOpbP7AbySFzYygkpFsYXhzUDXJYCeDqaGSYIwkPWz9NH14OUYjKc2BMopNjKSyORC3kUM1Q9jKULYylMaED9GOpFHHCDaHH8wjmpePoJLDeYvhbGE1n2UVE9hONhnsJJcKsthBBblsDfc9BKE7mBoGsZ1Mdja34tLCFtkGxrCJvG691kPYyljWU0AZ9aSxigmsZ1yb9bLYTh7lDGI7WxnKZka02+pMoaHT+2koVRRSSgqNzft5MyPafBEZzaek0kA5edST3un6GY0Mo4octrGdQVQzhDoyGM5m8ihnOFvYSSbVDKGGwaRR37xvm9YPPmg7PmbmQFZjOKuZkLR+Wewgk53Uk8ZGRvMJ+zW/VzPYSSMpSeuVQw3bGNxu3ZqCvunLTCMpXM7P+b/cGq5k0NjY+f2lIOjHkgVGRcWAaAo3kMKnfIY06smlglQ6flM3kEIt2axnLG8zhaUUU0EuU3mDabzKRN4jpXsHpuHQ3LKqYTBbGRp2oA0jk52MYlNzUOWwrfmQuO1ksYExpFPHWNa3OFSuEWM7g8imtsXynWSwlaHN356THV6XqIYcyiggk52MYDND2drcCqhhMKlhmGRTu8euuloGUcnI5u7CLHaQQ9sDG+rCFkkDqTjW/AFqwC7Sm1sg6dSRxQ4y2NW8/2rJbv4gbl23RowtDKecPLLYwf58TEb4MyInaMVUM6S5VZdCY3MrLZ26MCYzcIwhVLd5zzRiXXoPODS3XutJ41M+w1qKWEsRI6nkWF5jNMGRilUMZSnF7CKDibzHGDa0+9rVk0oqDVhYpo/Zn1IK2cQoxrKeA/mAoVSzmeGsYgLrGMcINpPPR4xhA0Oo3uP7Qi2COAVBezrbomg93ZUQGYj9ryIDQQ+PEeikc/1V0w/bGhth06bg1plpd3jooba/f2g9XVgYrNfR+oWFcOmle95WT09DMJ+oab6j+vREWbvz3J2Z7q192V/L2tHr0BfK15192dn3VmFhl0Ngj9o7nKiv3vrF4aMSvdaHC3bjcLp++dyy20B8HSKsE711+GgU1DUkItJ16hoSEZF2KQhERGJOQSAiEnMKAhGRmFMQiIjEXL87asjMyoHSbj58FLCpB4vTX8Sx3nGsM8Sz3nGsM3S93oXunvR8Jf0uCPaGmS1q7/CpgSyO9Y5jnSGe9Y5jnaFn662uIRGRmFMQiIjEXNyC4N7eLkAviWO941hniGe941hn6MF6x2qMQERE2opbi0BERFpREIiIxFxsgsDMTjez981stZld39vliYKZjTWzl81shZktN7MrwuUjzexFM1sV/h2xp231N2aWamZvmdnvw/k41Hm4mT1uZivD1/zYmNT7B+H7e5mZ/cbMsgZavc3sfjPbaGbLEpa1W0czuyH8bHvfzE7r6vPFIgjMLBW4BzgDmAh8y8wm9m6pIlEP/B93PxQ4Bvh+WM/rgT+5+wTgT+H8QHMFsCJhPg51vhP4o7sfAkwhqP+ArreZ5QOzgRJ3nwSkAucy8Or9IHB6q2VJ6xj+Hz8X+Fz4mH8PP/M6LRZBAEwFVrv7GnffBTwCzOjlMvU4d//Y3ZeE09UEHwz5BHX9dbjar4Gv9EoBI2JmBcBZwH0Jiwd6nYcCJwL/AeDuu9x9CwO83qE0YJCZpQHZwAYGWL3d/RWgstXi9uo4A3jE3Xe6+4fAaoLPvE6LSxDkA+sT5svCZQOWmRUBhwOvA59x948hCAtgdC8WLQp3ANdCiyuZD/Q6HwCUAw+EXWL3mVkOA7ze7v4R8K/AOuBjoMrdX2CA1zvUXh33+vMtLkFgSZYN2ONmzWww8ARwpbtv7e3yRMnMvghsdPfFvV2WfSwNOAL4hbsfDmyj/3eH7FHYLz4DGA+MAXLMbFbvlqrX7fXnW1yCoAwYmzBfQNCcHHDMLJ0gBOa5+5Ph4k/NbP/w/v2Bjb1VvggcB3zZzNYSdPl93sweZmDXGYL3dJm7vx7OP04QDAO93n8HfOju5e5eBzwJTGPg1xvar+Nef77FJQjeBCaY2XgzyyAYWHmml8vU48zMCPqMV7j7zxLuegY4P5w+H3h6X5ctKu5+g7sXuHsRwev63+4+iwFcZwB3/wRYb2YHh4tOAd5jgNeboEvoGDPLDt/vpxCMhQ30ekP7dXwGONfMMs1sPDABeKNLW27vqvYD7QacCfwN+ACY09vliaiOxxM0Cd8Bloa3M4FcgqMMVoV/R/Z2WSOq/8nA78PpAV9noBhYFL7e84ERMan3j4CVwDLgISBzoNUb+A3BGEgdwTf+73VUR2BO+Nn2PnBGV59Pp5gQEYm5uHQNiYhIOxQEIiIxpyAQEYk5BYGISMwpCEREYk5BIH2amS0ws8gvTG5ms8MzeM5rtbzYzM7sxvbGmNnjnVjvOTMb3tXt72tmdoGZ3d3b5ZBopPV2AUSiYmZp7l7fydX/keD46w9bLS8GSoDnurJ9d98AfG1PT+ruXQ4ZkZ6mFoHsNTMrCr9N/yo8T/wLZjYovK/5G72ZjQpPBdH0DXO+mf3OzD40s8vM7KrwBGp/NbORCU8xy8xeDc8/PzV8fE54zvY3w8fMSNjuY2b2O+CFJGW9KtzOMjO7Mlw2l+Akbs+Y2Q8S1s0Abga+aWZLzeybZnaTmd1rZi8A/xnWfaGZLQlv0xL2ybKEMj1pZn8MzyX/04TnWBvul4724VFm9o6ZvWZm/2IJ56hvVbdrwv3xjpn9KKEcK83s1+Hyx80sO7zvlHDfvRvuy8yE53vVzN42szfMbEj4FGNa18GC60A8GO7PdxP3n/Qjvf0LOt36/w0oIrgWQnE4/1tgVji9gODc8QCjgLXh9AUEp8sdAuQBVcAl4X23E5wwr+nxvwqnTwSWhdP/P+E5hhP8ajwn3G4ZSX5ZChwJvBuuNxhYDhwe3rcWGJXkMRcAdyfM3wQsBgaF89lAVjg9AViUsE+WJWxjDTAMyAJKgbGJz7uHfbgMmBZO39a03Vbl/ALBxcyN4Ave78P9VUTwa/PjwvXuB64Oy7EeOChc/p/AlUBGWNajwuVDCXoOktYh3KcvJpRjeG+/H3Xr+k0tAukpH7r70nB6McEH0J687O7V7l5OEAS/C5e/2+rxv4Hmc7QPDfvUvwBcb2ZLCcIiCxgXrv+iu7c+lzsEp+B4yt23uXsNwQnLTuhEOVt7xt23h9PpwK/M7F3gMYILHyXzJ3evcvcdBOcEKkyyTpt9GNZ1iLu/Gi7/r3a2/4Xw9hawBDiEIJgA1rv7X8Lphwn2w8Hh8/0tXP5rguA4GPjY3d8EcPetvrv7K1kd1gAHmNnPzex0YECf7Xag0hiB9JSdCdMNwKBwup7dXZBZHTymMWG+kZbvzdbnQXGCb77nuPv7iXeY2dEEp2ROJtnpersjcfs/AD4luEJYCrCjnce03j/J/u8l24edLbMBP3H3X7ZYGFyXor3919522jvvTJs6uPtmM5sCnAZ8H/gGcGEnyyx9hFoEErW1BN0H0InB03Z8E8DMjie4EEkV8DxweXgGSszs8E5s5xXgKxacuTIH+CqwcA+PqSbovmrPMIJv0I3AeQSXTuwx7r4ZqDazY8JF57az6vPAhRZciwIzyzezpguXjDOzY8PpbwF/JjhpW5GZfTZcfh7wP+HyMWZ2VLidIRZcCSwpMxsFpLj7E8CNBKfCln5GLQKJ2r8CvzWz84D/7uY2NpvZqwT91U3fNn9McGWyd8IwWAt8saONuPsSM3uQ3afovc/d39rDc7/M7i6onyS5/9+BJ8zs6+G67bVG9sb3CLqfthF0g1W1XsHdXzCzQ4HXwmysAWYRfHNfAZxvZr8kOHPlL9x9h5l9F3gs/KB/E5jr7rvM7JvAz8PB6u0E1wBoTz7BVdKavlTesPfVlX1NZx8V6ePMbHA4poGZXQ/s7+5XdPKxRQSn5p4UYRGln1OLQKTvO8vMbiD4/1pKcASPSI9Ri0BEJOY0WCwiEnMKAhGRmFMQiIjEnIJARCTmFAQiIjH3vywGekWXlL1ZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(train_losses)), train_losses, color='blue')\n",
    "plt.scatter(range(len(test_losses)), test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training epochs')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Encoded sentence\n",
    "message = \"It is time to test the model on real-world data\"#. We must choose a sentence and an image. We choose the following sentence from Wikipedia: Anthony Edward \\\"Tony\\\" Stark is a character portrayed by Robert Downey Jr. in the MCU film franchise and for the image, we use the same Tony Starkâ€™s image from the beginning of the article. The following code is used to embedding and extracting the message:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41, 84, 0, 73, 83, 0, 84, 73, 77, 69, 0, 84, 79, 0, 84, 69, 83, 84, 0, 84, 72, 69, 0, 77, 79, 68, 69, 76, 0, 79, 78, 0, 82, 69, 65, 76, 13, 87, 79, 82, 76, 68, 0, 68, 65, 84, 65]\n",
      "tensor([ 0.0208, -0.5479,  0.3699, -0.5435, -0.2456,  0.0691, -0.2448,  0.1447,\n",
      "         0.2614, -0.2412], device='cuda:0', grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "message_index = [(ord(c)-32) for c in message]\n",
    "print(message_index)\n",
    "secret_message = secret_key[:, message_index]#.to(device)\n",
    "output = model1(secret_message)\n",
    "print(output[:,0]-secret_message[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encrypt(message, model, secret_key):\n",
    "    message_index = [(ord(c)-32) for c in message]\n",
    "    print(message_index)\n",
    "    #print(secret_key)\n",
    "    #sk = secret_key.cpu()\n",
    "    #print(sk.shape, sk.device)\n",
    "    secret_message = secret_key[:, message_index]#.to(device)\n",
    "    print(secret_message[:,0])\n",
    "    return model.f_encoder.apply_wnoise(secret_message, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41, 84, 0, 73, 83, 0, 84, 73, 77, 69, 0, 84, 79, 0, 84, 69, 83, 84, 0, 84, 72, 69, 0, 77, 79, 68, 69, 76, 0, 79, 78, 0, 82, 69, 65, 76, 13, 87, 79, 82, 76, 68, 0, 68, 65, 84, 65]\n",
      "tensor([ 0.4421,  1.5233, -1.5746,  1.3358,  1.4714, -1.8763, -0.1206, -1.6462,\n",
      "        -1.1984,  0.5341], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "encrypted_message = encrypt(message, model1, secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 1., 0.]], device='cuda:0') torch.Size([1000, 47])\n"
     ]
    }
   ],
   "source": [
    "print(encrypted_message, encrypted_message.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_letter(vec, secret_key):\n",
    "    ind=torch.argmin(torch.norm(secret_key-vec,dim=0))\n",
    "    #print(ind)\n",
    "    return chr(ind+32)\n",
    "\n",
    "def decrypt(encrypted_message, model, secret_key):\n",
    "    m = \"\"\n",
    "    raw_m = model.decrypt(encrypted_message)\n",
    "    #print(raw_m[:,0], secret_key[:,41], torch.norm(raw_m[:,0]-secret_key[:,41]))\n",
    "    for i in range(raw_m.shape[1]):\n",
    "        m+=(determine_letter(raw_m[:,i].view(raw_m.shape[0],1),secret_key))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is time to test the model on real-world data\n"
     ]
    }
   ],
   "source": [
    "decrypted_m = decrypt(encrypted_message, model1, secret_key)\n",
    "print(decrypted_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
