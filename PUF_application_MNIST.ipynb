{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x246fbb811f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "random_seed = 420\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size_train = 64\n",
    "batch_size_test = 500\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "fname = \"figures\\cryptography\\\\ground_truth.png\"\n",
    "plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network flowchart:\n",
    "#input vector --- random initialized weights ---> hyperdimensional vecotor ---> one_hot_net l1 ---> ...ln... ---> sigmoid/softmax output\n",
    "#One hot net\n",
    "\n",
    "class One_hot_op(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, W, epsilon):\n",
    "        Z = torch.matmul(W, A)\n",
    "        ctx.Z = Z\n",
    "        ctx.A = A\n",
    "        ctx.W = W\n",
    "        ctx.epsilon = epsilon\n",
    "        ret = Z > epsilon\n",
    "        #print(ret[1:10][1:10])\n",
    "        return ret.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dL_dA):\n",
    "        step = ctx.Z > ctx.epsilon\n",
    "        step = step.float()\n",
    "        dL_dZ = dL_dA * step \n",
    "        \n",
    "        dZ_dW = torch.transpose(ctx.A, 0,1)\n",
    "        dZ_dW = torch.sign(dZ_dW)\n",
    "        dZ_dA = torch.transpose(ctx.W, 0,1)\n",
    "        dZ_dA = torch.sign(dZ_dA)\n",
    "        dA = torch.matmul(dZ_dA,dL_dZ)\n",
    "        dW = torch.matmul(dL_dZ,dZ_dW)\n",
    "        return dA, dW, None\n",
    "\n",
    "\n",
    "class One_hot_layer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, initialization_f, epsilon):\n",
    "        super(One_hot_layer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.initialization_f = initialization_f\n",
    "        initialized_weight = initialization_f(out_dim, in_dim)\n",
    "        self.weight = nn.Parameter(initialized_weight, requires_grad = True)\n",
    "        self.op = One_hot_op\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_dim) + ',' \\\n",
    "               + str(self.out_dim) + ')'\n",
    "        \n",
    "    def forward(self, A):\n",
    "        #print(self.weight[:3][:3])\n",
    "        return self.op.apply(A, self.weight, self.epsilon)\n",
    "    \n",
    "\n",
    "class One_hot_net(nn.Module):\n",
    "    def __init__(self, in_dim, n_class, f_encoder, encoder_multiplier, f_initializer, epsilon, n_layers=2, layer_size_factor=[1, 5], dropout=[-1, 0.5]):\n",
    "        super(One_hot_net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.in_dim = in_dim\n",
    "        feature_len = in_dim * encoder_multiplier\n",
    "        self.feature_len = feature_len\n",
    "        self.n_layers=n_layers\n",
    "        self.layer_size_factor=layer_size_factor\n",
    "        self.dropout=dropout\n",
    "        self.epsilon = epsilon\n",
    "        self.n_class = n_class\n",
    "        self.f_encoder = f_encoder\n",
    "        self.f_initializer = f_initializer\n",
    "        for i in range(n_layers):\n",
    "            if dropout[i] > 0:\n",
    "                self.layers.append(nn.Dropout(dropout[i]))\n",
    "            if i < n_layers - 1:\n",
    "                self.layers.append(\n",
    "                    One_hot_layer(int(feature_len // layer_size_factor[i]), int(feature_len // layer_size_factor[i + 1]), f_initializer, epsilon))\n",
    "        self.tail = nn.Linear(int(feature_len // layer_size_factor[-1]), n_class)\n",
    "    \n",
    "    def flatten(self, X):\n",
    "        return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "    \n",
    "    def unflatten(self, X):\n",
    "        return X.view(X.shape[0], 1,int(self.in_dim**(1/2)), int(self.in_dim**(1/2)))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        #X = self.f_encoder.apply(X)\n",
    "        X = self.f_encoder.apply_wnoise(X, 0.5)\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        X = self.tail(X)\n",
    "        return self.unflatten(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise \n",
    "def apply_gaussian_noise(tensor, sd, device = torch.device(\"cuda:0\")):\n",
    "    tensor = tensor + (sd)*torch.randn(*tuple(tensor.shape)).to(device)\n",
    "    return tensor\n",
    "\n",
    "def apply_binary_noise(tensor, p, device = torch.device(\"cuda:0\")):\n",
    "    #change 1 entries to 0\n",
    "    tensor = tensor * (torch.rand(*tuple(tensor.shape))>(p/2)).float().to(device)\n",
    "    #change 0 entries to 1\n",
    "    tensor = tensor + (torch.rand(*tuple(tensor.shape))>(1-p/2)).float().to(device)\n",
    "    return torch.sign(tensor)\n",
    "\n",
    "#a = [[1,1,0,0,1,1], [1,1,0,0,1,1], [1,1,0,0,1,1]]\n",
    "#a = torch.tensor(a)\n",
    "#print((torch.rand(*tuple(a.shape))<(0.1/2)).float())\n",
    "#print(a)\n",
    "#print(apply_binary_noise(a, 0.1, device = torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializers and encoders\n",
    "def uniform_initializer(out_dim, in_dim, cuda = True):\n",
    "    tensor = torch.empty(out_dim, in_dim)\n",
    "    if cuda:\n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2).cuda()\n",
    "    else: \n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2)\n",
    "\n",
    "class simple_encoder():\n",
    "    def __init__(self, out_dim, in_dim):\n",
    "        self.W = uniform_initializer(out_dim, in_dim)\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return torch.matmul(self.W, X)\n",
    "    \n",
    "class simple_encoder_wthreshold():\n",
    "    def __init__(self, out_dim, in_dim, epsilon, cuda = True):\n",
    "        self.W = uniform_initializer(out_dim, in_dim, cuda)\n",
    "        self.device = torch.device(\"cuda:0\") if cuda else torch.device(\"cpu\")\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def apply(self, X):\n",
    "        return (torch.matmul(self.W, X) > self.epsilon).float()\n",
    "    \n",
    "    def apply_wnoise(self, X, sd):\n",
    "        return (torch.matmul(apply_gaussian_noise(self.W, sd, device = self.device), X) > self.epsilon).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'in_dim': 784,\n",
    "    'n_class': 784,\n",
    "    'f_encoder': simple_encoder_wthreshold(784*20, 784, 1e-3),\n",
    "    'f_initializer': uniform_initializer,\n",
    "    'encoder_multiplier': 20,\n",
    "    'epsilon': 10e-3,\n",
    "    'n_layers': 1,\n",
    "    'layer_size_factor': [1],\n",
    "    'dropout': [-1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model1 = One_hot_net(parameters['in_dim'], parameters['n_class'], parameters['f_encoder'], parameters['encoder_multiplier'], \n",
    "                     parameters['f_initializer'], parameters['epsilon'], parameters['n_layers'], \n",
    "                     parameters['layer_size_factor'], parameters['dropout']).to(device)\n",
    "\n",
    "#model1 = toy_Net().to(device)\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.01, momentum=0.5)\n",
    "#optimizer1 = torch.optim.SGD([{'params': model1.layers.parameters(), 'lr': 0.01}, {'params': model1.tail.parameters(), 'lr': 0.01}], lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 784]      12,293,904\n",
      "================================================================\n",
      "Total params: 12,293,904\n",
      "Trainable params: 12,293,904\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 46.90\n",
      "Estimated Total Size (MB): 46.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model1, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, trainloader, log_interval = 10, device = torch.device(\"cuda:0\")):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = nn.MSELoss()(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "            (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            #torch.save(model.state_dict(), '/results/model.pth')\n",
    "            #torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device = torch.device(\"cuda:0\")):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += nn.MSELoss()(output, data).item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_loss *=500\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.6f}\\n'.format(\n",
    "        test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 1.172376\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.086093\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.746895\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.694489\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.659255\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.650339\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.659530\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.568958\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.562900\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.557623\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.536629\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.530389\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.508814\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.491839\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.477701\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.467430\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.471890\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.456570\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.449599\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.438567\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.419742\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.411311\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.398748\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.406389\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.426874\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.406296\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.372086\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.392556\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.380019\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.383186\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.381088\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.376448\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.344874\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.370975\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.332924\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.302668\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.331541\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.323497\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.310350\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.327962\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.326094\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.302302\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.319624\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.318632\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.319102\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.319547\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.305067\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.304204\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.299646\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.302113\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.286589\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.312728\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.285532\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.281232\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.293638\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.277812\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.260575\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.285539\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.284243\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.265475\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.262533\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.263229\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.262242\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.272239\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.237640\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.248334\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.270995\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.291234\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.246112\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.252410\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.258839\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.252263\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.240655\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.238829\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.240369\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.237502\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.232506\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.225169\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.249041\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.244973\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.221159\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.235187\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.219136\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.229625\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.239217\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.242851\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.219895\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.238778\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.209278\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.238206\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.215884\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.218071\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.216895\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.222399\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.199086\n",
      "\n",
      "Test set: Avg. loss: 0.213752\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.208951\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.200474\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.232582\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.219974\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.218812\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.209776\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.207146\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.211436\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.215557\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.208970\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.198165\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.215057\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.215230\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.205538\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.214851\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.203608\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.207821\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.208441\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.199248\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.191133\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.200820\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.200636\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.192285\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.195897\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.190597\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.194020\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.195434\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.187447\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.183614\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.194985\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.179756\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.188190\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.182736\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.184522\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.195272\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.184341\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.182853\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.190536\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.191919\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.173455\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.181466\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.181953\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.175967\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.196511\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.171642\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.187211\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.186957\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.185361\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.176572\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.177172\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.168095\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.182803\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.165472\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.177792\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.173468\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.167814\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.173061\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.179041\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.168722\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.175838\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.169022\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.169984\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.173275\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.174854\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.165085\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.164316\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.163220\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.172326\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.169042\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.165787\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.168147\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.160045\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.174493\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.162693\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.162911\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.166471\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.165438\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.172856\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.158916\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.164081\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.148162\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.166044\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.161708\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.162035\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.170525\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.145731\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.164061\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.149580\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.178657\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.151502\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.163074\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.175049\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.155673\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.158373\n",
      "\n",
      "Test set: Avg. loss: 0.157249\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.161713\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.161536\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.165670\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.149471\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.157263\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.162070\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.158296\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.153882\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.160476\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.159062\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.152030\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.159522\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.152637\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.169127\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.149871\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.139293\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.149607\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.152469\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.162799\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.154546\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.147316\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.158502\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.154084\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.151326\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.165592\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.147401\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.150978\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.152784\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.152149\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.151602\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.148272\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.142676\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.168965\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.149116\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.144620\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.146201\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.155327\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.139905\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.147656\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.140447\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.147553\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.147283\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.147638\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.141523\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.146532\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.143839\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.144814\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.146413\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.150857\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.142150\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.138732\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.141048\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.144656\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.146963\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.136952\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.154242\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.141718\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.140021\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.144326\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.136385\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.136933\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.139182\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.135930\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.144159\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.135087\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.139549\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.144898\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.140140\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.147399\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.139942\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.144803\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.135334\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.140689\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-77be7b8ffc6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-f3c10bd19d75>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, model, optimizer, trainloader, log_interval, device)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ba41fed0e61d>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;31m#X = self.f_encoder.apply(X)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_wnoise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-e6a2ed64e770>\u001b[0m in \u001b[0;36mapply_wnoise\u001b[1;34m(self, X, sd)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_wnoise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapply_gaussian_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-47a64398eaa6>\u001b[0m in \u001b[0;36mapply_gaussian_noise\u001b[1;34m(tensor, sd, device)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# noise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mapply_gaussian_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = \"crypto_net1\"\n",
    "test(model1, test_loader)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch, model1, optimizer1, train_loader)\n",
    "    test(model1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('MSE loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"figures\\cryptography\\\\reconstructed_160D.png\"\n",
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "reconstructed_example_data = model1(example_data.to(torch.device(\"cuda:0\"))).cpu().detach().numpy()\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(reconstructed_example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "#plt.savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
