{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x200087df1f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "random_seed = 420\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "batch_size_train = 64\n",
    "batch_size_test = 500\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network flowchart:\n",
    "#input vector --- random initialized weights ---> hyperdimensional vecotor ---> one_hot_net l1 ---> ...ln... ---> sigmoid/softmax output\n",
    "#One hot net\n",
    "\n",
    "class One_hot_op(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, W, epsilon):\n",
    "        Z = torch.matmul(W, A)\n",
    "        ctx.Z = Z\n",
    "        ctx.A = A\n",
    "        ctx.W = W\n",
    "        ctx.epsilon = epsilon\n",
    "        ret = Z > epsilon\n",
    "        #print(ret[1:10][1:10])\n",
    "        return ret.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dL_dA):\n",
    "        step = ctx.Z > ctx.epsilon\n",
    "        step = step.float()\n",
    "        dL_dZ = dL_dA * step \n",
    "        \n",
    "        dZ_dW = torch.transpose(ctx.A, 0,1)\n",
    "        dZ_dW = torch.sign(dZ_dW)\n",
    "        dZ_dA = torch.transpose(ctx.W, 0,1)\n",
    "        dZ_dA = torch.sign(dZ_dA)\n",
    "        dA = torch.matmul(dZ_dA,dL_dZ)\n",
    "        dW = torch.matmul(dL_dZ,dZ_dW)\n",
    "        return dA, dW, None\n",
    "\n",
    "\n",
    "class One_hot_layer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, initialization_f, epsilon):\n",
    "        super(One_hot_layer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.initialization_f = initialization_f\n",
    "        initialized_weight = initialization_f(out_dim, in_dim)\n",
    "        self.weight = nn.Parameter(initialized_weight, requires_grad = True)\n",
    "        self.op = One_hot_op\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_dim) + ',' \\\n",
    "               + str(self.out_dim) + ')'\n",
    "        \n",
    "    def forward(self, A):\n",
    "        #print(self.weight[:3][:3])\n",
    "        return self.op.apply(A, self.weight, self.epsilon)\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,initialization_f = None,device = torch.device(\"cuda:0\")):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.device = device\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_dim, in_dim), requires_grad = True)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim,1), requires_grad = True)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_dim) + ',' \\\n",
    "               + str(self.out_dim) + ')'\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / self.weight.size(1) ** 1 / 2\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, X, noise = None):\n",
    "        if noise is None or noise == 0:\n",
    "            return torch.matmul(self.weight, X) + self.bias\n",
    "        else:\n",
    "            return torch.matmul(apply_gaussian_noise(self.weight, noise, device = self.device), X) + apply_gaussian_noise(self.bias, noise, device = self.device)\n",
    "\n",
    "class One_hot_net(nn.Module):\n",
    "    def __init__(self, in_dim, n_class, f_encoder, encoder_multiplier, f_initializer, epsilon, n_layers=2, layer_size_factor=[1, 5], dropout=[-1, 0.5]):\n",
    "        super(One_hot_net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.in_dim = in_dim\n",
    "        feature_len = in_dim * encoder_multiplier\n",
    "        self.feature_len = feature_len\n",
    "        self.n_layers=n_layers\n",
    "        self.layer_size_factor=layer_size_factor\n",
    "        self.dropout=dropout\n",
    "        self.epsilon = epsilon\n",
    "        self.n_class = n_class\n",
    "        self.f_encoder = f_encoder\n",
    "        self.f_initializer = f_initializer\n",
    "        for i in range(n_layers):\n",
    "            if dropout[i] > 0:\n",
    "                self.layers.append(nn.Dropout(dropout[i]))\n",
    "            if i < n_layers - 1:\n",
    "                self.layers.append(\n",
    "                    One_hot_layer(int(feature_len // layer_size_factor[i]), int(feature_len // layer_size_factor[i + 1]), f_initializer, epsilon))\n",
    "        #self.tail = nn.Linear(int(feature_len // layer_size_factor[-1]), n_class)\n",
    "        self.tail = Linear(int(feature_len // layer_size_factor[-1]), n_class, f_initializer)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    \n",
    "    def flatten(self, X):\n",
    "        return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        X = self.f_encoder.apply_wnoise(X, sd = 0.2)\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        #X = torch.transpose(X, 0, 1)\n",
    "        #X = apply_binary_noise(X, 0.05)\n",
    "        X = self.tail(X, noise = 0.2)\n",
    "        return self.out(X).T\n",
    "\n",
    "class toy_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_Net, self).__init__()\n",
    "        self.fc2 = Linear(784, 10)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    def flatten(self, X):\n",
    "            return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = x.T\n",
    "        x = (x > -0.4).float()\n",
    "        x = self.fc2(x, noise = 0)\n",
    "        return self.out(x).T\n",
    "    \n",
    "class toy_Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_Net2, self).__init__()\n",
    "        self.fc1 = Linear(784, 20*784)\n",
    "        self.fc2 = Linear(20*784, 10)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    def flatten(self, X):\n",
    "            return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = x.T\n",
    "        x = self.fc1(x, noise = 0.5)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x, noise = 0.5)\n",
    "        return self.out(x).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise \n",
    "def apply_gaussian_noise(tensor, sd, device = torch.device(\"cuda:0\")):\n",
    "    tensor = tensor + (sd)*torch.randn(*tuple(tensor.shape)).to(device)\n",
    "    return tensor\n",
    "\n",
    "def apply_binary_noise(tensor, p, device = torch.device(\"cuda:0\")):\n",
    "    #change 1 entries to 0\n",
    "    tensor = tensor * (torch.rand(*tuple(tensor.shape))>(p/2)).float().to(device)\n",
    "    #change 0 entries to 1\n",
    "    tensor = tensor + (torch.rand(*tuple(tensor.shape))>(1-p/2)).float().to(device)\n",
    "    return torch.sign(tensor)\n",
    "\n",
    "#a = [[1,1,0,0,1,1], [1,1,0,0,1,1], [1,1,0,0,1,1]]\n",
    "#a = torch.tensor(a)\n",
    "#print((torch.rand(*tuple(a.shape))<(0.1/2)).float())\n",
    "#print(a)\n",
    "#print(apply_binary_noise(a, 0.1, device = torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializers and encoders\n",
    "def uniform_initializer(out_dim, in_dim, cuda = True):\n",
    "    tensor = torch.empty(out_dim, in_dim)\n",
    "    if cuda:\n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2).cuda()\n",
    "    else: \n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2)\n",
    "\n",
    "class simple_encoder():\n",
    "    def __init__(self, out_dim, in_dim):\n",
    "        self.W = uniform_initializer(out_dim, in_dim)\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return torch.matmul(self.W, X)\n",
    "    \n",
    "class simple_encoder_wthreshold():\n",
    "    def __init__(self, out_dim, in_dim, epsilon, cuda = True):\n",
    "        self.W = uniform_initializer(out_dim, in_dim, cuda)\n",
    "        self.device = torch.device(\"cuda:0\") if cuda else torch.device(\"cpu\")\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def apply(self, X):\n",
    "        return (torch.matmul(self.W, X) > self.epsilon).float()\n",
    "    \n",
    "    def apply_wnoise(self, X, sd):\n",
    "        return (torch.matmul(apply_gaussian_noise(self.W, sd, device = self.device), X) > self.epsilon).float()\n",
    "    \n",
    "class non_linear_encoder():\n",
    "    def __init__(self, out_dim, in_dim, activation, cuda = True):\n",
    "        self.W = uniform_initializer(out_dim, in_dim, cuda)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return self.activation((torch.matmul(self.W, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'in_dim': 784,\n",
    "    'n_class': 10,\n",
    "    'f_encoder': simple_encoder_wthreshold(784*80, 784, 10e-3),\n",
    "    'f_initializer': uniform_initializer,\n",
    "    'encoder_multiplier': 80,\n",
    "    'epsilon': 10e-3,\n",
    "    'n_layers': 1,\n",
    "    'layer_size_factor': [1, 1, 1],\n",
    "    'dropout': [0.3, -1, -1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "#model1 = One_hot_net(parameters['in_dim'], parameters['n_class'], parameters['f_encoder'], parameters['encoder_multiplier'], \n",
    "#                     parameters['f_initializer'], parameters['epsilon'], parameters['n_layers'], \n",
    "#                     parameters['layer_size_factor'], parameters['dropout']).to(device)\n",
    "\n",
    "model1 = toy_Net2().to(device)\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.01, momentum=0.5)\n",
    "#optimizer1 = torch.optim.SGD([{'params': model1.layers.parameters(), 'lr': 0.01}, {'params': model1.tail.parameters(), 'lr': 0.01}], lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                    [-1, 2]      12,308,800\n",
      "            Linear-2                    [-1, 2]         156,810\n",
      "        LogSoftmax-3                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 12,465,610\n",
      "Trainable params: 12,465,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 47.55\n",
      "Estimated Total Size (MB): 47.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model1, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, trainloader, log_interval = 10, device = torch.device(\"cuda:0\")):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "            (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            #torch.save(model.state_dict(), '/results/model.pth')\n",
    "            #torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device = torch.device(\"cuda:0\")):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 989.9427, Accuracy: 1035/10000 (10.3%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 652.275696\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2418.047363\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2249.581055\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2486.112061\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1975.661377\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1426.111084\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1322.042969\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 379.770630\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 409.252350\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 944.774475\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 725.892334\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 325.927551\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 363.595123\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 419.950012\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 573.729309\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 174.070679\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 432.078491\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 142.265198\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 327.872131\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 352.558044\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 138.156616\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 492.044739\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 333.157837\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 200.327911\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 152.822311\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 303.180603\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 89.008804\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 150.191177\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 159.888382\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 217.864441\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 212.397766\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 258.670410\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 156.175430\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 189.472504\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 325.509796\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 283.875122\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 289.150940\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 176.944183\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 175.942383\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 161.290054\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 139.943054\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 237.289764\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 221.647980\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 379.855103\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 185.681015\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 111.323524\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 187.879517\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 86.531578\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 179.117142\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 184.516281\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 129.952072\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 213.004852\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 267.007538\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 199.532532\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 207.753998\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 191.463379\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 181.002289\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 157.270370\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 170.285034\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 299.787689\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 76.227028\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 239.000137\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 247.704224\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 126.231003\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 184.836472\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 142.396408\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 85.550583\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 83.442413\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 219.570511\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 86.250664\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 192.463440\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 171.896973\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 154.697311\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 212.250687\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 37.542301\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 182.554657\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 103.960793\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 137.453156\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 152.278671\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 137.015259\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 114.353714\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 136.943069\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 106.964897\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 169.378021\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 200.448669\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 70.183777\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 138.251221\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 156.063950\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 165.367188\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 104.285736\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 224.716766\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 44.850128\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 231.053772\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 179.047333\n",
      "\n",
      "Test set: Avg. loss: 174.3518, Accuracy: 7842/10000 (78.4%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 188.469727\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 217.571930\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 177.634003\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 234.524109\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 175.570145\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 150.367706\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 68.825073\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 53.099285\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 161.303726\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 128.547180\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 104.252350\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 38.685017\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 20.684307\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 139.276779\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 95.449249\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 76.297806\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 77.061935\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 88.815117\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 151.396210\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 100.206711\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 49.478569\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 120.038177\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 88.633347\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 173.881790\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 106.891663\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 255.105423\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 89.193863\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 179.365631\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 112.709503\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 89.752594\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 132.984024\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 60.358871\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 53.259907\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 88.222656\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 249.890121\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 194.645874\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 80.478844\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 76.862495\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 58.434929\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 164.362335\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 47.751102\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 40.360153\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 126.081535\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 114.678970\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 187.878922\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 122.634880\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 231.030319\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 110.881165\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 115.791397\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 167.709320\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 59.432236\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 171.609558\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 57.075035\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 105.573563\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 138.629623\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 128.630493\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 111.310883\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 254.737686\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 77.006310\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 141.337051\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 60.221157\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 74.030029\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 167.129807\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 175.882187\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 172.241592\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 63.976349\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 89.874832\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 199.910339\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 175.336975\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 152.402893\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 110.200462\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 145.081741\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 51.090065\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 128.139526\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 97.938705\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 167.841949\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 137.085754\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 159.433228\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 95.165726\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 72.964272\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 116.400017\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 24.292128\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 116.217743\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 51.709282\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 27.996548\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 119.710510\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 51.291771\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 117.987808\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 151.384995\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 70.005516\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 142.979080\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 64.758926\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 92.644081\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 119.738190\n",
      "\n",
      "Test set: Avg. loss: 100.3062, Accuracy: 8518/10000 (85.2%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 125.229309\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 58.346317\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 128.013489\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 29.070507\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 139.354004\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 121.689392\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 33.654305\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 130.910919\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 94.318977\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 306.236664\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 44.223232\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 91.744095\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 153.072296\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 46.857456\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 207.550934\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 76.044968\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 44.088463\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 100.077637\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 85.568802\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 63.287239\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 83.829666\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 144.157593\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 56.573532\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 86.626823\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 73.656357\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 72.822914\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 72.180595\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 86.777832\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 143.467178\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 50.922798\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 201.457108\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 148.170853\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 78.787392\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 64.920624\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 75.272659\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 116.191437\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 63.842068\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 230.354446\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 160.646805\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 92.251274\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 62.192627\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 101.120209\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 81.322754\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 118.590309\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 70.269096\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 83.885757\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 95.953537\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 87.705833\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 63.165550\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 110.932266\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 114.923836\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 22.997152\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 26.689142\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 113.692772\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 113.600739\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 108.091736\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 96.516098\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 123.811111\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 152.937103\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 98.566872\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 85.466362\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 116.849953\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 73.653976\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 56.261890\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 63.463039\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 113.885963\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 53.171856\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 42.968887\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 96.858986\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 173.863708\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 18.146072\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 69.355553\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 35.611061\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 70.913742\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 73.695602\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 48.863518\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 64.947601\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 127.819221\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 108.639069\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 162.281006\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 51.923820\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 190.355392\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 90.971107\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 21.849348\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 102.753372\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 75.198280\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 52.169006\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 158.942230\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 35.031250\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 91.523354\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 50.724533\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 93.801392\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 64.085518\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 136.651550\n",
      "\n",
      "Test set: Avg. loss: 74.3467, Accuracy: 8726/10000 (87.3%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 65.005211\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 80.563194\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 78.284660\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 50.435974\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 58.278481\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 78.673950\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 40.987606\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 75.072968\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 14.229749\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 67.169380\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 103.535263\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 88.729431\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 109.092239\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 90.468506\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 40.489243\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 105.239326\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 174.368134\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 98.620590\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 94.587463\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 35.758060\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 65.125359\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 50.122459\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 106.734985\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 67.149673\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 64.645973\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 17.849421\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 85.326302\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 135.122910\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 75.506203\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 56.181637\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 95.085098\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 52.623886\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 105.234039\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 150.075867\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 104.353455\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 75.183289\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 64.295502\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 58.053410\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 83.507027\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 90.902046\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 32.728508\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 107.884781\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 87.560135\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 99.392395\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 106.321175\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 85.451538\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 144.254654\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 46.636574\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 81.059425\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 31.167290\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 29.867422\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 126.247009\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 73.958832\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 39.271217\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 56.669682\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 43.270741\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 103.245964\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 130.534241\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 99.960228\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 52.084778\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 115.192642\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 29.225477\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 63.605114\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 85.044693\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 50.231697\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 39.531502\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 95.746979\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 23.233276\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 98.738571\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 84.501602\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 58.875290\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 30.538330\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 55.239216\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 38.102203\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 87.024391\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 40.380768\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 105.530884\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 56.172089\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 27.824007\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 92.501045\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 29.217876\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 124.462418\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 24.806252\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 64.608643\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 30.477753\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 58.835701\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 39.044510\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 54.681099\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 76.983429\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 99.310776\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 78.223701\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 29.092583\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 40.705311\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 25.519918\n",
      "\n",
      "Test set: Avg. loss: 63.4120, Accuracy: 8816/10000 (88.2%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 50.826126\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 64.747757\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 10.326517\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 89.431145\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 104.050560\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 110.136185\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 115.054169\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 48.325123\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 71.676712\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 50.368160\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 13.956757\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 46.786964\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 51.974976\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 9.271238\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 23.621771\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 74.128387\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 86.847458\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 31.667282\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 86.884407\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 25.516800\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 77.030571\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 106.746460\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 77.921135\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 37.957954\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 78.169739\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 15.895549\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 63.845867\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 29.115694\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 107.464096\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 48.832767\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 57.223335\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 50.549316\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 60.978813\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 52.785984\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 37.699051\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 82.816368\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 56.090424\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 117.953545\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 86.087936\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 17.352558\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 40.599777\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 51.314018\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 52.217102\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 34.095669\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 17.065279\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 19.404476\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 33.989830\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 28.259228\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 26.866489\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 94.074814\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 42.288246\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 38.773830\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 22.677671\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 15.082788\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 61.471046\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 40.464203\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 59.373585\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 72.041306\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 51.546959\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 147.032486\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 77.044106\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 61.614571\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 70.116432\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 76.652122\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 23.590908\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 31.938169\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 57.617256\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 28.783237\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 35.110527\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 47.629368\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 56.386528\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 18.164537\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 20.687799\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 25.703117\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 32.199821\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 29.731249\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 29.022335\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 44.513691\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 43.445869\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 38.550774\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 28.218998\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 57.977367\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 18.338139\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 41.107906\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 41.883476\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 43.513580\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 40.743690\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 82.417633\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 35.847645\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 32.065723\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 26.853912\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 65.142715\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 9.447006\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 40.142532\n",
      "\n",
      "Test set: Avg. loss: 50.1164, Accuracy: 8927/10000 (89.3%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 28.659714\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 27.850996\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 56.463062\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 17.119539\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 68.738083\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 61.502769\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 27.737415\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 83.690239\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 47.165638\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 45.777916\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 52.284126\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 26.436956\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 53.202984\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 22.688198\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 42.709709\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 33.173386\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 39.142990\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 42.721367\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 104.462494\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 98.427757\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 146.706436\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 42.107941\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 69.597115\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 98.782349\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 101.069321\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 74.140289\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 78.187683\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 67.679634\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 57.062206\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 11.392790\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 90.290970\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 38.457081\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 117.941536\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 33.713474\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 31.848080\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 156.451935\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 32.396469\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 41.492363\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 62.085724\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 44.713799\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 38.598297\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 20.126198\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 50.427757\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 23.135353\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 56.269653\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 88.082214\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 54.093285\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 21.730000\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 35.581600\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 45.973133\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 64.462990\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 91.185410\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 55.031921\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 82.912804\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 56.036301\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 17.934315\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 28.516144\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 37.159546\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 57.713295\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 49.739536\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 49.717594\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 25.212265\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 17.510529\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 77.978340\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 56.251392\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 46.046402\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 21.190800\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 64.879128\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 15.454088\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 27.051043\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 50.777267\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 40.756378\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 92.006577\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 31.454117\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 21.901360\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 10.264994\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 152.205856\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 45.019081\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 45.894241\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 39.515095\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 19.142580\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 65.175980\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 11.806443\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 49.839195\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 19.265720\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 71.990776\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 48.606159\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 33.492538\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 40.484917\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 22.733749\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 61.478760\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 105.399391\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 13.463651\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 87.365021\n",
      "\n",
      "Test set: Avg. loss: 48.4733, Accuracy: 8934/10000 (89.3%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 33.825279\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 73.926468\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 31.386681\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 43.115562\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 87.088776\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 59.103664\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 53.776890\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 57.716854\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 57.625450\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 49.820923\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 87.970116\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 25.584188\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 24.858471\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 20.440338\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 49.195137\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 39.523705\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 73.517509\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 35.668526\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 58.156239\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 44.118248\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 31.778637\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 38.592823\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 28.813574\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 66.229309\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 44.817287\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 25.925819\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 6.973425\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 75.413078\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 63.091579\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 35.011028\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 21.319082\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 63.848320\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 36.246513\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 56.864876\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 39.421761\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 7.903932\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 80.796585\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 34.069805\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 67.150475\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 52.711643\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 84.989861\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 34.621147\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 43.965023\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 8.801968\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 107.898430\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 50.839508\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 20.175671\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 43.797997\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 32.939583\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 105.787270\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 74.885605\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 26.530470\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 49.617386\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 106.832428\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 36.887810\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 17.988321\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 18.661484\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 83.431137\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 20.485830\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 24.205166\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 31.416700\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 94.399132\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 96.252914\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 50.535110\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 29.633085\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 92.705765\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 48.484062\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 10.209765\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 31.751932\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 42.703369\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 54.500717\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 10.513336\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 61.034370\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 17.279114\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 14.645575\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 80.985886\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 53.828674\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 90.055717\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 52.488083\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 13.178372\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 65.326050\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 31.958328\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 25.893219\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 21.668943\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 64.683914\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 23.149681\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 33.001446\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 110.161804\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 44.187874\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 27.779949\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 42.598351\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 27.083363\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 20.962208\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 39.058426\n",
      "\n",
      "Test set: Avg. loss: 39.1292, Accuracy: 9057/10000 (90.6%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 37.878979\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 41.261150\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 30.561972\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 32.787048\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 87.576256\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 76.410172\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 40.970314\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 26.823381\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 113.489578\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 42.485039\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 31.277967\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 45.716820\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 6.815411\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 36.406525\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 31.590160\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 64.221329\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 65.863525\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 45.978722\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 79.111839\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 15.107702\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 40.717873\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 26.405354\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 42.667915\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 66.601875\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 54.141281\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 21.087662\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 27.284306\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 30.030434\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 45.143280\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 48.711617\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 37.396568\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 71.517906\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 49.793030\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 25.696110\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 24.149590\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 51.777008\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 20.152283\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 87.871529\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 22.632912\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 20.142109\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 30.807047\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 27.904602\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 17.759914\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 65.373016\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 24.546602\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 6.447790\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 45.710236\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 15.860128\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 105.649475\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 40.845676\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 26.812016\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 36.064468\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 40.087036\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 81.467476\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 21.693224\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 36.145607\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 21.019592\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 39.087971\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 59.948277\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 18.278404\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 38.517189\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 8.094499\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 28.463398\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 13.427515\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 23.582121\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 27.283243\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 15.305688\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 51.433319\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 27.559019\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 27.737305\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 31.060295\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 19.917622\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 10.307107\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 4.086437\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 37.086617\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 39.318466\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 42.555866\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 47.070930\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 23.933483\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 64.038124\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 34.404457\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 16.179886\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 12.900309\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 20.984751\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 23.853220\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 44.163475\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 35.152443\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 5.741283\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 14.920904\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 11.137770\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 31.380713\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 12.551336\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 39.828957\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 66.356071\n",
      "\n",
      "Test set: Avg. loss: 34.8159, Accuracy: 9132/10000 (91.3%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 13.640483\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 34.214455\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 60.156513\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 33.723667\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 83.534782\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 25.792576\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 18.738579\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 55.737175\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 91.113174\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 64.857315\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 28.661423\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 17.091795\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 12.516216\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 44.231899\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 23.929707\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 32.367035\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 72.935516\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 19.812103\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 13.906887\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 28.202232\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 33.813873\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 27.776207\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 54.373119\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 24.897051\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 48.135693\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 12.850872\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 27.456415\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 35.463417\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 51.162689\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 10.847727\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 25.029306\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 13.078074\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 23.162037\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 15.283360\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 24.188246\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 21.623272\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 26.282955\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 65.169601\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 25.652679\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 39.135433\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 30.729063\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 31.184362\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 3.960136\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 64.868004\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 16.248804\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 16.686405\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 12.503368\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 49.008408\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 17.908993\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 15.196420\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 52.787418\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 55.146660\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 46.314983\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 20.352423\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 36.117252\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 47.030811\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 21.051918\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 22.393127\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 6.959362\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 36.725677\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 33.021263\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 51.324703\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 21.904585\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 25.547836\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 44.092102\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 27.360182\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 2.023144\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 29.791183\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 20.196991\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 53.937141\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 31.597601\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 73.461929\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 38.694309\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 7.264187\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 7.919834\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 16.448109\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 27.642076\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 79.493141\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 41.176609\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 3.443875\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 16.958815\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 27.936832\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 32.008732\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 47.634262\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 12.090321\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 34.741814\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 12.545959\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 40.821243\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 46.398888\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 32.676247\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 14.767369\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 15.941740\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 31.172176\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 8.535000\n",
      "\n",
      "Test set: Avg. loss: 29.1595, Accuracy: 9229/10000 (92.3%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 26.561562\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 54.584454\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 39.280209\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 74.789948\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 52.465385\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 9.661159\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 15.237234\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 31.938713\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 27.292267\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 16.661140\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 28.293631\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 18.501266\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 21.418407\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 23.896860\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 27.868393\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 48.196922\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 36.532837\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 7.031067\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 18.679581\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 19.574574\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 34.792282\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 10.662081\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 27.451092\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 5.971502\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 21.872499\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 1.079754\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 27.195202\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 32.624783\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 13.833767\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 31.056015\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 4.892965\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 48.485832\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 7.047002\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 8.252611\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 36.268326\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 28.676617\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 67.560356\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 27.149071\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 41.727852\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 35.604858\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 22.797142\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 14.391399\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 22.665922\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 20.219763\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 18.454464\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 42.016273\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 47.277416\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 22.378479\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 14.566593\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 11.031210\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 23.719627\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 10.914840\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 47.346504\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 35.878464\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 12.856714\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 53.534016\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 36.860817\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 32.258839\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 26.466194\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 15.577167\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 37.286827\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 56.899513\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 29.899179\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 13.860368\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 31.474154\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 37.293415\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 40.734131\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 14.887686\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 23.383226\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 55.835785\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 61.861347\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 16.165783\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 12.910295\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 39.793510\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 25.208025\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 23.236742\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 19.926254\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 26.842436\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 13.228618\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 26.028294\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 7.267365\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 24.029285\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 13.015626\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 26.078735\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 20.241947\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 36.348415\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 32.812981\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 9.777205\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 50.870636\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 7.384500\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 55.721512\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 21.574430\n",
      "\n",
      "Test set: Avg. loss: 26.0035, Accuracy: 9265/10000 (92.6%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 49.188210\n",
      "Train Epoch: 11 [640/60000 (1%)]\tLoss: 9.798765\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 6.566690\n",
      "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 33.009563\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 16.936064\n",
      "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 50.682648\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 29.315298\n",
      "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 20.589790\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 11.383906\n",
      "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 21.218613\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 21.777840\n",
      "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 37.273487\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 32.725334\n",
      "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 5.870517\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 71.826843\n",
      "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 17.328930\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 30.444477\n",
      "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 28.254087\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 21.982628\n",
      "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 90.585770\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 3.740763\n",
      "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 4.598429\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 10.612468\n",
      "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 67.751045\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 2.396050\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 15.471841\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 24.733559\n",
      "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 53.754425\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 19.507538\n",
      "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 8.280190\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 24.133667\n",
      "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 20.004023\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 18.864220\n",
      "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 31.111874\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 16.827545\n",
      "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 22.620577\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 29.203590\n",
      "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 12.320372\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 7.879899\n",
      "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 42.038063\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 32.801022\n",
      "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 32.189293\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 22.724415\n",
      "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 51.923351\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 4.964652\n",
      "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 15.072429\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 45.019886\n",
      "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 1.351267\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 39.901546\n",
      "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 32.449974\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 23.960526\n",
      "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 17.452444\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 30.978067\n",
      "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 14.211352\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 16.108566\n",
      "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 3.422402\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 39.658936\n",
      "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 22.083035\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 7.815094\n",
      "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 15.476008\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 52.435234\n",
      "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 3.962670\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 31.312962\n",
      "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 5.727284\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 29.361233\n",
      "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 5.156323\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 14.910305\n",
      "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 27.389793\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 17.977352\n",
      "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 47.520042\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 11.328268\n",
      "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 12.124879\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 23.629662\n",
      "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 48.208313\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 66.602600\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 18.196915\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 31.202118\n",
      "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 13.417104\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 10.540424\n",
      "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 18.707796\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 3.396808\n",
      "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 26.579268\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 60.703506\n",
      "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 45.555134\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 26.116499\n",
      "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 48.352661\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 20.068724\n",
      "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 19.486267\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 22.848600\n",
      "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 27.904322\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 9.294680\n",
      "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 36.021713\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 10.114984\n",
      "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 24.349762\n",
      "\n",
      "Test set: Avg. loss: 25.8278, Accuracy: 9221/10000 (92.2%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 1.779564\n",
      "Train Epoch: 12 [640/60000 (1%)]\tLoss: 41.965874\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 39.447945\n",
      "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 45.672569\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 24.697395\n",
      "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 31.966717\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 45.721451\n",
      "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 22.314789\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 23.952236\n",
      "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 30.350918\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 24.339851\n",
      "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.000001\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 11.027452\n",
      "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 57.723473\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 13.338487\n",
      "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 8.216814\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 41.211212\n",
      "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 24.259811\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 22.931108\n",
      "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 4.545814\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 20.293898\n",
      "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 10.041682\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 22.753950\n",
      "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 11.310790\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 22.261997\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 20.409006\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 58.422638\n",
      "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 10.194032\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 26.852337\n",
      "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 9.767040\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 71.077423\n",
      "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 31.715313\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 61.476341\n",
      "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 28.587666\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 24.295103\n",
      "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 5.100250\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 10.446411\n",
      "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 5.401707\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 11.454614\n",
      "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 34.757751\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 25.984398\n",
      "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 1.538116\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 40.715767\n",
      "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 14.274242\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 27.322618\n",
      "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 35.548267\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 36.831524\n",
      "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 3.658250\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 35.226097\n",
      "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 14.334277\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 11.622103\n",
      "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 39.001858\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.932518\n",
      "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 40.180790\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 30.251749\n",
      "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 11.744761\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 17.801537\n",
      "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 41.072971\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 20.675873\n",
      "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 58.072136\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 23.325207\n",
      "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 18.376343\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 4.868126\n",
      "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 37.763519\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 14.823400\n",
      "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 28.727463\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 5.128625\n",
      "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 29.380312\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 17.942142\n",
      "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 18.696016\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 41.475254\n",
      "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 22.731258\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 18.361240\n",
      "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 19.813318\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 4.240306\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 21.836658\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 28.616854\n",
      "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 18.312023\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 8.175733\n",
      "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 16.012287\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 2.306166\n",
      "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 19.277924\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 15.234234\n",
      "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 11.198348\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 13.764767\n",
      "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 29.565422\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 3.748494\n",
      "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 5.923715\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 28.192715\n",
      "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 59.894150\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 23.684366\n",
      "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 22.704029\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 19.001114\n",
      "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 10.200768\n",
      "\n",
      "Test set: Avg. loss: 23.1249, Accuracy: 9309/10000 (93.1%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 43.054253\n",
      "Train Epoch: 13 [640/60000 (1%)]\tLoss: 22.944912\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 35.595310\n",
      "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 18.860422\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 22.681377\n",
      "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 35.465225\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 18.869644\n",
      "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 12.494312\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 3.310565\n",
      "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 43.013443\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 9.333886\n",
      "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 7.519534\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 14.975685\n",
      "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 34.350891\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 32.619942\n",
      "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 3.534332\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 68.378357\n",
      "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 21.677563\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 5.808804\n",
      "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 27.328709\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 10.438509\n",
      "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 22.907848\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 9.073780\n",
      "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 32.841511\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 21.752930\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 6.715901\n",
      "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 20.721930\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 26.541004\n",
      "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 51.452850\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 3.520637\n",
      "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 28.052008\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 38.073151\n",
      "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 6.089406\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 11.047371\n",
      "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 15.557621\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 31.271326\n",
      "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 11.897642\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 23.020966\n",
      "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 14.273828\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 5.520418\n",
      "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 4.633152\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 2.016301\n",
      "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 46.839233\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 8.483807\n",
      "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 20.423904\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 46.706970\n",
      "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 25.040543\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 6.755413\n",
      "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 17.390621\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 27.845955\n",
      "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 1.958505\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 25.608713\n",
      "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 38.701664\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 21.242218\n",
      "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 9.356138\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 2.347438\n",
      "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 7.628585\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 8.228999\n",
      "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 17.859808\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 31.093477\n",
      "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 58.707619\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 6.239151\n",
      "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 21.702991\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 23.355005\n",
      "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 3.238911\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 16.388092\n",
      "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 21.059793\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 12.312249\n",
      "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 16.488544\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 19.957066\n",
      "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 58.891068\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 17.615128\n",
      "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 17.978283\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 11.101710\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 35.710545\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 40.900597\n",
      "Train Epoch: 13 [49280/60000 (82%)]\tLoss: 33.336620\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 31.614113\n",
      "Train Epoch: 13 [50560/60000 (84%)]\tLoss: 12.405915\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 17.691128\n",
      "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 27.269508\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 34.374580\n",
      "Train Epoch: 13 [53120/60000 (88%)]\tLoss: 34.085846\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 15.183278\n",
      "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 14.636670\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 36.416759\n",
      "Train Epoch: 13 [55680/60000 (93%)]\tLoss: 32.531494\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 2.995915\n",
      "Train Epoch: 13 [56960/60000 (95%)]\tLoss: 14.633875\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 22.016556\n",
      "Train Epoch: 13 [58240/60000 (97%)]\tLoss: 43.807495\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 38.256168\n",
      "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 9.502693\n",
      "\n",
      "Test set: Avg. loss: 21.4815, Accuracy: 9334/10000 (93.3%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 16.083490\n",
      "Train Epoch: 14 [640/60000 (1%)]\tLoss: 35.109535\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 25.432060\n",
      "Train Epoch: 14 [1920/60000 (3%)]\tLoss: 29.454811\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 20.343700\n",
      "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 30.973593\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 1.599622\n",
      "Train Epoch: 14 [4480/60000 (7%)]\tLoss: 18.203674\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 3.043238\n",
      "Train Epoch: 14 [5760/60000 (10%)]\tLoss: 5.005310\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 24.060291\n",
      "Train Epoch: 14 [7040/60000 (12%)]\tLoss: 18.568016\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 38.120831\n",
      "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 1.666829\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 30.473454\n",
      "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 44.424194\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 18.177746\n",
      "Train Epoch: 14 [10880/60000 (18%)]\tLoss: 39.412388\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 26.887949\n",
      "Train Epoch: 14 [12160/60000 (20%)]\tLoss: 11.802570\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 12.488427\n",
      "Train Epoch: 14 [13440/60000 (22%)]\tLoss: 7.580678\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 20.003887\n",
      "Train Epoch: 14 [14720/60000 (25%)]\tLoss: 41.314884\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 40.621674\n",
      "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 6.090719\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 28.475897\n",
      "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 12.916404\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 3.333471\n",
      "Train Epoch: 14 [18560/60000 (31%)]\tLoss: 10.609953\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 12.292110\n",
      "Train Epoch: 14 [19840/60000 (33%)]\tLoss: 6.243778\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 13.998523\n",
      "Train Epoch: 14 [21120/60000 (35%)]\tLoss: 15.563875\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 5.763886\n",
      "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 42.666016\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 3.463295\n",
      "Train Epoch: 14 [23680/60000 (39%)]\tLoss: 20.453737\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 18.726536\n",
      "Train Epoch: 14 [24960/60000 (42%)]\tLoss: 40.878311\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 23.937147\n",
      "Train Epoch: 14 [26240/60000 (44%)]\tLoss: 47.077332\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 19.279415\n",
      "Train Epoch: 14 [27520/60000 (46%)]\tLoss: 13.450291\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 21.174406\n",
      "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 12.292575\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 40.158493\n",
      "Train Epoch: 14 [30080/60000 (50%)]\tLoss: 11.607660\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 19.956953\n",
      "Train Epoch: 14 [31360/60000 (52%)]\tLoss: 17.085697\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 11.711718\n",
      "Train Epoch: 14 [32640/60000 (54%)]\tLoss: 55.032711\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 14.558372\n",
      "Train Epoch: 14 [33920/60000 (57%)]\tLoss: 17.926334\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 58.297150\n",
      "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 58.683418\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 26.288372\n",
      "Train Epoch: 14 [36480/60000 (61%)]\tLoss: 12.022207\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [37760/60000 (63%)]\tLoss: 7.542602\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 1.261226\n",
      "Train Epoch: 14 [39040/60000 (65%)]\tLoss: 40.259075\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 27.455524\n",
      "Train Epoch: 14 [40320/60000 (67%)]\tLoss: 2.574883\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 18.265718\n",
      "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 28.482792\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 14.804906\n",
      "Train Epoch: 14 [42880/60000 (71%)]\tLoss: 8.621304\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 29.025595\n",
      "Train Epoch: 14 [44160/60000 (74%)]\tLoss: 7.992027\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 39.842453\n",
      "Train Epoch: 14 [45440/60000 (76%)]\tLoss: 16.924652\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 13.708798\n",
      "Train Epoch: 14 [46720/60000 (78%)]\tLoss: 2.857629\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 14.496185\n",
      "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 9.110727\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 9.944522\n",
      "Train Epoch: 14 [49280/60000 (82%)]\tLoss: 9.945069\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 27.287346\n",
      "Train Epoch: 14 [50560/60000 (84%)]\tLoss: 45.772469\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 11.584404\n",
      "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 13.628531\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 21.799717\n",
      "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 16.366379\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 32.912720\n",
      "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 29.469500\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 30.966446\n",
      "Train Epoch: 14 [55680/60000 (93%)]\tLoss: 12.977221\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 14.015023\n",
      "Train Epoch: 14 [56960/60000 (95%)]\tLoss: 6.729765\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 20.119747\n",
      "Train Epoch: 14 [58240/60000 (97%)]\tLoss: 39.054482\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 15.401461\n",
      "Train Epoch: 14 [59520/60000 (99%)]\tLoss: 1.922669\n",
      "\n",
      "Test set: Avg. loss: 19.6312, Accuracy: 9361/10000 (93.6%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 8.132148\n",
      "Train Epoch: 15 [640/60000 (1%)]\tLoss: 12.633297\n",
      "Train Epoch: 15 [1280/60000 (2%)]\tLoss: 14.792018\n",
      "Train Epoch: 15 [1920/60000 (3%)]\tLoss: 23.998314\n",
      "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 2.778717\n",
      "Train Epoch: 15 [3200/60000 (5%)]\tLoss: 32.616253\n",
      "Train Epoch: 15 [3840/60000 (6%)]\tLoss: 9.121028\n",
      "Train Epoch: 15 [4480/60000 (7%)]\tLoss: 8.149278\n",
      "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 29.661419\n",
      "Train Epoch: 15 [5760/60000 (10%)]\tLoss: 35.624523\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 15.854448\n",
      "Train Epoch: 15 [7040/60000 (12%)]\tLoss: 32.771988\n",
      "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 18.592659\n",
      "Train Epoch: 15 [8320/60000 (14%)]\tLoss: 11.129143\n",
      "Train Epoch: 15 [8960/60000 (15%)]\tLoss: 33.302792\n",
      "Train Epoch: 15 [9600/60000 (16%)]\tLoss: 42.688347\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 58.698265\n",
      "Train Epoch: 15 [10880/60000 (18%)]\tLoss: 28.795961\n",
      "Train Epoch: 15 [11520/60000 (19%)]\tLoss: 29.051405\n",
      "Train Epoch: 15 [12160/60000 (20%)]\tLoss: 22.999327\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 44.822250\n",
      "Train Epoch: 15 [13440/60000 (22%)]\tLoss: 23.022533\n",
      "Train Epoch: 15 [14080/60000 (23%)]\tLoss: 18.352009\n",
      "Train Epoch: 15 [14720/60000 (25%)]\tLoss: 5.713089\n",
      "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 12.038290\n",
      "Train Epoch: 15 [16000/60000 (27%)]\tLoss: 37.824806\n",
      "Train Epoch: 15 [16640/60000 (28%)]\tLoss: 20.796558\n",
      "Train Epoch: 15 [17280/60000 (29%)]\tLoss: 16.919176\n",
      "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 7.218608\n",
      "Train Epoch: 15 [18560/60000 (31%)]\tLoss: 7.821714\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 39.911110\n",
      "Train Epoch: 15 [19840/60000 (33%)]\tLoss: 22.852642\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 18.936871\n",
      "Train Epoch: 15 [21120/60000 (35%)]\tLoss: 8.929865\n",
      "Train Epoch: 15 [21760/60000 (36%)]\tLoss: 10.957641\n",
      "Train Epoch: 15 [22400/60000 (37%)]\tLoss: 26.329321\n",
      "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 44.843315\n",
      "Train Epoch: 15 [23680/60000 (39%)]\tLoss: 23.611183\n",
      "Train Epoch: 15 [24320/60000 (41%)]\tLoss: 37.144318\n",
      "Train Epoch: 15 [24960/60000 (42%)]\tLoss: 19.091166\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 23.028334\n",
      "Train Epoch: 15 [26240/60000 (44%)]\tLoss: 10.868059\n",
      "Train Epoch: 15 [26880/60000 (45%)]\tLoss: 21.994316\n",
      "Train Epoch: 15 [27520/60000 (46%)]\tLoss: 11.287345\n",
      "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 15.222702\n",
      "Train Epoch: 15 [28800/60000 (48%)]\tLoss: 14.525352\n",
      "Train Epoch: 15 [29440/60000 (49%)]\tLoss: 16.122936\n",
      "Train Epoch: 15 [30080/60000 (50%)]\tLoss: 33.335175\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 38.309425\n",
      "Train Epoch: 15 [31360/60000 (52%)]\tLoss: 3.849921\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 28.972258\n",
      "Train Epoch: 15 [32640/60000 (54%)]\tLoss: 24.236176\n",
      "Train Epoch: 15 [33280/60000 (55%)]\tLoss: 17.472660\n",
      "Train Epoch: 15 [33920/60000 (57%)]\tLoss: 18.700920\n",
      "Train Epoch: 15 [34560/60000 (58%)]\tLoss: 11.506247\n",
      "Train Epoch: 15 [35200/60000 (59%)]\tLoss: 17.936117\n",
      "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 23.445679\n",
      "Train Epoch: 15 [36480/60000 (61%)]\tLoss: 43.511215\n",
      "Train Epoch: 15 [37120/60000 (62%)]\tLoss: 29.277369\n",
      "Train Epoch: 15 [37760/60000 (63%)]\tLoss: 15.866408\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 15 [39040/60000 (65%)]\tLoss: 24.151064\n",
      "Train Epoch: 15 [39680/60000 (66%)]\tLoss: 51.559769\n",
      "Train Epoch: 15 [40320/60000 (67%)]\tLoss: 11.820104\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 20.830120\n",
      "Train Epoch: 15 [41600/60000 (69%)]\tLoss: 13.517418\n",
      "Train Epoch: 15 [42240/60000 (70%)]\tLoss: 24.998547\n",
      "Train Epoch: 15 [42880/60000 (71%)]\tLoss: 10.163540\n",
      "Train Epoch: 15 [43520/60000 (72%)]\tLoss: 4.734886\n",
      "Train Epoch: 15 [44160/60000 (74%)]\tLoss: 0.000151\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 11.387786\n",
      "Train Epoch: 15 [45440/60000 (76%)]\tLoss: 22.021511\n",
      "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 7.549664\n",
      "Train Epoch: 15 [46720/60000 (78%)]\tLoss: 5.706696\n",
      "Train Epoch: 15 [47360/60000 (79%)]\tLoss: 28.543217\n",
      "Train Epoch: 15 [48000/60000 (80%)]\tLoss: 7.881932\n",
      "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 24.580511\n",
      "Train Epoch: 15 [49280/60000 (82%)]\tLoss: 17.511841\n",
      "Train Epoch: 15 [49920/60000 (83%)]\tLoss: 9.641788\n",
      "Train Epoch: 15 [50560/60000 (84%)]\tLoss: 11.673288\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 55.872047\n",
      "Train Epoch: 15 [51840/60000 (86%)]\tLoss: 1.366306\n",
      "Train Epoch: 15 [52480/60000 (87%)]\tLoss: 26.935551\n",
      "Train Epoch: 15 [53120/60000 (88%)]\tLoss: 31.539406\n",
      "Train Epoch: 15 [53760/60000 (90%)]\tLoss: 30.839962\n",
      "Train Epoch: 15 [54400/60000 (91%)]\tLoss: 17.799566\n",
      "Train Epoch: 15 [55040/60000 (92%)]\tLoss: 8.338673\n",
      "Train Epoch: 15 [55680/60000 (93%)]\tLoss: 6.926756\n",
      "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 15.265623\n",
      "Train Epoch: 15 [56960/60000 (95%)]\tLoss: 5.340346\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 9.067980\n",
      "Train Epoch: 15 [58240/60000 (97%)]\tLoss: 35.405296\n",
      "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 12.916809\n",
      "Train Epoch: 15 [59520/60000 (99%)]\tLoss: 10.018741\n",
      "\n",
      "Test set: Avg. loss: 20.2409, Accuracy: 9388/10000 (93.9%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"base_net1\"\n",
    "acc_max = test(model1, test_loader)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch, model1, optimizer1, train_loader)\n",
    "    acc = test(model1, test_loader)\n",
    "    if acc > acc_max:\n",
    "        acc = acc_max\n",
    "        print()\n",
    "        print(\"--- saving model for best accuracy ---\")\n",
    "        #torch.save(model1.state_dict(), 'results/'+model_name+'.pth')\n",
    "        #torch.save(optimizer1.state_dict(), 'results/'+model_name+'_optimizer.pth')\n",
    "        print(\"--- saving finished ---\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'negative log likelihood loss')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model1.state_dict(), 'results/base_model.pth')\n",
    "#torch.save(optimizer1.state_dict(), 'results/base_optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
