{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d019e5e1d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "random_seed = 0\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "batch_size_train = 64\n",
    "batch_size_test = 500\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root='./data', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network flowchart:\n",
    "#input vector --- random initialized weights ---> hyperdimensional vecotor ---> one_hot_net l1 ---> ...ln... ---> sigmoid/softmax output\n",
    "#One hot net\n",
    "\n",
    "class One_hot_op(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, W, epsilon):\n",
    "        Z = torch.matmul(W, A)\n",
    "        ctx.Z = Z\n",
    "        ctx.A = A\n",
    "        ctx.W = W\n",
    "        ctx.epsilon = epsilon\n",
    "        ret = Z > epsilon\n",
    "        #print(ret[1:10][1:10])\n",
    "        return ret.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dL_dA):\n",
    "        step = ctx.Z > ctx.epsilon\n",
    "        step = step.float()\n",
    "        dL_dZ = dL_dA * step \n",
    "        \n",
    "        dZ_dW = torch.transpose(ctx.A, 0,1)\n",
    "        dZ_dW = torch.sign(dZ_dW)\n",
    "        dZ_dA = torch.transpose(ctx.W, 0,1)\n",
    "        dZ_dA = torch.sign(dZ_dA)\n",
    "        dA = torch.matmul(dZ_dA,dL_dZ)\n",
    "        dW = torch.matmul(dL_dZ,dZ_dW)\n",
    "        return dA, dW, None\n",
    "\n",
    "\n",
    "class One_hot_layer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, initialization_f, epsilon):\n",
    "        super(One_hot_layer, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.initialization_f = initialization_f\n",
    "        initialized_weight = initialization_f(out_dim, in_dim)\n",
    "        self.weight = nn.Parameter(initialized_weight, requires_grad = True)\n",
    "        self.op = One_hot_op\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_dim) + ',' \\\n",
    "               + str(self.out_dim) + ')'\n",
    "        \n",
    "    def forward(self, A):\n",
    "        #print(self.weight[:3][:3])\n",
    "        return self.op.apply(A, self.weight, self.epsilon)\n",
    "\n",
    "class Linear_noise_op(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, W, noise):\n",
    "        ctx.noise = noise\n",
    "        return W\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dW):\n",
    "        #print(\"go\")\n",
    "        #print(dW.shape)\n",
    "        return apply_gaussian_noise(dW, ctx.noise, device = dW.device), None\n",
    "    \n",
    "class Linear(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,initialization_f = None,device = torch.device(\"cuda:0\")):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.device = device\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_dim, in_dim), requires_grad = True)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim,1), requires_grad = True)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_dim) + ',' \\\n",
    "               + str(self.out_dim) + ')'\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / self.weight.size(1) ** 1 / 2\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, X, noise = None, grad_noise = None):\n",
    "        #print(self.__class__.__name__, self.weight[:2,:2])\n",
    "        \n",
    "        if grad_noise is not None and grad_noise != 0:\n",
    "            W = Linear_noise_op.apply(self.weight, grad_noise)\n",
    "            B = Linear_noise_op.apply(self.bias, grad_noise)\n",
    "        else:\n",
    "            W = self.weight\n",
    "            B = self.bias\n",
    "        \n",
    "        \n",
    "        if noise is None or noise == 0:\n",
    "            return torch.matmul(W, X) + B\n",
    "        else:\n",
    "            return torch.matmul(apply_gaussian_noise(W, noise, device = self.device), X) + apply_gaussian_noise(B, noise, device = self.device)\n",
    "        '''\n",
    "        #return torch.matmul(self.weight, X) + self.bias\n",
    "        if noise is None or noise == 0:\n",
    "            return torch.matmul(self.weight, X) + self.bias\n",
    "        else:\n",
    "            return torch.matmul(apply_gaussian_noise(self.weight, noise, device = self.device), X) + apply_gaussian_noise(self.bias, noise, device = self.device)\n",
    "        '''\n",
    "class One_hot_net(nn.Module):\n",
    "    def __init__(self, in_dim, n_class, f_encoder, encoder_multiplier, f_initializer, epsilon, n_layers=2, layer_size_factor=[1, 5], dropout=[-1, 0.5]):\n",
    "        super(One_hot_net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.in_dim = in_dim\n",
    "        feature_len = int(in_dim * encoder_multiplier)\n",
    "        self.feature_len = feature_len\n",
    "        self.n_layers=n_layers\n",
    "        self.layer_size_factor=layer_size_factor\n",
    "        self.dropout=dropout\n",
    "        self.epsilon = epsilon\n",
    "        self.n_class = n_class\n",
    "        self.f_encoder = f_encoder\n",
    "        self.f_initializer = f_initializer\n",
    "        for i in range(n_layers):\n",
    "            if dropout[i] > 0:\n",
    "                self.layers.append(nn.Dropout(dropout[i]))\n",
    "            if i < n_layers - 1:\n",
    "                self.layers.append(\n",
    "                    One_hot_layer(int(feature_len // layer_size_factor[i]), int(feature_len // layer_size_factor[i + 1]), f_initializer, epsilon))\n",
    "        #self.tail = nn.Linear(int(feature_len // layer_size_factor[-1]), n_class)\n",
    "        self.tail = Linear(int(feature_len // layer_size_factor[-1]), n_class, f_initializer)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    \n",
    "    def flatten(self, X):\n",
    "        return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.flatten(X)\n",
    "        X = torch.transpose(X, 0, 1)\n",
    "        #X = self.f_encoder.apply_wnoise(X, sd = 0.5)\n",
    "        X = self.f_encoder.apply(X)\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        #X = torch.transpose(X, 0, 1)\n",
    "        #X = apply_binary_noise(X, 0.05)\n",
    "        X = self.tail(X, noise = 0, grad_noise = 0)\n",
    "        return self.out(X).T\n",
    "\n",
    "class toy_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_Net, self).__init__()\n",
    "        self.fc2 = Linear(784, 10)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    def flatten(self, X):\n",
    "            return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = x.T\n",
    "        x = (x > -0.4).float()\n",
    "        x = self.fc2(x, noise = 0)\n",
    "        return self.out(x).T\n",
    "    \n",
    "class toy_Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_Net2, self).__init__()\n",
    "        self.fc1 = Linear(784, 300)\n",
    "        self.fc2 = Linear(300, 10)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    def flatten(self, X):\n",
    "            return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = x.T\n",
    "        x = self.fc1(x, noise = 0, grad_noise = 0)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x, noise = 0, grad_noise = 0)\n",
    "        return self.out(x).T\n",
    "    \n",
    "class toy_Net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(toy_Net3, self).__init__()\n",
    "        self.fc1 = Linear(784, 400)\n",
    "        self.fc2 = Linear(400, 100)\n",
    "        self.fc3 = Linear(100, 10)\n",
    "        self.out = nn.LogSoftmax(dim = 0)\n",
    "    def flatten(self, X):\n",
    "            return X.view(X.shape[0], X.shape[1]*X.shape[2]*X.shape[3])\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = x.T\n",
    "        x = self.fc1(x, noise = 0.5, grad_noise = 1)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x, noise = 0.5, grad_noise = 1)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc3(x, noise = 0.5, grad_noise = 1)\n",
    "        return self.out(x).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise \n",
    "def apply_gaussian_noise(tensor, sd, device = torch.device(\"cuda:0\")):\n",
    "    tensor = tensor + (sd)*torch.randn(*tuple(tensor.shape)).to(device)\n",
    "    return tensor\n",
    "\n",
    "def apply_binary_noise(tensor, p, device = torch.device(\"cuda:0\")):\n",
    "    #change 1 entries to 0\n",
    "    tensor = tensor * (torch.rand(*tuple(tensor.shape))>(p/2)).float().to(device)\n",
    "    #change 0 entries to 1\n",
    "    tensor = tensor + (torch.rand(*tuple(tensor.shape))>(1-p/2)).float().to(device)\n",
    "    return torch.sign(tensor)\n",
    "\n",
    "#a = [[1,1,0,0,1,1], [1,1,0,0,1,1], [1,1,0,0,1,1]]\n",
    "#a = torch.tensor(a)\n",
    "#print((torch.rand(*tuple(a.shape))<(0.1/2)).float())\n",
    "#print(a)\n",
    "#print(apply_binary_noise(a, 0.1, device = torch.device(\"cpu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializers and encoders\n",
    "def uniform_initializer(out_dim, in_dim, cuda = True):\n",
    "    tensor = torch.empty(out_dim, in_dim)\n",
    "    if cuda:\n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2).cuda()\n",
    "    else: \n",
    "        return torch.nn.init.uniform_(tensor, a=-2, b=2)\n",
    "\n",
    "class simple_encoder():\n",
    "    def __init__(self, out_dim, in_dim):\n",
    "        self.W = uniform_initializer(out_dim, in_dim)\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return torch.matmul(self.W, X)\n",
    "    \n",
    "class simple_encoder_wthreshold():\n",
    "    def __init__(self, out_dim, in_dim, epsilon, cuda = True):\n",
    "        self.W = uniform_initializer(out_dim, in_dim, cuda)\n",
    "        self.device = torch.device(\"cuda:0\") if cuda else torch.device(\"cpu\")\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def apply(self, X):\n",
    "        return (torch.matmul(self.W, X) > self.epsilon).float()\n",
    "    \n",
    "    def apply_wnoise(self, X, sd):\n",
    "        return (torch.matmul(apply_gaussian_noise(self.W, sd, device = self.device), X) > self.epsilon).float()\n",
    "    \n",
    "class non_linear_encoder():\n",
    "    def __init__(self, out_dim, in_dim, activation, cuda = True):\n",
    "        self.W = uniform_initializer(out_dim, in_dim, cuda)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def apply(self, X):\n",
    "        #print(X.shape)\n",
    "        #print(self.W.shape)\n",
    "        return self.activation((torch.matmul(self.W, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'in_dim': 784,\n",
    "    'n_class': 10,\n",
    "    'f_encoder': simple_encoder_wthreshold(784//2, 784, 10e-3),\n",
    "    'f_initializer': uniform_initializer,\n",
    "    'encoder_multiplier': 0.5,\n",
    "    'epsilon': 10e-3,\n",
    "    'n_layers': 1,\n",
    "    'layer_size_factor': [1, 1, 1],\n",
    "    'dropout': [0.3, -1, -1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model1 = One_hot_net(parameters['in_dim'], parameters['n_class'], parameters['f_encoder'], parameters['encoder_multiplier'], parameters['f_initializer'], parameters['epsilon'], parameters['n_layers'], parameters['layer_size_factor'], parameters['dropout']).to(device)\n",
    "#model1 = toy_Net2().to(device)\n",
    "#model1 = toy_Net3().to(device)\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.01, momentum=0.5)\n",
    "#optimizer1 = torch.optim.SGD([{'params': model1.layers.parameters(), 'lr': 0.01}, {'params': model1.tail.parameters(), 'lr': 0.01}], lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Dropout-1                    [-1, 2]               0\n",
      "            Linear-2                    [-1, 2]           3,930\n",
      "        LogSoftmax-3                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 3,930\n",
      "Trainable params: 3,930\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model1, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, trainloader, log_interval = 10, device = torch.device(\"cuda:0\")):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "            (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            #torch.save(model.state_dict(), '/results/model.pth')\n",
    "            #torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device = torch.device(\"cuda:0\")):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3031, Accuracy: 1154/10000 (11.5%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303139\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.120127\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.021809\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.766909\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.677190\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.539331\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.448591\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.380113\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.353810\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.303342\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.311778\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.184278\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.187466\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.152380\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.124464\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.048178\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.017401\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.011562\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.980637\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.037305\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.857628\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.939453\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.961212\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.926662\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.912927\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.961958\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.800350\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.884316\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.759912\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.753500\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.732009\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.799158\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.668175\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.636482\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.726428\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.663477\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.725729\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.835828\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.604533\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.860354\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.647342\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.747469\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.705472\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.644121\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.769917\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.704718\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.687925\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.677260\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.806551\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.397536\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.696121\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.708353\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.673203\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.609021\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.686509\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.715735\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.744466\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.804950\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.658514\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.758261\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.628986\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.829675\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.558099\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.731240\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.700851\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.803164\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.783345\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.823857\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.701770\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.620186\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.572076\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.589630\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.615141\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.727394\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.524129\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.758227\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.671610\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.509148\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.568805\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.542506\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.551293\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.694158\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.578301\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.500706\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.707316\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.605127\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.472190\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.604282\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.473837\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.734578\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.614757\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.600746\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.752524\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.780257\n",
      "\n",
      "Test set: Avg. loss: 0.5041, Accuracy: 8726/10000 (87.3%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.554300\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.641863\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.637663\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.712495\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.652741\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.518666\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.597462\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.572782\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.577244\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.483147\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.823126\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.521873\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.617727\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.624154\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.514588\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.516845\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.606692\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.541415\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.514468\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.568793\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.568949\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.562588\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.606226\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.552294\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.652794\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.505399\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.495394\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.518461\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.599937\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.650088\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.501933\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.424674\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.556480\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.460764\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.516414\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.567265\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.482137\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.612902\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.469840\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.636658\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.622000\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.605403\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.591281\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.557009\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.510951\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.471827\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.421840\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.460623\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.544991\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.354837\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.470622\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.745404\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.380492\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.660660\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.571705\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.436780\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.558655\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.510455\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.504146\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.626806\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.606711\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.546879\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.550705\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.530535\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.605405\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.511888\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.577123\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.397911\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.356741\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.650396\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.648594\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.535270\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.436209\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.490471\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.728771\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.511423\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.631785\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.550318\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.593904\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.530306\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.547234\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.587552\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.640254\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.446267\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.411116\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.464599\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.464619\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.473653\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.616812\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.481234\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.694405\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.540865\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.638104\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.533251\n",
      "\n",
      "Test set: Avg. loss: 0.4270, Accuracy: 8843/10000 (88.4%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.586407\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.527873\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.585316\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.409315\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.603674\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.495322\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.586464\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.622911\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.395906\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.694177\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.840217\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.633526\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.408204\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.535091\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.761725\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.586431\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.440909\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.380142\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.671899\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.562951\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.507916\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.640133\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.569210\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.534730\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.648760\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.496224\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.488155\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.535387\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.577476\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.436557\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.612908\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.584660\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.627319\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.549992\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.529607\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.488435\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.421310\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.598253\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.359195\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.599330\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.558063\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.427237\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.495929\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.670433\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.541571\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.353816\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.407538\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.389575\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.664760\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.448994\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.415234\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.665648\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.685577\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.417240\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.449317\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.574752\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.482365\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.430089\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.485913\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.459463\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.362546\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.530244\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.501558\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.553023\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.495192\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.542074\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.398505\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.344759\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.451026\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.256613\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.506503\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.314728\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.429362\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.604952\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.411777\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.762898\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.519532\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.530422\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.808518\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.694387\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.833823\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.404676\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.602705\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.354540\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.508358\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.382121\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.550005\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.410983\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.524793\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.435828\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.758548\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.330992\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.485660\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.518298\n",
      "\n",
      "Test set: Avg. loss: 0.3946, Accuracy: 8896/10000 (89.0%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.451018\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.523154\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.434713\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.534674\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.661447\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.477756\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.391560\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.422343\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.548696\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.583128\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.579539\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.649309\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.682212\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.295220\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.509623\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.511206\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.761115\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.468941\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.377980\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.672987\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.656791\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.547738\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.560017\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.444789\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.502786\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.543714\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.584174\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.351006\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.354862\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.623808\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.537386\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.549005\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.398575\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.460246\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.440708\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.488974\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.481791\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.668134\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.481257\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.568854\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.441275\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.477267\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.523194\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.412100\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.514955\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.249229\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.412165\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.413734\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.759778\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.505123\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.458895\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.356715\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.410820\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.334269\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.399865\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.524191\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.447973\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.634515\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.601259\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.562120\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.630776\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.508869\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.486214\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.498062\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.478935\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.654638\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.535840\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.427307\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.637198\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.538556\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.366864\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.754739\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.524696\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.505356\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.402658\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.466044\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.545909\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.672562\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.605024\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.514023\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.467690\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.432869\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.892640\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.402178\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.550065\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.510183\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.398471\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.447824\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.563350\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.708936\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.392768\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.387247\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.373434\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.493753\n",
      "\n",
      "Test set: Avg. loss: 0.3761, Accuracy: 8939/10000 (89.4%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.556579\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.355440\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.313141\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.383440\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.480431\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.620259\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.390679\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.516717\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.651327\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.426909\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.315416\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.553635\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.569661\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.405651\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.512625\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.607853\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.700539\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.412736\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.369150\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.618685\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.763997\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.413379\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.649500\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.414149\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.645546\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.646380\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.405431\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.443231\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.609673\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.455038\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.502262\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.367905\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.548289\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.676686\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.535050\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.401512\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.729353\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.614491\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.544524\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.445380\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.353476\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.459390\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.433895\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.563250\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.293526\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.339799\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.430470\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.458673\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.541020\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.664760\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.458857\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.419473\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.599063\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.485699\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.386438\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.473634\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.660432\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.468282\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.254839\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.355481\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.542331\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.411884\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.552313\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.418222\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.364251\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.373547\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.601441\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.618686\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.447797\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.426439\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.328433\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.671584\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.365659\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.434823\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.349226\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.477893\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.562426\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.364958\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.485414\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.304799\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.450729\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.337229\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.561128\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.387719\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.612572\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.604306\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.447884\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.443097\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.477599\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.355549\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.618449\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.308260\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.554551\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.291786\n",
      "\n",
      "Test set: Avg. loss: 0.3658, Accuracy: 8952/10000 (89.5%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.403991\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.343957\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.678639\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.362575\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.589869\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.622483\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.651465\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.482190\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.485780\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.353572\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.458429\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.584765\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.459009\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.278202\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.554269\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.709717\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.299869\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.525601\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.693056\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.400663\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.642430\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.357993\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.364871\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.452641\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.476506\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.655329\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.450388\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.523337\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.493034\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.542792\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.407696\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.458617\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.462260\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.500910\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.405439\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.472292\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.407413\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.538417\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.367752\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.708077\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.457151\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.425355\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.478989\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.391348\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.490400\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.464134\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.437872\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.758247\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.556678\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.475353\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.563782\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.678064\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.386003\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.455247\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.414361\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.512203\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.322600\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.790282\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.413928\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.450742\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.514172\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.483741\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.586853\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.430258\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.460516\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.364915\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.485455\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.618923\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.411115\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.603448\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.455062\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.462563\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.360213\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.354570\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.813507\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.456912\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.538874\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.243766\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.351440\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.518800\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.328788\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.569824\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.674621\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.642648\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.352296\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.528711\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.349614\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.471286\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.624399\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.373446\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.518120\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.326535\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.387472\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.512130\n",
      "\n",
      "Test set: Avg. loss: 0.3573, Accuracy: 8967/10000 (89.7%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.450360\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.449675\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.513307\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.456041\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.553965\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.426854\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.395020\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.489476\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.558695\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.550517\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.530492\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.479371\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.566386\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.286679\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.383835\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.331220\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.512393\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.378234\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.380228\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.640450\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.435553\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.462376\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.635978\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.445134\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.480535\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.324769\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.803216\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.533588\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.415557\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.363193\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.414898\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.653137\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.432520\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.544903\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.447126\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.282179\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.460128\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.452825\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.514683\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.334330\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.616079\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.389885\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.529735\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.534249\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.442323\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.402514\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.661896\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.488750\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.323363\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.504183\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.480450\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.306410\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.514626\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.477154\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.272321\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.787659\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.590578\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.346815\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.368134\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.537335\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.184604\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.382837\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.449903\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.560580\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.357202\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.460685\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.387669\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.534261\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.489696\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.604051\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.402994\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.383830\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.603914\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.534578\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.377031\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.491464\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.413728\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.429276\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.402796\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.607775\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.534130\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.457202\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.484009\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.510948\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.429977\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.386662\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.605105\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.247356\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.457278\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.651173\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.430255\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.320921\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.504930\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.340152\n",
      "\n",
      "Test set: Avg. loss: 0.3531, Accuracy: 8974/10000 (89.7%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.463652\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.358184\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.559777\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.393531\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.625206\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.534836\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.301560\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.524274\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.370668\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.410663\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.397504\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.506216\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.528454\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.639498\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.486931\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.437488\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.352842\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.700217\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.392235\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.434646\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.543617\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.479354\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.373641\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.616894\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.481360\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.356214\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.351688\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.599551\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.296914\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.314019\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.384801\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.495279\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.409067\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.408664\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.412727\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.510114\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.424044\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.464299\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.461360\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.409383\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.369138\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.384280\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.285989\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.361159\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.527581\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.282107\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.294485\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.484365\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.552196\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.677471\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.485749\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.458942\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.426122\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.421718\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.502813\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.549224\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.534088\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.405130\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.323189\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.432329\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.382876\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.493941\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.443904\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.528182\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.496205\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.419944\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.652976\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.369512\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.317121\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.346590\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.542700\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.658591\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.435652\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.514476\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.538790\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.382696\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.263913\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.317336\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.442927\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.475364\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.329949\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.569403\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.550183\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.278453\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.672500\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.593996\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.400827\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.467965\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.343500\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.538158\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.410382\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.297945\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.404508\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.314196\n",
      "\n",
      "Test set: Avg. loss: 0.3512, Accuracy: 8964/10000 (89.6%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.488447\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.550298\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.491783\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.593686\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.506390\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.593185\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.539438\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.574008\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.546793\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.433891\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.372753\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.502543\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.480863\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.723050\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.542004\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.302894\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.424566\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.699572\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.416387\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.518514\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.555859\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.522506\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.672595\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.342496\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.301289\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.342396\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.370623\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.373001\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.504709\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.352245\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.499282\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.531762\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.582745\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.426458\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.499369\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.337838\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.555858\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.461881\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.378653\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.343708\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.549520\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.457435\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.426171\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.250276\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.714739\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.387617\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.492402\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.629448\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.376470\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.412673\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.492957\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.400520\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.291044\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.458333\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.247949\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.328096\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.626797\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.851193\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.525944\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.307822\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.521336\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.490168\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.393479\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.469140\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.366782\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.512926\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.386029\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.370610\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.447558\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.391929\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.487672\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.625709\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.356301\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.379460\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.535202\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.504297\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.566875\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.466258\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.184694\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.309710\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.362565\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.421871\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.564675\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.589399\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.525926\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.483753\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.462703\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.384620\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.397210\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.491264\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.570980\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.475473\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.320266\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.510727\n",
      "\n",
      "Test set: Avg. loss: 0.3440, Accuracy: 9001/10000 (90.0%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.511260\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.525257\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.510375\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.465452\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.457708\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.421334\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.522419\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.318762\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.623025\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.539921\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.610741\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.514431\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.838685\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.603393\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.507312\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.533811\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.515702\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.462281\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.374925\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.745612\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.373055\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.336410\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.387587\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.502133\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.511011\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.433785\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.387638\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.353456\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.397449\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.565217\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.515836\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.538252\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.406822\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.386683\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.209227\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.578609\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.519006\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.386512\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.511554\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.592515\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.447891\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.451462\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.288282\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.477925\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.398484\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.488125\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.447594\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.405889\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.374634\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.406292\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.451869\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.304432\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.366453\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.304928\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.376487\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.390595\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.587650\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.395639\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.504980\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.585865\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.393180\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.284868\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.355563\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.595613\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.468236\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.372623\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.415713\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.624478\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.426443\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.500578\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.360989\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.487836\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.417507\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.242432\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.398190\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.432874\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.387949\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.526820\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.390767\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.366410\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.649544\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.402915\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.505818\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.496106\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.451394\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.398518\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.659087\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.456091\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.416606\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.413171\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.387000\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.514822\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.372681\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.317034\n",
      "\n",
      "Test set: Avg. loss: 0.3413, Accuracy: 8997/10000 (90.0%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.436832\n",
      "Train Epoch: 11 [640/60000 (1%)]\tLoss: 0.591293\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.296840\n",
      "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.664642\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.311628\n",
      "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.613567\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.347214\n",
      "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 0.421293\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.486422\n",
      "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 0.499520\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.435939\n",
      "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 0.537924\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.325495\n",
      "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.247073\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.398418\n",
      "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.392330\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.440627\n",
      "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 0.558036\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.424689\n",
      "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.801827\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.338671\n",
      "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.492438\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.684196\n",
      "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.575257\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.288215\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.306935\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.373070\n",
      "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.468919\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.554369\n",
      "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 0.486477\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.357074\n",
      "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 0.324171\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.447731\n",
      "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 0.340148\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.321188\n",
      "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.380478\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.438551\n",
      "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.803383\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.482748\n",
      "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 0.356834\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.366623\n",
      "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 0.444889\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.322785\n",
      "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 0.437571\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.355231\n",
      "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.550690\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.279663\n",
      "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 0.326426\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.553229\n",
      "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.456446\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.459229\n",
      "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 0.734010\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.374570\n",
      "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.276476\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.476361\n",
      "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.541069\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.445326\n",
      "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.383847\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.591959\n",
      "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.299955\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.658509\n",
      "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.373651\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.689806\n",
      "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 0.666359\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.616945\n",
      "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.280244\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.540345\n",
      "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.303008\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.577784\n",
      "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.327419\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.434927\n",
      "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.315251\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.667420\n",
      "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 0.350154\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.581841\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.503191\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.510078\n",
      "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 0.286029\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.441875\n",
      "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.336229\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.799742\n",
      "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.338176\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.298945\n",
      "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.396314\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.374405\n",
      "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.532752\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.520241\n",
      "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 0.568910\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.391057\n",
      "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 0.599087\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.575550\n",
      "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.387660\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.648782\n",
      "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.344382\n",
      "\n",
      "Test set: Avg. loss: 0.3403, Accuracy: 8993/10000 (89.9%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.399252\n",
      "Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.354532\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.428155\n",
      "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.379266\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.659376\n",
      "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.463267\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.589927\n",
      "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.423975\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.344670\n",
      "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 0.387812\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.416926\n",
      "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.662115\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.667114\n",
      "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 0.342227\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.407032\n",
      "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.406351\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.368006\n",
      "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 0.462188\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.371875\n",
      "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 0.364470\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.394663\n",
      "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.418180\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.305448\n",
      "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.429312\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.556863\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.450083\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.466607\n",
      "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.613761\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.533137\n",
      "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 0.525991\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.332820\n",
      "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 0.665208\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.652018\n",
      "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 0.343039\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.349171\n",
      "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.429441\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.429446\n",
      "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 0.342045\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.699312\n",
      "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.321643\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.362861\n",
      "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.626947\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.342899\n",
      "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.793235\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.540928\n",
      "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.649492\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.361409\n",
      "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 0.467670\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.552559\n",
      "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 0.558170\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.345028\n",
      "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 0.426045\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.750767\n",
      "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.380562\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.506841\n",
      "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.261568\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.631253\n",
      "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 0.304884\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.515526\n",
      "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.483943\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.483793\n",
      "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.516452\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.535798\n",
      "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.517737\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.607698\n",
      "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.311235\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.383361\n",
      "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.314854\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.409580\n",
      "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 0.505807\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.449432\n",
      "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 0.493966\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.487186\n",
      "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 0.593267\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.334556\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.417021\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.608370\n",
      "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 0.585268\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.745308\n",
      "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 0.808765\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.507861\n",
      "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.449112\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.491949\n",
      "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 0.513888\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.508004\n",
      "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.587917\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.474433\n",
      "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 0.550435\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.682117\n",
      "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.365632\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.490041\n",
      "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.465126\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.367163\n",
      "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.505715\n",
      "\n",
      "Test set: Avg. loss: 0.3396, Accuracy: 8981/10000 (89.8%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.508872\n",
      "Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.569005\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.470043\n",
      "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 0.655637\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.322902\n",
      "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.417172\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.663129\n",
      "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.497424\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.650962\n",
      "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.338398\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.799447\n",
      "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.362245\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.426166\n",
      "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.534375\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.442369\n",
      "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.516867\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.376014\n",
      "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 0.234216\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.440528\n",
      "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.520469\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.331956\n",
      "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 0.503742\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.361993\n",
      "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.496430\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.652648\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.501846\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.267639\n",
      "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.644631\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.450536\n",
      "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 0.479169\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.323620\n",
      "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 0.403847\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.450962\n",
      "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.443823\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.416218\n",
      "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.518267\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.435466\n",
      "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 0.384507\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.580475\n",
      "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 0.423885\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.344373\n",
      "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.735752\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.499557\n",
      "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 0.449120\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.227163\n",
      "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.609241\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.779831\n",
      "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.569311\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.570401\n",
      "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.590873\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.486931\n",
      "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 0.712718\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.325826\n",
      "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.279766\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.362731\n",
      "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.591255\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.473261\n",
      "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 0.401950\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.486795\n",
      "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 0.289032\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.348432\n",
      "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.423365\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.314640\n",
      "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 0.379121\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.518465\n",
      "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.581968\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.321678\n",
      "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.354426\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.418681\n",
      "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.461763\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.423500\n",
      "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 0.477740\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.450292\n",
      "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.341618\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.436762\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.392293\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.689329\n",
      "Train Epoch: 13 [49280/60000 (82%)]\tLoss: 0.562356\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.389724\n",
      "Train Epoch: 13 [50560/60000 (84%)]\tLoss: 0.560074\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.343887\n",
      "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 0.301409\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.557266\n",
      "Train Epoch: 13 [53120/60000 (88%)]\tLoss: 0.531125\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.204500\n",
      "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.584310\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.568609\n",
      "Train Epoch: 13 [55680/60000 (93%)]\tLoss: 0.442392\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.387878\n",
      "Train Epoch: 13 [56960/60000 (95%)]\tLoss: 0.411011\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.306226\n",
      "Train Epoch: 13 [58240/60000 (97%)]\tLoss: 0.299154\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.387685\n",
      "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.407542\n",
      "\n",
      "Test set: Avg. loss: 0.3391, Accuracy: 8997/10000 (90.0%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.408450\n",
      "Train Epoch: 14 [640/60000 (1%)]\tLoss: 0.576906\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.554102\n",
      "Train Epoch: 14 [1920/60000 (3%)]\tLoss: 0.473782\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.581486\n",
      "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.315547\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.331693\n",
      "Train Epoch: 14 [4480/60000 (7%)]\tLoss: 0.579695\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.428530\n",
      "Train Epoch: 14 [5760/60000 (10%)]\tLoss: 0.557231\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.309131\n",
      "Train Epoch: 14 [7040/60000 (12%)]\tLoss: 0.378351\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.546564\n",
      "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 0.702706\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.552093\n",
      "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.454302\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.289081\n",
      "Train Epoch: 14 [10880/60000 (18%)]\tLoss: 0.418198\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.283154\n",
      "Train Epoch: 14 [12160/60000 (20%)]\tLoss: 0.538309\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.469224\n",
      "Train Epoch: 14 [13440/60000 (22%)]\tLoss: 0.481870\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.607205\n",
      "Train Epoch: 14 [14720/60000 (25%)]\tLoss: 0.404951\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.631334\n",
      "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.412976\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.285644\n",
      "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.515695\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.375627\n",
      "Train Epoch: 14 [18560/60000 (31%)]\tLoss: 0.534116\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.759818\n",
      "Train Epoch: 14 [19840/60000 (33%)]\tLoss: 0.503905\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.483907\n",
      "Train Epoch: 14 [21120/60000 (35%)]\tLoss: 0.429149\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.346207\n",
      "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.333988\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.505330\n",
      "Train Epoch: 14 [23680/60000 (39%)]\tLoss: 0.364526\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.509567\n",
      "Train Epoch: 14 [24960/60000 (42%)]\tLoss: 0.386681\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.493542\n",
      "Train Epoch: 14 [26240/60000 (44%)]\tLoss: 0.512707\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.422403\n",
      "Train Epoch: 14 [27520/60000 (46%)]\tLoss: 0.433579\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.605685\n",
      "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.350024\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.356871\n",
      "Train Epoch: 14 [30080/60000 (50%)]\tLoss: 0.335873\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.435941\n",
      "Train Epoch: 14 [31360/60000 (52%)]\tLoss: 0.259556\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.562053\n",
      "Train Epoch: 14 [32640/60000 (54%)]\tLoss: 0.357010\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.611491\n",
      "Train Epoch: 14 [33920/60000 (57%)]\tLoss: 0.378929\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.468087\n",
      "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.394415\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.398736\n",
      "Train Epoch: 14 [36480/60000 (61%)]\tLoss: 0.412974\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.276463\n",
      "Train Epoch: 14 [37760/60000 (63%)]\tLoss: 0.660819\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.446391\n",
      "Train Epoch: 14 [39040/60000 (65%)]\tLoss: 0.526242\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.401223\n",
      "Train Epoch: 14 [40320/60000 (67%)]\tLoss: 0.316298\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.421369\n",
      "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.350740\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.347326\n",
      "Train Epoch: 14 [42880/60000 (71%)]\tLoss: 0.531706\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.630969\n",
      "Train Epoch: 14 [44160/60000 (74%)]\tLoss: 0.403159\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.318707\n",
      "Train Epoch: 14 [45440/60000 (76%)]\tLoss: 0.259463\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.357982\n",
      "Train Epoch: 14 [46720/60000 (78%)]\tLoss: 0.566316\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.429104\n",
      "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.455061\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.604913\n",
      "Train Epoch: 14 [49280/60000 (82%)]\tLoss: 0.287967\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.352821\n",
      "Train Epoch: 14 [50560/60000 (84%)]\tLoss: 0.503959\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.549570\n",
      "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 0.449390\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.441840\n",
      "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.431647\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.581968\n",
      "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.457009\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.475420\n",
      "Train Epoch: 14 [55680/60000 (93%)]\tLoss: 0.528828\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.375445\n",
      "Train Epoch: 14 [56960/60000 (95%)]\tLoss: 0.525589\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.507878\n",
      "Train Epoch: 14 [58240/60000 (97%)]\tLoss: 0.495791\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.556415\n",
      "Train Epoch: 14 [59520/60000 (99%)]\tLoss: 0.492408\n",
      "\n",
      "Test set: Avg. loss: 0.3354, Accuracy: 9022/10000 (90.2%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.248553\n",
      "Train Epoch: 15 [640/60000 (1%)]\tLoss: 0.603103\n",
      "Train Epoch: 15 [1280/60000 (2%)]\tLoss: 0.674584\n",
      "Train Epoch: 15 [1920/60000 (3%)]\tLoss: 0.423639\n",
      "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 0.399947\n",
      "Train Epoch: 15 [3200/60000 (5%)]\tLoss: 0.427079\n",
      "Train Epoch: 15 [3840/60000 (6%)]\tLoss: 0.412427\n",
      "Train Epoch: 15 [4480/60000 (7%)]\tLoss: 0.465412\n",
      "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 0.551562\n",
      "Train Epoch: 15 [5760/60000 (10%)]\tLoss: 0.460796\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.342120\n",
      "Train Epoch: 15 [7040/60000 (12%)]\tLoss: 0.408116\n",
      "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 0.251105\n",
      "Train Epoch: 15 [8320/60000 (14%)]\tLoss: 0.566217\n",
      "Train Epoch: 15 [8960/60000 (15%)]\tLoss: 0.335192\n",
      "Train Epoch: 15 [9600/60000 (16%)]\tLoss: 0.489450\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.254042\n",
      "Train Epoch: 15 [10880/60000 (18%)]\tLoss: 0.559451\n",
      "Train Epoch: 15 [11520/60000 (19%)]\tLoss: 0.371680\n",
      "Train Epoch: 15 [12160/60000 (20%)]\tLoss: 0.519381\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.817421\n",
      "Train Epoch: 15 [13440/60000 (22%)]\tLoss: 0.423200\n",
      "Train Epoch: 15 [14080/60000 (23%)]\tLoss: 0.340814\n",
      "Train Epoch: 15 [14720/60000 (25%)]\tLoss: 0.307538\n",
      "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 0.394149\n",
      "Train Epoch: 15 [16000/60000 (27%)]\tLoss: 0.585980\n",
      "Train Epoch: 15 [16640/60000 (28%)]\tLoss: 0.437068\n",
      "Train Epoch: 15 [17280/60000 (29%)]\tLoss: 0.402019\n",
      "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 0.448499\n",
      "Train Epoch: 15 [18560/60000 (31%)]\tLoss: 0.529536\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.409705\n",
      "Train Epoch: 15 [19840/60000 (33%)]\tLoss: 0.655909\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.540321\n",
      "Train Epoch: 15 [21120/60000 (35%)]\tLoss: 0.471860\n",
      "Train Epoch: 15 [21760/60000 (36%)]\tLoss: 0.501787\n",
      "Train Epoch: 15 [22400/60000 (37%)]\tLoss: 0.581629\n",
      "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 0.599449\n",
      "Train Epoch: 15 [23680/60000 (39%)]\tLoss: 0.564025\n",
      "Train Epoch: 15 [24320/60000 (41%)]\tLoss: 0.279287\n",
      "Train Epoch: 15 [24960/60000 (42%)]\tLoss: 0.550622\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.355160\n",
      "Train Epoch: 15 [26240/60000 (44%)]\tLoss: 0.340255\n",
      "Train Epoch: 15 [26880/60000 (45%)]\tLoss: 0.302843\n",
      "Train Epoch: 15 [27520/60000 (46%)]\tLoss: 0.487034\n",
      "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 0.284162\n",
      "Train Epoch: 15 [28800/60000 (48%)]\tLoss: 0.541036\n",
      "Train Epoch: 15 [29440/60000 (49%)]\tLoss: 0.485930\n",
      "Train Epoch: 15 [30080/60000 (50%)]\tLoss: 0.300430\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.414929\n",
      "Train Epoch: 15 [31360/60000 (52%)]\tLoss: 0.646879\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.291580\n",
      "Train Epoch: 15 [32640/60000 (54%)]\tLoss: 0.432650\n",
      "Train Epoch: 15 [33280/60000 (55%)]\tLoss: 0.451342\n",
      "Train Epoch: 15 [33920/60000 (57%)]\tLoss: 0.334117\n",
      "Train Epoch: 15 [34560/60000 (58%)]\tLoss: 0.355776\n",
      "Train Epoch: 15 [35200/60000 (59%)]\tLoss: 0.312252\n",
      "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 0.675297\n",
      "Train Epoch: 15 [36480/60000 (61%)]\tLoss: 0.535729\n",
      "Train Epoch: 15 [37120/60000 (62%)]\tLoss: 0.432267\n",
      "Train Epoch: 15 [37760/60000 (63%)]\tLoss: 0.555562\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.430236\n",
      "Train Epoch: 15 [39040/60000 (65%)]\tLoss: 0.480783\n",
      "Train Epoch: 15 [39680/60000 (66%)]\tLoss: 0.472416\n",
      "Train Epoch: 15 [40320/60000 (67%)]\tLoss: 0.317146\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.407776\n",
      "Train Epoch: 15 [41600/60000 (69%)]\tLoss: 0.322012\n",
      "Train Epoch: 15 [42240/60000 (70%)]\tLoss: 0.444291\n",
      "Train Epoch: 15 [42880/60000 (71%)]\tLoss: 0.259609\n",
      "Train Epoch: 15 [43520/60000 (72%)]\tLoss: 0.389648\n",
      "Train Epoch: 15 [44160/60000 (74%)]\tLoss: 0.249299\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.423743\n",
      "Train Epoch: 15 [45440/60000 (76%)]\tLoss: 0.487370\n",
      "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 0.523415\n",
      "Train Epoch: 15 [46720/60000 (78%)]\tLoss: 0.382744\n",
      "Train Epoch: 15 [47360/60000 (79%)]\tLoss: 0.567780\n",
      "Train Epoch: 15 [48000/60000 (80%)]\tLoss: 0.346999\n",
      "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 0.495193\n",
      "Train Epoch: 15 [49280/60000 (82%)]\tLoss: 0.421495\n",
      "Train Epoch: 15 [49920/60000 (83%)]\tLoss: 0.401271\n",
      "Train Epoch: 15 [50560/60000 (84%)]\tLoss: 0.506786\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.752605\n",
      "Train Epoch: 15 [51840/60000 (86%)]\tLoss: 0.188885\n",
      "Train Epoch: 15 [52480/60000 (87%)]\tLoss: 0.525743\n",
      "Train Epoch: 15 [53120/60000 (88%)]\tLoss: 0.296708\n",
      "Train Epoch: 15 [53760/60000 (90%)]\tLoss: 0.306574\n",
      "Train Epoch: 15 [54400/60000 (91%)]\tLoss: 0.312983\n",
      "Train Epoch: 15 [55040/60000 (92%)]\tLoss: 0.410239\n",
      "Train Epoch: 15 [55680/60000 (93%)]\tLoss: 0.470694\n",
      "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 0.582724\n",
      "Train Epoch: 15 [56960/60000 (95%)]\tLoss: 0.427922\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.483183\n",
      "Train Epoch: 15 [58240/60000 (97%)]\tLoss: 0.401836\n",
      "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 0.600467\n",
      "Train Epoch: 15 [59520/60000 (99%)]\tLoss: 0.323614\n",
      "\n",
      "Test set: Avg. loss: 0.3341, Accuracy: 9016/10000 (90.2%)\n",
      "\n",
      "\n",
      "--- saving model for best accuracy ---\n",
      "--- saving finished ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"base_net1\"\n",
    "acc_max = test(model1, test_loader)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch, model1, optimizer1, train_loader)\n",
    "    acc = test(model1, test_loader)\n",
    "    if acc > acc_max:\n",
    "        acc = acc_max\n",
    "        print()\n",
    "        print(\"--- saving model for best accuracy ---\")\n",
    "        #torch.save(model1.state_dict(), 'results/'+model_name+'.pth')\n",
    "        #torch.save(optimizer1.state_dict(), 'results/'+model_name+'_optimizer.pth')\n",
    "        print(\"--- saving finished ---\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'negative log likelihood loss')"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model1.state_dict(), 'results/base_model.pth')\n",
    "#torch.save(optimizer1.state_dict(), 'results/base_optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "import matplotlib.pyplot as plt\n",
    "import os    \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "fig = plt.figure()\n",
    "model1.eval()\n",
    "output = model1(example_data.to(torch.device(\"cuda:0\"))).cpu().detach()\n",
    "prediction = torch.argmax(output, dim = 1)\n",
    "print(prediction)\n",
    "for i in range(20):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"G T: {} P: {}\".format(example_targets[i], prediction[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
